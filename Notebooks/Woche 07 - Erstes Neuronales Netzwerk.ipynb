{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Erstes Neuronales Netzwerk\n",
    "---\n",
    "### Lernziele\n",
    "\n",
    "- Sie lernen wie one-hot encoding funktioniert.\n",
    "- Sie können ein einfaches Neuronales Netzwerk in Python programmieren.\n",
    "---\n",
    "\n",
    "Bevor wir das Netzwerk trainieren, müssen wir als Erstes unsere Libraries und Funktionen laden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt # neben Numpy laden wir auch eine Funktion der Libary matplotlib -> sie ermöglicht es Graphen in Python zu erstellen\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aktiverungsfunktionen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x)) # np.exp() = e^()\n",
    "\n",
    "def softmax(x, axis=1):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=axis, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding\n",
    "\n",
    "Wir wollen die Labels nicht als einfache Zahl, sondern als Vektor aus Nullen und Einsen haben. Siehe unten:<br>\n",
    "<center>\n",
    "0  =  [1 0 0 0 0 0 0 0 0 0]<br>\n",
    "1  =  [0 1 0 0 0 0 0 0 0 0]<br>\n",
    "2  =  [0 0 1 0 0 0 0 0 0 0]<br>\n",
    "3  =  [0 0 0 1 0 0 0 0 0 0]<br>\n",
    "4  =  [0 0 0 0 1 0 0 0 0 0]<br>\n",
    "5  =  [0 0 0 0 0 1 0 0 0 0]<br>\n",
    "usw. <br>\n",
    "9  =  [0 0 0 0 0 0 0 0 0 1]<br>\n",
    "</center>\n",
    "\n",
    "Die Funktion `one-hot` macht genau das."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(x):\n",
    "    \"\"\"Die Labels der Bilder müssen noch in Vektoren von Länge 10 codiert werden\"\"\"\n",
    "    dod = len(set(x)) # Checkt wie viele verschieden Ziffern es im Datennsatz gibt\n",
    "    target = np.zeros([x.shape[0], dod]) # Eine Matrix aus Nullen wird erstellt\n",
    "    for i in range(x.shape[0]): # Der for-loop setzt eine 1 in die Matrix abhängig davon welches Label das Bild hat\n",
    "        target[i, x[i]] = 1\n",
    "\n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skaliert die originalen Bilder, die Werte zwischen 0 und 255 haben können, zu Bildern mit Werten zwischen 0 und 1.\n",
    "# Neuronale Netzwerke können besser trainiert werden\n",
    "def min_max(x):\n",
    "    return (x - np.min(x)) / (np.max(x) - np.min(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daten einlesen\n",
    "\n",
    "Wir können mit Numpy ohne Probleme die benötigten Daten einlesen. Die Trainingsdaten `mnist_train.csv` enthalten Bilder und deren Labels. Die Bilder sind schon von einer Matrix in einen Vektor transformiert worden.<br>\n",
    "<center>\n",
    "<img src=\"https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/73_blog_image_1.png\" style=\"width: 800px;\">\n",
    "</center>\n",
    "\n",
    "Der Datensatz kann auch extern angeschaut werden. Dafür klicken Sie einfach in der linken Spalte (je nach Jupyter Version) auf den Ordner `data` und danach auf die Datei `mnist_train.csv`. Die erste Spalte des jeweiligen Datensatzes enthält die Labels. In jeder Reihe ist ein Bild, zudem ist jeder Spalte (bis auf der ersten) ein Pixel zugeordnet.\n",
    "\n",
    "Mit `np.genfromtxt()` lassen sich aus .txt-Datein Arrays in Python erstellen. \n",
    "\n",
    "\n",
    "In diesem Beispiel sind die Labels bzw. Targets in der ersten Spalte und nicht in der letzten.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.genfromtxt('../data/mnist/mnist_train.csv', delimiter=',', skip_header =False) #genfromtxt liest .txt Datein, mit delimiter =\",\" können auch .csv (comma seperated values) Datein einglesen werden  \n",
    "test_data=np.genfromtxt('../data/mnist/mnist_test.csv', delimiter=',', skip_header =False) # hier lesen wir die Test Daten ein\n",
    "\n",
    "# Nach dem wir die Daten einglesen haben, trennen wir die Daten noch in Bilder und Labels\n",
    "# Des Weiteren konvertieren wir Labels von Float zu Integer mit .astype(int)\n",
    "train_labels=train_data[:,0].astype(int) \n",
    "train_images = train_data[:,1:]\n",
    "\n",
    "test_labels=test_data[:,0].astype(int)\n",
    "test_images = test_data[:,1:]\n",
    "\n",
    "del train_data, test_data #für mehr Speicher löschen wir ursprüblichen Daten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir haben jetzt den Trainingsdatensatz eingelesen. Sie können mit `train_images.shape` sehen, dass die Variable `train_images` eine 60000 x 784 Matrix ist, also 60000 Reihen und 784 Spalten hat. Jede Reihe ist ein Bild und jede Spalte ist ein Pixel. Die originalen Bilder waren 28 x 28 Pixel groß. Im nächsten Schritt benutzen wir die *Funktion* `one_hot`, um die Labels richtig zu codieren.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets=one_hot(train_labels)\n",
    "test_targets = one_hot(test_labels)\n",
    "\n",
    "train_targets[:5,:] # die Labels der ersten fünf Bilder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als letzten Schritt müssen wir noch den `min_max` Scaler benutzen, um die Pixelwerte zwischen Null und Eins zu skalieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = min_max(train_images)\n",
    "test_images = min_max(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In der folgenden Zelle können Sie ein Beispielbild sehen. Die Funktion `.reshape([28,28])` transformiert das Bild in seine ursprüngliche Version zurück. So kann man das Bild auch wirklich erkennen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(train_images[0].reshape([28, 28]), cmap=\"gray\")\n",
    "print(\"Correct Label: %s\" % train_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Das Modell\n",
    "\n",
    "\n",
    "**Weights Initalisieren** <br>\n",
    "Als Erstes müssen die Weight und Bias Matrizen/Vektoren mit den richtigen Dimensionen erstellt werden. Dafür wird eine Funktion geschrieben. In ihr muss man die Größe der jeweiligen Layer (`input_size`, `hidden_size`, `output_size`) festlegen. Mit diesen können Sie die Weightmatrizen erstellen. Während die `b`- Vektoren mit Nullen gefüllt werden können, müssen die Weightmatrizen mit zufälligen kleinen Zahl initialisiert werden.\n",
    "\n",
    "Die Funktion `np.random.randn(number_of_rows, number_of_columns)` setzt zufällige Werte zwischen minus Eins und plus Eins in die Matrix ein. Zusätzlich wird der Term`* np.sqrt(2/layer_size)` hinzugefügt. Dieser verkleinert die Werte noch weiter und soll damit ein besseres Trainieren des Netzwerkes garantieren. \n",
    "\n",
    "Im folgenden Code sollen Sie die richtigen Matrixgrößen (Anzahl der Reihen/Spalten) eintragen. Die Inputmatrix `X` hat 60000 Reihen und 784 Spalten und bei einer Matrixmultiplikation müssen die Anzahl der Spalten der ersten Matrix mit der Anzahl der Reihen der zweiten Matrix übereinstimmen.\n",
    "Denken Sie auch daran, dass wir das Transpose der Weightmatrix benutzen werden.\n",
    "\n",
    "Den Bias `b` wird mit Nullen initialisiert. `np.zeros(500)` würde ein Vektor der Länge 500 mit Nullen füllen.\n",
    "Auch brauchen wir nicht die tatsächlichen Zahlen, sondern nur die Namen der Inputvariablen nennen,  also nicht 784, sondern `input_size`.\n",
    "Die  tatsächlichen Werte können dann bei der eigentlichen Benutzung der Funktion definiert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion zur intialisierung der Weights\n",
    "def init_weights(input_size, hidden_size, output_size):\n",
    "    # Zwei leere Listen mit Länge zwei\n",
    "    b = [0] * 2\n",
    "    W = [0] * 2\n",
    "\n",
    "    # hier werden die Weights W mit kleinen, zufälligen Gewichten initialisiert\n",
    "    \n",
    "    W[0] = np.random.randn(        ,         ) * np.sqrt(2 / input_size)  # HIER MÜSSEN DIE RICHTIGEN GRÖßEN ANGEGBEN WERDEN\n",
    "    W[1] = np.random.randn(        ,         ) * np.sqrt(2 / hidden_size) # HIER MÜSSEN DIE RICHTIGEN GRÖßEN ANGEGBEN WERDEN\n",
    "\n",
    "    # der bias kann 0 sein\n",
    "    b[0] = np.zeros(       ) # HIER MÜSSEN DIE RICHTIGEN GRÖßEN ANGEGBEN WERDEN\n",
    "    b[1] = np.zeros(       ) # HIER MÜSSEN DIE RICHTIGEN GRÖßEN ANGEGBEN WERDEN\n",
    "\n",
    "\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Lösung:</strong></summary>\n",
    "\n",
    "```python\n",
    "def init_weights(input_size, hidden_size, output_size):\n",
    "    # Zwei leere Listen mit Länge zwei\n",
    "    b = [0] * 2\n",
    "    W = [0] * 2\n",
    "\n",
    "    # hier werden die Weights W mit kleinen, zufälligen Gewichten initialisiert\n",
    "    \n",
    "    W[0] = np.random.randn(hidden_size,input_size) * np.sqrt(2 / input_size)  \n",
    "    W[1] = np.random.randn(output_size,hidden_size) * np.sqrt(2 / hidden_size) \n",
    "\n",
    "    # der bias kann 0 sein\n",
    "    b[0] = np.zeros(hidden_size) # HIER MÜSSEN DIE RICHTIGEN GRÖßEN ANGEGBEN WERDEN\n",
    "    b[1] = np.zeros(output_size) # HIER MÜSSEN DIE RICHTIGEN GRÖßEN ANGEGBEN WERDEN\n",
    "\n",
    "\n",
    "    return W, b\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Weights können jetzt initialisiert werden: \n",
    "\n",
    "Die Inputgröße ist vorgegeben, da 784 Pixel in jedem Bild sind. Auch die Outputgröße ist vorgegeben, da das Netzwerk zwischen 10 verschiedene Ziffern unterscheiden muss.\n",
    "Der einzige Wert, den Sie selber bestimmen können, ist die Größe der Hidden-Layer. Es gibt keine einfache Regel für die Größe, aber wenn sie zu klein ist, kann das die Genauigkeit des neuronalen Netzwerkes verringern. Wenn sie zu groß ist, trainiert das Netzwerk zu langsam und wird eventuell instabil. Auch das kann die Genauigkeit beeinflussen. Probieren Sie am Anfang Werte wie 100 oder 200 aus. \n",
    "\n",
    "Grundsätzliche werden Zahlen wie die Größe der Hidden-Layer oder die Lernrate, die Sie frei wählen können, als **Hyperparameter** bezeichnet. \n",
    "\n",
    "Mit dieser Funktion können jetzt die Weights initialisiert werden. Die Funktion gibt Ihnen zwei Listen `W` und `b` aus. Die beiden Listen enthalten jeweils zwei Matrizen/Vektoren, einmal für die erste und einmal für die zweite Transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W, b = init_weights(input_size=784, hidden_size=200,output_size= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "W[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenn Sie die Weights richtig initialisiert haben, sollte `W[0].shape` `(200,784)` sein.\n",
    "\n",
    "---\n",
    "\n",
    "Sie fragen sich vielleicht: Wieso initialisieren wir nicht einfach unsere Weights mit `  W[0] = np.random.randn(input_size,hidden_size)`. Dann müssten wir nicht später, das `.transpose` benutzen, da die Matrizen schon im richtigen Format sind.\n",
    "\n",
    "Tatsächlich ist das möglich, allerdings hat sich diese Art des Initialisieren als ein Standard etabliert. Um zukünftiger Verwirrung vorzubeugen, folgen wir auch diesem Standard. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Forward Pass** <br>\n",
    "\n",
    "Nachdem die Weights erstellt worden sind, können Sie den Forwardpass durch das Netzwerk durchführen. Die Bilder werden durch das Netzwerk geschickt, welches am Ende diese Bilder klassifiziert.\n",
    "Die Funktion `forward_pass()` macht genau das. Ihr Input ist:\n",
    "* `W`: die Liste der Weight Matrizen\n",
    "* `b`: die Liste der Bias Vektoren\n",
    "* `X`: die Inputmatrix der Bilder \n",
    "\n",
    "\n",
    "Als Erstes wird $Z_1 = xW_1^T+b_1$ berechnet. <br>\n",
    "Dannach $A_1= sigmoid(Z_1)$ <br>\n",
    "<br>\n",
    "Somit sind die Activations für die Hiddenlayer berechnet. Um die Klassifzierung machen zu können, müssen die Werte noch ein zweites mal transformiert werden:<br>\n",
    "$\\hat{Y} = softmax(A_1W_2^T+b_2)$\n",
    "\n",
    "Die Funktion gibt am Ende drei Variablen aus:\n",
    "* `Z_1`: die Werte der ersten linearen Transformation\n",
    "* `A_1`: die Werte nach der ersten Aktivierungsfunktion\n",
    "* `Y_hat`: die Werte der Outputlayer \n",
    "\n",
    "\n",
    "*Denken Sie daran, dass die Weights der ersten Layer nicht in `W[1]`, sondern in `W[0]` gespeichert sind, da die Indizierung bei Null und nicht bei Eins anfängt.* \n",
    "\n",
    "*Denken Sie auch daran, dass wir zur Matrixmultiplikation* `np.matmul` *und nicht mehr* `np.dot` *benutzen.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(W, b, X):\n",
    "    \n",
    "    Z_1 = # CODE UM Z_1 AUZURECHNEN\n",
    "    A_1 = # CODE UM A_1 AUSZURECHNEN\n",
    "    Z_2 = # CODE UM Z_2 AUSZURECHNEN\n",
    "    Y_hat = #CODE UM Y_HAT AUSZURECHNEN\n",
    "    return Z_1, A_1, Y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Lösung:</strong></summary>\n",
    "\n",
    "```python\n",
    "def forward_pass(W, b, X):\n",
    "    \n",
    "    Z_1 = np.matmul(X,W[0].transpose())+b[0] # CODE UM Z_1 AUZURECHNEN\n",
    "    A_1 = sigmoid(Z_1) # CODE UM A_1 AUSZURECHNEN\n",
    "    Z_2 = np.matmul(A_1,W[1].transpose())+b[1] # CODE UM Z_2 AUSZURECHNEN\n",
    "    Y_hat = softmax(Z_2) #CODE UM Y_HAT AUSZURECHNEN\n",
    "    return Z_1, A_1, Y_hat\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Funktion\n",
    "Nach dem Forward Pass wird der Loss berechnet. Dieser misst, wie gut oder schlecht das Model die Zahlen klassifizieren kann.\n",
    "\n",
    "Hierfür werden nur die Werte der Output Layer (`y_hat`) und die tatsächlichen Werte (`y`) gebraucht. Die wahren Werte wurden schon vorher mit der Funktion `one_hot` berechnet und `train_targets` genannt. \n",
    "\n",
    "Diese Lossfunktion heißt Cross Entropy Loss. Wie der Name verrät, hängt sie mit dem Binary Cross Entropy Loss zusammen. Tatsächlich ist der Binary Cross Entropy Loss nur eine spezielle Version des allgemeinen Cross Entropy Loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(y_hat, y):\n",
    "    return -np.sum(np.log(y_hat) * y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sie können jetzt die ersten drei Funktionen zusammen benutzen, um Ihre erste Klassifizierung durchzuführen.\n",
    "Denkt daran, dass die `train_images` der Input für das Netzwerk sind. Wir wollen, dass die Hidden Layer 300 Nodes hat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234) # Ein Seed wird gesetzt, damit die Ergebnisse der zufälligen Initialisierung für alle Teilnehmer:innen gleich sind\n",
    "W, b = # CODE, UM DIE WEIGHTS ZU INITIALISIEREN\n",
    "\n",
    "# Benutzen Sie die train_images als X (Input)\n",
    "Z_1, A_1, Y_hat = # CODE FÜR DEN FORWARD PASS\n",
    "\n",
    "# Hier berechnen Sie den Loss\n",
    "calc_loss(           ,           )/Y_hat.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Lösung:</strong></summary>\n",
    "\n",
    "```python\n",
    "np.random.seed(1234)\n",
    "W, b = init_weights(784,1000,10)\n",
    "\n",
    "# Benutzen Sie die train_images als X (Input)\n",
    "Z_1, A_1, Y_hat = forward_pass(W,b,train_images)\n",
    "    \n",
    "# Hier berechnen Sie den Loss\n",
    "calc_loss(Y_hat,train_targets)/Y_hat.shape[0]\n",
    "    \n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Loss ist sehr schwierig zu interpretieren. \n",
    "Deswegen können wir zusätzlich die Genaugikeit (accuracy) als Maß benutzen. Die Genauigeit ist der Prozentsatz an richtig klassifizierten Bildern\n",
    "$ \\frac{n_{richtig}}{n}$.\n",
    "\n",
    "Die Genauigkeit ist ein viel einfacheres Maß um zu beurteilen, wie gut das Model funktioniert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(true_labels,predicted):\n",
    "    return (np.sum(true_labels == np.argmax(predicted, axis=1)) / predicted.shape[0]) # argmax gibt den Index des Maximalwertes wieder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(train_labels,Y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Momentan hat das Netzwerk eine Genauigkeit von 10.3 % Prozent. 10 % Genauigkeit sind zu erwarten, wenn das Netzwerk zufällig entscheidet.\n",
    "Um besser als 10 % zu werden, müssen Sie das Netzwerk trainieren und die Weights verändern. Dazu benutzen Sie die Back Propagation.\n",
    "\n",
    "**Back Propagation**\n",
    "\n",
    "Der Fehler in der Klassifizierung wird nun zurück durch das Netzwerk geschickt und mithilfe der Gradienten werden die Weights angepasst.\n",
    "\n",
    "* $dZ_2 = \\hat{y} - y$ \n",
    "* $dW_2 = \\frac{1}{n} \\cdot dZ_2^Ta_1$\n",
    "* $db_2 = \\frac{1}{n} \\cdot \\sum_{i=1}^n dZ_2$\n",
    "* $dZ_1 = dZ_2W_2 \\cdot a_1 \\cdot (1-a_1)$\n",
    "* $dW_1 = \\frac{1}{n} \\cdot dZ_1^TX$\n",
    "* $db_1 = \\frac{1}{n} \\cdot\\sum_{i=1}^n dZ_1$\n",
    "\n",
    "Wie gesagt: Wie die Ableitungen mathematische Zustande kommen, sprengt den Rahmen dieses Seminars. In der tatsächlichen Anwendung gibt es Libraries, welche die Gradienten automatisch berechnen können. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(X, Z_1, A_1, Y_hat, y):\n",
    "    n = X.shape[0] # n ist die Anzahl der Bilder\n",
    "    \n",
    "    # Gradients für die Weights der zweiten Layer\n",
    "    dZ_2 =  # CODE UM dZ_2 AUZURECHNEN\n",
    "    dW_2 =  # CODE UM dW_2 AUZURECHNEN\n",
    "    db_2 = np.sum(dZ_2, axis=0) / n\n",
    "    \n",
    "    # Gradients für die Weights der ersten Layer\n",
    "    dZ_1 = np.multiply(np.matmul(dZ_2, W[1]), np.multiply(A_1, 1 - A_1))\n",
    "    dW_1 = # CODE UM dW_1 AUZURECHNEN\n",
    "    db_1 = # CODE UM db_1 AUZURECHNEN\n",
    "\n",
    "    return [dW_1, dW_2], [db_1, db_2] # Hier werden wieder zwei Listen als Output gegeben, in jeder der Listen befinden sich jeweils die Gradienten für W_1,W_2 und b_1 und b_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Lösung:</strong></summary>\n",
    "\n",
    "```python\n",
    "def back_prop(X, Z_1, A_1, Y_hat, y):\n",
    "    n = X.shape[0] # n ist die Anzahl der Bilder\n",
    "    # Gradients für die Weights der zweiten Layer\n",
    "    dZ_2 = Y_hat - y\n",
    "    dW_2 = np.matmul(dZ_2.transpose(), A_1) / n\n",
    "    db_2 = np.sum(dZ_2, axis=0) / n\n",
    "    \n",
    "    # Gradients für die Weights der ersten Layer\n",
    "    dZ_1 = np.multiply(np.matmul(dZ_2, W[1]), np.multiply(A_1, 1 - A_1))\n",
    "    dW_1 = np.matmul(dZ_1.transpose(), X) / n\n",
    "    db_1 = np.sum(dZ_1, axis=0) / n\n",
    "\n",
    "    return [dW_1, dW_2], [db_1, db_2] \n",
    "```\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights Updaten\n",
    "\n",
    "Im letzten Schritt werden die Weights angepasst. Dafür werden die Weights ein klein wenig entgegen des Gradienten verschoben. \n",
    "Wie weit genau hängt von der Lernrate `lr` ab. Je größer diese ist, umso größer werden die Schritte.\n",
    "Ist die Lernrate zu klein, dauert das Trainieren eventuell zu lange, oder das Netzwerk bleibt in einem lokalen Minimum hängen. Ist die Lernrate zu groß, springt der Loss des  Netzwerkes zu viel und dementsprechend kann keine optimale Performance garantiert werden.\n",
    "Die Lernrate gehört, wie die Größe der Hidden Layer, zu den Hyperparametern.\n",
    "<center>\n",
    "<img src=\"https://sebastianraschka.com/images/blog/2015/singlelayer_neural_networks_files/perceptron_learning_rate.png\" style=\"width: 800px;\">\n",
    "<h8><center>Source: Sebastian Raschka, https://sebastianraschka.com</center></h8>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(W, b, grad_W, grad_b, lr=0.0001):\n",
    "    W[0] = # CODE FÜR W[0]\n",
    "    W[1] = # CODE FÜR W[1]\n",
    "    b[0] = # CODE FÜR b[0]\n",
    "    b[1] = # CODE FÜR b[1]\n",
    "\n",
    "    return W, b # die Funktion gibt die neuen Weights und Biases aus (2 Listen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Lösung:</strong></summary>\n",
    "\n",
    "```python\n",
    "def update(W, b, grad_W, grad_b, lr=0.0001):\n",
    "    W[0] = W[0] - lr * grad_W[0]\n",
    "    W[1] = W[1] - lr * grad_W[1]\n",
    "    b[0] = b[0] - lr * grad_b[0]\n",
    "    b[1] = b[1] - lr * grad_b[1]\n",
    "\n",
    "    return W, b # die Funktion gibt die neuen Weights und Biases aus\n",
    "```\n",
    "    \n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "Jetzt können Sie alles zusammenfügen. Sie initialisieren erst die Weights, dann werden die Inputs durch das Netzwerk geschickt und der Loss wird berechnet.  Anschließend wird der Loss wieder zurück durch das Netzwerk geführt und die Gradienten berechnet. Die Gradienten werden benutzt die  Weights zu updaten. Für die Lernrate gebrauchen Sie 0,1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.WEIGHTS INITALISIEREN\n",
    "np.random.seed(1234) \n",
    "W, b = init_weights(input_size=784, hidden_size=300,output_size= 10) \n",
    "\n",
    "# 2.FORWARD PROPAGATION\n",
    "Z_1, A_1, Y_hat = forward_pass(W,b,train_images)\n",
    "\n",
    "# 3.LOSS BERECHNEN\n",
    "print(\"Loss beim ersten Durchlauf: \",calc_loss(Y_hat,train_targets)/Y_hat.shape[0], \"\\nGenauigkeit beim ersten Durchlauf:\", accuracy(train_labels, Y_hat) )\n",
    "\n",
    "# 4. BACKPROPAGATION\n",
    "grad_W, grad_b = # CODE FÜR DIE BACKPROPAGATION\n",
    "# 5. WEIGHTS UPDATEN\n",
    "W, b = # WEIGHT UPDATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Lösung:</strong></summary>\n",
    "\n",
    "```python\n",
    "# 1.WEIGHTS INITALISIEREN\n",
    "np.random.seed(1234) \n",
    "W, b = init_weights(input_size=784, hidden_size=300,output_size= 10) \n",
    "\n",
    "# 2.FORWARD PROPAGATION\n",
    "Z_1, A_1, Y_hat = forward_pass(W,b,train_images)\n",
    "\n",
    "# 3.LOSS BERECHNEN\n",
    "print(\"Loss beim ersten Durchlauf: \",calc_loss(Y_hat,train_targets)/Y_hat.shape[0], \"\\nGenauigkeit beim ersten Durchlauf:\", accuracy(train_labels, Y_hat) )\n",
    "\n",
    "# 4. BACKPROPAGATION\n",
    "grad_W, grad_b = back_prop(train_images, Z_1, A_1, Y_hat, train_targets)\n",
    "\n",
    "# 5. WEIGHTS UPDATEN\n",
    "W, b = update(W, b, grad_W, grad_b, lr = 0.1)\n",
    "```\n",
    "    \n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Loss und die Genauigkeit sind noch gleich. Erst wenn Sie erneut den Input durch das Netzwerk schicken, können Sie den Effekt der veränderten Weights sehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_1, A_1, Y_hat = forward_pass(W,b,train_images)\n",
    "\n",
    "# jetzt berechnen Sie den Loss\n",
    "print(\"Loss beim zweiten Durchlauf: \",calc_loss(Y_hat,train_targets)/Y_hat.shape[0], \"\\nGenauigkeit beim zweiten Durchlauf:\", accuracy(train_labels, Y_hat) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tatsächlich verringert sich der Loss und die Genauigkeit wird besser. Es kann während des Trainings aber auch eine kurzzeitige Verschlechterung auftreten, das ist aber nicht weiter schlimm.  Der Effekt des Trainings wird oft erst nach mehreren Epochs sichtbar. Sie können den Schritt des Trainierens einfach wiederholen.\n",
    "Um dies effizienter zu gestalten, können Sie einfach einen `for-loop` benutzen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.WEIGHTS INITALISIEREN\n",
    "np.random.seed(1234)\n",
    "W, b = init_weights(784, 300, 10)\n",
    "\n",
    "EPOCHS= 50 # wie oft der Datensatz durch das Netzwerk geführt wird\n",
    "for i in range(EPOCHS):\n",
    "    \n",
    "    # 2. FORWARD PROPAGATON\n",
    "    Z_1, A_1, Y_hat = forward_pass(W, b,train_images)\n",
    "    \n",
    "    # 3. LOSS BERECHNEN\n",
    "    loss = calc_loss(Y_hat, train_targets) / Y_hat.shape[0]\n",
    "    acc = accuracy(train_labels, Y_hat)\n",
    "    \n",
    "    print(i, \n",
    "        \"Training Loss: %.2f Training Accuracy: %.2f\"\n",
    "        % (loss, acc)\n",
    "    )\n",
    "    \n",
    "    # 4. BACK PROPAGATION\n",
    "    grad_W, grad_b = back_prop(train_images, Z_1, A_1, Y_hat, train_targets)\n",
    "    \n",
    "    # 5. WEIGHTS UPDATEN\n",
    "    W, b = update(W, b, grad_W, grad_b, lr = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sie sehen schon eine Verbesserung und kommen auf eine Genaugigkeit von 73%. Allerdings dauert das Trainieren sehr lange. Eine größere Lernrate sollte das Training schneller machen. Probieren Sie eine Lernrate von 0,3 aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "W, b = init_weights(784, 300, 10)\n",
    "loss= []\n",
    "EPOCHS= 50 # wie oft der Datensatz durch das Netzwerk geführt wird\n",
    "for i in range(EPOCHS):\n",
    "    Z_1, A_1, Y_hat = forward_pass(W, b,train_images)\n",
    "    \n",
    "    loss= calc_loss(Y_hat, train_targets) / Y_hat.shape[0]\n",
    "    \n",
    "    acc = accuracy(train_labels, Y_hat)\n",
    "    \n",
    "    print(i,\n",
    "        \"Training Loss: %.2f Training Accuracy: %.2f\"\n",
    "        % (loss, acc)\n",
    "    )\n",
    "    \n",
    "    # BACK PROPAGATION\n",
    "    grad_W, grad_b = back_prop(train_images, Z_1, A_1, Y_hat, train_targets)\n",
    "    # WEIGHTS UPDATEN\n",
    "    W, b = update(W, b, grad_W, grad_b, lr = 0.3 )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit einer Lernrate von 0.3 erreichen Sie nach 50 Epochs eine Genauigkeit von 78%. Sie können das Netzwerk noch weiter verbessern indem Sie  noch  länger trainieren. \n",
    "\n",
    "Was müssten Sie am Code ändern, damit für 25 weitere Epochs trainiert wird **ohne, dass das Netzwerk ganz neu trainiert werden muss**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "W, b = init_weights(784, 300, 10)\n",
    "loss= []\n",
    "EPOCHS= 25 # wie oft der Datensatz durch das Netzwerk geführt wird\n",
    "for i in range(EPOCHS):\n",
    "    Z_1, A_1, Y_hat = forward_pass(W, b,train_images)\n",
    "    loss= calc_loss(Y_hat, train_targets) / Y_hat.shape[0]\n",
    "    acc = accuracy(train_labels, Y_hat)\n",
    "    print(\n",
    "        \"Training Loss: %.2f Training Accuracy: %.2f\"\n",
    "        % (loss, acc)\n",
    "    )\n",
    "    \n",
    "    # BACK PROPAGATION\n",
    "    grad_W, grad_b = back_prop(train_images, Z_1, A_1, Y_hat, train_targets)\n",
    "    # WEIGHTS UPDATEN\n",
    "    W, b = update(W, b, grad_W, grad_b, lr = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Lösung:</strong></summary>\n",
    "\n",
    "```python\n",
    "np.random.seed(1234)\n",
    "# W, b = init_weights(784, 300, 10) # Sie dürfen nicht erneut die Gewichte initialisieren\n",
    "loss= []\n",
    "EPOCHS= 25 # wie oft der Datensatz durch das Netzwerk geführt wird\n",
    "for i in range(EPOCHS):\n",
    "    Z_1, A_1, Y_hat = forward_pass(W, b,train_images)\n",
    "    loss= calc_loss(Y_hat, train_targets) / Y_hat.shape[0]\n",
    "    acc = accuracy(train_labels, Y_hat)\n",
    "    print(\n",
    "        \"Training Loss: %.2f Training Accuracy: %.2f\"\n",
    "        % (loss, acc)\n",
    "    )\n",
    "    \n",
    "    # BACK PROPAGATION\n",
    "    grad_W, grad_b = back_prop(train_images, Z_1, A_1, Y_hat, train_targets)\n",
    "    # WEIGHTS UPDATEN\n",
    "    W, b = update(W, b, grad_W, grad_b, lr = 0.3)\n",
    "```\n",
    "    \n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sie dürfen einfach nicht erneut die Weights initialisieren. Sonst verlieren Sie das gesamte Gelernte des Netzwerkes.\n",
    "\n",
    "Durch weiteres Training haben Sie das Netzwerk um 6 % Genauigkeit verbessern können. Grundsätzlich haben Sie noch die Möglichkeit mit der Größe der Hidden Layer etwas zu verändern. \n",
    "\n",
    "Bevor wir das machen, können Sie erst einmal schauen, wie gut unser Modell funktioniert für Bilder, die es noch nicht gesehen hat. Die Rede ist von dem Testdatensatz. Dazu speisen sie den Testdatensatz in das trainierte Netzwerk ein:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, test_y_hat = forward_pass(W, b, test_images) # durch das _,_,test_y_hat werden z_1 und a_1 nicht mit ausgegeben, da wir diese nicht brauchen\n",
    "accuracy(test_labels, test_y_hat)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Genauigkeit auf dem Testdatensatz beträgt auch 85 %. Man kann sagen, dass 85 % der Bilder richtig erkannt wurden.\n",
    "Es ist unüblich, dass Netzwerke besser oder genauso gut für den Testdatensatz funktioniert. Dies ist ein Hinweis darauf, dass Sie das Modell nicht lange genug trainiert haben.\n",
    "\n",
    "Als Nächstes können Sie sich anschauen, bei welchen Bilder das Netzwerk am meisten Probleme hat.\n",
    "Der Code in der nächsten Zelle, sortiert die falsch identifizierten Bilder nach ihre Wahrscheinlichkeit (Dieser Code ist nicht unbedingt leicht zu verstehen, ist aber für das Verständnis von neuronalen Netzwerken auch nicht essenziell):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "falsche_klassifizierung = np.where(test_labels != np.argmax(test_y_hat, axis=1))[0]# welche Bilder wurde falsch klassifiziert\n",
    "len(falsche_klassifizierung) # soviele Bilder wurden falsch klassifiziert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hier werden de Wahrscheinlichkeiten gesammelt die das Model dem Bild zugeorndet hat in der richtige Kategorie zu sein\n",
    "probs = [] \n",
    "for image in falsche_klassifizierung:\n",
    "    probs.append(test_y_hat[image,test_labels[image]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wir sortieren die Bilder Anhand der Wahrscheinlichkeiten, je kleiner die Wahrscheinlichkeit desto sicherer war das Model das das Bild nicht in der richtige Katehorie ist\n",
    "falsche_klassifizierung=falsche_klassifizierung[np.argsort(probs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so sehen 10 Bilder aus die falsch klassifiziert werden\n",
    "for i in range(10):\n",
    "    plt.imshow(test_images[falsche_klassifizierung[i]].reshape([28, 28]), cmap=\"gray\")\n",
    "    plt.show()\n",
    "    print(\n",
    "        \"Predicted Label: %s, Correct Label %s\"\n",
    "        % (\n",
    "            np.argmax(test_y_hat, axis=1)[falsche_klassifizierung[i]],\n",
    "            test_labels[falsche_klassifizierung[i]],\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bei manchen Bilder können Sie klar sehen, warum diese falsch kategorisiert wurden. Bei anderen jedoch ist es für ein menschliches Auge ganz einfach die richtige Ziffer zuerkennen, trotzdem hat das Netzwerk damit Probleme.\n",
    "\n",
    "# Übungsaufgabe\n",
    "\n",
    "Schreiben Sie den obrigen Code so um, dass sowohl Test Accuracy und Test Loss nach jedem Epoch berechnet und ausgegeben werden. Dies soll zusätzlich zu der Trainings Accuracy/Loss passieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "W, b = init_weights(784, 300, 10)\n",
    "loss= []\n",
    "EPOCHS= 25 # wie oft der Datensatz durch das Netzwerk geführt wird\n",
    "for i in range(EPOCHS):\n",
    "    z_1, a_1, y_hat = forward_pass(W, b,train_images)\n",
    "    loss= calc_loss(y_hat, train_targets) / y_hat.shape[0]\n",
    "    acc = accuracy(train_labels, y_hat)\n",
    "    print(\n",
    "        \"Training Loss: %.2f Training Accuracy: %.2f\"\n",
    "        % (loss, acc)\n",
    "    )\n",
    "    \n",
    "    # wir machen die backpropagation\n",
    "    grad_W, grad_b = back_prop(train_images, z_1, a_1, y_hat, train_targets)\n",
    "    # und mit den Gradienten updaten wir die Weights\n",
    "    W, b = update(W, b, grad_W, grad_b, lr = 0.3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
