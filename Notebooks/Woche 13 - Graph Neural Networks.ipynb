{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f5efb70",
   "metadata": {},
   "source": [
    "# Graph Neural Networks\n",
    "\n",
    "---\n",
    "**Lernziele**\n",
    "* Sie verstehen, wie Moleküle für den Computer als Graph dargestellt werden können. \n",
    "* Sie verstehen, wie Graph Neural Networks funktionieren.\n",
    "* Sie können ein Graph Neural Network als Pytorch Klasse schreiben.\n",
    "---\n",
    "\n",
    "Graph Neural Networks sind noch eine relativ neue Methode. Intuitiv können wir sehen, dass sich Moleküle sehr gut als (mathematischen) Graph darstellen lassen. Dabei entsprechen die Bindungen im Molekül den Kanten (edges) des Graph und die Atome  den Knoten (nodes).<br>Für den Computer sind jedoch Graphen nicht so leicht zu lesen wie zum Beispiel ein SMILES string. Wir stellen die Moleküle als zwei Matrizen dar. Zum einen die Adjacency-Matrix, die die Atomverknüpfung (also die Bindungen) darstellt. Dazu kommen dann noch die Feature Matrix. Für das Machine Learning One-Hot encoden wir hier die Atome.<br><br>\n",
    "<center>\n",
    "<img src=\"https://www.researchgate.net/profile/Jorge_Galvez2/publication/236018587/figure/fig1/AS:299800013623305@1448489301609/The-chemical-graph-and-adjacency-matrix-of-the-isopentane.png\" style=\"width: 600px;\">\n",
    "</center>\n",
    "<h8><center>Galvez et. al. 2010</center></h8><br><br>\n",
    "Wir nehmen nochmal die Daten aus der Tox21 Challenge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdc9870f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.rdmolops import GetAdjacencyMatrix\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils import data\n",
    "import math\n",
    "from sklearn.metrics import roc_auc_score\n",
    "%run ../utils/onehotencoder.py\n",
    "np.set_printoptions(linewidth=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2024983d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OC(=O)[C@H](O)[C@@H](O)[C@H](O)C(=O)CO</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C[C@]12CC[C@H]3[C@@H](CCc4cc(O)ccc43)[C@@H]1CC...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CC(C)(C)c1cc(O)ccc1O</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CN(C)c1ccc(cc1)C(c1ccccc1)=C1C=CC(C=C1)=[N+](C)C</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NC(Cc1ccccc1)C(O)=O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              smiles  activity\n",
       "0             OC(=O)[C@H](O)[C@@H](O)[C@H](O)C(=O)CO         0\n",
       "1  C[C@]12CC[C@H]3[C@@H](CCc4cc(O)ccc43)[C@@H]1CC...         1\n",
       "2                               CC(C)(C)c1cc(O)ccc1O         1\n",
       "3   CN(C)c1ccc(cc1)C(c1ccccc1)=C1C=CC(C=C1)=[N+](C)C         1\n",
       "4                                NC(Cc1ccccc1)C(O)=O         0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Laden der Daten\n",
    "data_tox = pd.read_csv(\"../data/toxicity/sr-mmp.tab\", sep = \"\\t\")\n",
    "data_tox = data_tox.iloc[:,1:] #alle Spalten bis auf die erste (index 0) werden ausgewählt\n",
    "data_tox.columns = [\"smiles\", \"activity\"]\n",
    "data_tox.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1321a720",
   "metadata": {},
   "source": [
    "## Adjacency Matrix und One-Hot Encoded Feature Matrix\n",
    "Mit den Smiles können wir diesmal leider  nicht so viel mit anfangen. Aber zum Glück gibt es in RDKit direkt eine Funktion, um die Adjacency Matrix zu erstellen. Diese haben wir oben schon importiert.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "120a224f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "mols = [Chem.MolFromSmiles(x) for x in data_tox['smiles']]\n",
    "A = [GetAdjacencyMatrix(x) for x in mols]\n",
    "print(A[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c0e03f",
   "metadata": {},
   "source": [
    "Um die Atome zu One-Hot Encoden benutzen wir die vorgeschriebene `onehotencode()` Funktion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e9fb9b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat = onehotencode(mols)\n",
    "feat[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2fd903",
   "metadata": {},
   "source": [
    "Vielleicht ist ihnen aufgefallen, dass auf der Diagonalen der Adjacency Matrix noch Nullen stehen. In einer Graph Convolution sollen aber nicht nur die Features der benachbarten, sondern auch des zentralen Atoms mit in die Rechnung genommen werden. Dafür werden Einsen auf der Diagonalen der Adjacency-Matrix benötigt. \n",
    "Die Funktion `np.fill_diagonal(matrix, value)` erlaubt es Ihnen die Werte der Diagonalen einer Matrix zu ändern. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c81e5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "for matrix in A:\n",
    "    np.fill_diagonal(matrix, 1)\n",
    "print(A[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362ff2ee",
   "metadata": {},
   "source": [
    "## Graph Convolution\n",
    "Jetzt wollen wir die Informationen der Knoten an die Nachbarknoten entlang der Kanten weiterzugeben, was als (Graph) Convolution bezeichnet wird. Hierfür werden während des forward pass diese Mathematischen Operationen des ausgeführ: $$\\hat{X} = \\hat{D}^{-1}\\hat{A}XW$$\n",
    "Hier ist $\\hat{A}$ unsere Adjacency Matrix mit Einsen auf der Diagonalen. $X$ ist die Feature-Matrix und $W$ sind die Weights, die pytorch initialisiert. Es fehlt noch $D$, die Degree Matrix. Diese Matrix enthält die Anzahl der Verbindungen, die jedes Atom im Molekül hat. Diese Werte werden in der Degree-Matrix auf der Diagonalen platziert. Die Anzahl der Verbindung jedes Moleküls lässt sich leicht aus der Adjacency-Matrix berechnen. Dafür müssen Sie die Summe der Reihe oder Spalten der$\\hat{A}$ Matrix berechnen. Diese Summe gibt die Degrees eines Atom an. Um genau zu sein, ist es der Degree + 1 des Atoms, da Sie die Diagonale der Adjacency-Matrix bereits mit Einsen gefüllt haben.\n",
    "Um die Degree-Matrix zu erstellen, müssen Sie die Summen der Reihen in jeder Adjacency-Matrix berechnen und dieses auf die Diagonale einer neuen Matrix platzieren.\n",
    "\n",
    "\n",
    "Dieser Prozess ist relativ simpel mit `numpy`. `np.sum(A[i], axis=0)` berechnet die Summe pro Spalte und `np.diag()` erstellt aus einem 1D-Array eine Matrix mit den Werten des 1D Arrays auf der Diagonalen. Sie können die beiden Funktion in Kombination benutzen, um die Degree-Matrizen zu erstellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c61bef88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]]\n"
     ]
    }
   ],
   "source": [
    "D =[]\n",
    "for matrix in A:\n",
    "    D.append(np.diag(np.sum(matrix, axis=1)))\n",
    "print(D[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb2560b",
   "metadata": {},
   "source": [
    "Bevor wir anfangen ein Netzwerk zu bauen und es mit unseren Daten zu füttern, können wir noch einen Schritt durchführen um einiges an Rechenaufwand zu sparen, denn $\\hat{D}^{-1}\\hat{A}$ kann schon vor dem Trainieren des Netzwerk berechnet werden. Dadurch kann Zeit gespart werden, da dieser Schritt nicht ständig im Netzwerk wiederholt werden muss.\n",
    "\n",
    "`np.linalg.matrix_power` wird benötigt, um das Invers der Matrix `D` zu berechnen. Ansonsten ist dieser Schritt eine einfache Matrixmultiplikation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c0159329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5       , 0.5       , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.25      , 0.25      , 0.25      , 0.25      , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.5       , 0.5       , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.25      , 0.        , 0.25      , 0.25      , 0.25      , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.5       , 0.5       , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.25      , 0.        , 0.25      , 0.25      , 0.25      , 0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        , 0.5       , 0.5       , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        , 0.25      , 0.        , 0.25      , 0.25      , 0.25      , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.5       , 0.5       , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.25      , 0.        , 0.25      , 0.25      , 0.25      , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.5       , 0.5       , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.33333333, 0.        , 0.33333333, 0.33333333],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.5       , 0.5       ]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DA = []\n",
    "for i in range(len(D)):\n",
    "    DA.append(np.matmul(np.linalg.matrix_power(D[i], -1),A[i]))\n",
    "DA[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1fe5a8",
   "metadata": {},
   "source": [
    "Wir haben also die Liste der Adjacency Matrizen `DA`. Diese enthält die Informationen über die Struktur des Moleküles. Die Liste `feat` enthät die Features, also die Informationen zu den einzelnen Knoten. Und als letztes erstellen wir noch eine `labels` Liste. Diese enthät die Information die wir vorhersagen wolllen (toxisch vs. nicht toxisch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "040c982e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Janos\\anaconda3\\envs\\intro_kiv2\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "DA = [torch.tensor(x,dtype=torch.float32) for x in DA] # Konvertieren der Arrays zu Tensoren\n",
    "feat = [torch.tensor(x,dtype=torch.float32) for x in feat] # Konvertieren der Arrays zu Tensoren\n",
    "labels = [torch.tensor([x], dtype=torch.float32) for x in data_tox['activity']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bd3802",
   "metadata": {},
   "source": [
    "## Graph Convolution Layer\n",
    "Wir wollen uns für die Graph Convolution die Sturktur von PyTorch zunutze machen. Wie schon letzte Woche verwenden wir dafür Klassen.\n",
    "Letze Woche haben wir eine Klasse benutzt um aus verschiedene Layern (z.B. `nn.Linear()`)ein Netzwerk zu erstellen. \n",
    "Diese Layers sind in pyTorch schon vorgeschrieben, aber auch wir können unsere eigene Layer erstellen. Auch hierfür brauchen wir wieder die PyTorch Klasse.\n",
    "Unser Ziel ist es, so etwas wie nn.Linear() zu programmieren, sodass wir unser Repertoire an Hidden Layers (Linear, RNN, GRU, Dropout, ...) um eine Graph Convolution Layer erweitern.<br>Auch dafür können wir die nn.Module Klasse als Basis für unsere GraphConvolution verwenden. Wir fangen mit dem minimalen Grundgerüst an:\n",
    "```python\n",
    "class GraphConvolution(nn.Module):\n",
    "    pass\n",
    "```\n",
    "Damit wir richtig auf die Funktionen der übergeordneten nn.Module Klasse zugreifen können müssen wir diese mit `super().__init__()` initialisieren. Unsere Convolutional Layer muss natürlich wissen, wie groß der input ist, und wie groß der output werden soll. Außerdem müssen wir hier selbst die Weights und Biases initialiseren.\n",
    "```python\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.in_featuers = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        self.bias = nn.Parameter(torch.FloatTensor(out_features))\n",
    "```\n",
    "Dann können wir auch schon die forward() Funktion programmieren. Hier passiert die eigentliche convolution, bei der die Knoten mit den Features `x` durch Matrixmultiplikation mit der Adjacency Matrix `adj` über ihre Nachbarknoten lernen. Indem wir noch die lernbaren `weights` dazwischenschalten, können wir das ganze später optimieren.\n",
    "```python\n",
    "    def forward(self, x, adj):\n",
    "        support = torch.mm(x, self.weight)\n",
    "        output = torch.mm(adj, support)\n",
    "        return output + self.bias\n",
    "```\n",
    "Damit würde unsere GraphConvolution Klasse auch schon funktionieren. Wir initialisieren die weights aber noch etwas optimiert mit der Funktion `reset_parameters()`. Und mit der Funktion `__repr__()` können wir uns die Layer noch richtig anzeigen lassen. Unsere finale Klasse sieht dann so aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "34e85d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        self.bias = nn.Parameter(torch.FloatTensor(out_features))\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(0, stdv)\n",
    "        self.bias.data.uniform_(-stdv, stdv)\n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        support = torch.mm(x, self.weight)\n",
    "        output = torch.mm(adj, support)\n",
    "        return output + self.bias\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + 'in_features=' + str(self.in_features) + ', ' \\\n",
    "               + 'out_features=' + str(self.out_features) + ')'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8d289f",
   "metadata": {},
   "source": [
    "Wir können auch überprüfen ob unsere Graph Convolution Layer auch schon funktioniert.\n",
    "Zunächst speichern wir eine Feature Matrix und eine Adjacency Matrix als Beispiel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fc2d1fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: torch.Size([29, 25])\n",
      "DA: torch.Size([29, 29])\n"
     ]
    }
   ],
   "source": [
    "feat_beispiel = feat[1]\n",
    "adj_beispiel = DA[1]\n",
    "print('Features:', feat_beispiel.shape)\n",
    "print('DA:', adj_beispiel.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab0b55b",
   "metadata": {},
   "source": [
    "Wir erstellen nun eine GraphConvolution. Beachten Sie, dass die Inputgröße der Layer dem Größe des Featurevektors (`29`) entspricht. Also genauso wie bei einer `nn.Linear` Layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c73a6370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphConvolution (in_features=25, out_features=100)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = GraphConvolution(25, 100)\n",
    "conv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab305ee3",
   "metadata": {},
   "source": [
    "Wir können nun das Beipsiel durhc die COnvolution führen. Sie werden sehen, dass sich die Dimension der Featueres auf 100 vergrößert haben wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5a1c2cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output: torch.Size([29, 100])\n"
     ]
    }
   ],
   "source": [
    "output = conv(feat_beispiel, adj_beispiel)\n",
    "print('\\nOutput:', output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9417a666",
   "metadata": {},
   "source": [
    "## Graph Neural Network\n",
    "Um jetzt ein Netz zu bauen, können wir wieder die nn.Module Basisklasse verwenden. \n",
    "\n",
    "Letze Woche haben wir, unseren eigenen Autoencoder zusammen gestelllt. Diese Woche benutzen wir diese Art von KLasse um mehrere Graph Convolutions miteinander zu verbinden. Wichitg ist aber auch zu beachten, dass wir wie bei regulären Convolution Neural Networks nicht nur Convolutional Layers benutzen können. Im CNN selber, müssen wir am Ende den tensor flatten. Dadruch erhalten wir einen Vektor der dann durch eine Linear Layer geführt werden kann. Das gleiche gilt auch für Graph Convolutions. Um den Output dieser Graph COnvolutions in eine Linear Layer zu führen, können wir aber nicht `Flatten` benutzen, sondern wir berechnen einfach den Mittelwert über alle Spalten.\n",
    "\n",
    "Das macht die Funktion `aggregate()`.\n",
    "\n",
    "Sie können im Netzwerk erkennen, dass wir zwei Graph Convolution Layers benutzen, und dann eine Linear Layer.\n",
    "Im `forward` pass wird erst der Input `(x, adj)` durch die erste Graph Layer geführt, dann folt eien ReLU Funktion. Dann wiederholt sich das selber SPiel für die zweite Convolution. \n",
    "Nun wenden wir die `aggregate` Funktion an. Diese berechnet nun den Mittelwert eines jeden Features. Der Output dieser Layer hat also in dem Beispiel die Größe `[1,100]`.\n",
    "Als letztes benutzen wir die Linear Layer um die Prediction zu machen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "63ed67d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphNN(nn.Module):\n",
    "    def __init__(self):#in_features, out_features, size_labels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GraphConvolution(25, 100)\n",
    "        self.conv2 = GraphConvolution(100, 100)\n",
    "        self.lin = nn.Linear(100, 1)\n",
    "        \n",
    "    def aggregate(self, convoluted_graph): # we use mean aggregation, max or min could also be used as hyperparameter\n",
    "        return torch.mean(convoluted_graph, dim=0, keepdim=True)\n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        x = self.conv1(x, adj)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, adj)\n",
    "        x = F.relu(x)\n",
    "        x = self.aggregate(x)\n",
    "        x = self.lin(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ec1ef789",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training und Evaluation\n",
    "\n",
    "train_feat = feat[:1800]\n",
    "train_DA = DA[:1800]\n",
    "train_labels = labels[:1800]\n",
    "\n",
    "\n",
    "test_feat = feat[1800:]\n",
    "test_DA = DA[1800:]\n",
    "test_labels = labels[1800:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f3b44562",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn = GraphNN()\n",
    "loss_funktion= nn.BCEWithLogitsLoss()\n",
    "optimizer=optim.Adam(gnn.parameters(), lr =0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5a75bda0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss: 0.59 Train Accuracy: 0.70 Test Loss: 0.60 Test Accuracy: 0.69\n",
      "1 Train Loss: 0.58 Train Accuracy: 0.70 Test Loss: 0.59 Test Accuracy: 0.72\n",
      "2 Train Loss: 0.58 Train Accuracy: 0.70 Test Loss: 0.59 Test Accuracy: 0.71\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2336/3985946759.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mloss_list_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0macc_list_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\intro_kiv2\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\intro_kiv2\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\intro_kiv2\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    116\u001b[0m                    \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m                    \u001b[0mweight_decay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'weight_decay'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m                    eps=group['eps'])\n\u001b[0m\u001b[0;32m    119\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\intro_kiv2\\lib\\site-packages\\torch\\optim\\_functional.py\u001b[0m in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[0;32m     92\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m             \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    loss_list_train = []\n",
    "    acc_list_train= []\n",
    "    gnn.train()\n",
    "    for k in range(len(train_feat)):\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        output=gnn(train_feat[k], train_DA[k]).flatten()\n",
    "\n",
    "        loss=loss_funktion(output,train_labels[k])\n",
    "        loss.backward()\n",
    "        loss_list_train.append(loss.item())\n",
    "        optimizer.step()\n",
    "\n",
    "        acc_list_train.append(np.sum((torch.round(torch.sigmoid(output)) == train_labels[k]).detach().numpy()))\n",
    "    loss_list_test = []\n",
    "    acc_list_test= []\n",
    "    gnn.eval()\n",
    "    for k in range(len(test_feat)):\n",
    "    \n",
    "        output=gnn(test_feat[k], test_DA[k]).flatten()\n",
    "\n",
    "        loss=loss_funktion(output,test_labels[k])\n",
    "        loss_list_test.append(loss.item())\n",
    " \n",
    "\n",
    "        acc_list_test.append(np.sum((torch.round(torch.sigmoid(output)) == test_labels[k]).detach().numpy()))\n",
    "            \n",
    "        \n",
    "    print(i,\"Train Loss: %.2f Train Accuracy: %.2f Test Loss: %.2f Test Accuracy: %.2f\"\n",
    "        % (np.mean(loss_list_train), np.mean(acc_list_train),np.mean(loss_list_test), np.mean(acc_list_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d732ba",
   "metadata": {},
   "source": [
    "PyG hat schon Convolutional, Pooling etc Layers und Batching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe796496",
   "metadata": {},
   "source": [
    "## Übungsaufgabe: ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23046cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
