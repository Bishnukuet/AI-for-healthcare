{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84bb42a8",
   "metadata": {},
   "source": [
    "# Lineare Algebra\n",
    "\n",
    "Heute werden wir die essentiellen mathematische Grundlagen für Neuronale Netzwerke erklären.\n",
    "\n",
    "Das erste mathematische Konzept notwenig ist der **Vektor**\n",
    "Ein Vektor ist Sammlung von mehrer Werte, und wird wie folgt definiert\n",
    "\n",
    "$$\\begin{bmatrix}3 & 4 & 0.5\\end{bmatrix}$$ \n",
    "Dieser Vektor enthält genau drei Werte. Mit Vektoren können wir einzelne Datenpunkt beschrieben. Zum Beispiel könnten wir die Daten eines Hauses in diesem Vektor speichern. Der erste Werte gibt an viele Bäder das Haus hatz, der zweite wie viele Schlafzimmer, und der dritte Wert gibt das Alter der Heizung in Jahren an.\n",
    "\n",
    "Ihnen ist bestimmt aufgefallen, dass ein Vektor erstaunlich Ähnlichkeiten zum einem 1-dimensionalen `array` hat.\n",
    "`array([3,4,0.5])`. Tatsächlich, sollen `np.arrays` die gleiche Funktionen wie Vektoren haben. Die mathematischen Regeln die für Vektoren gelten, gelten auch für die `arrays`.\n",
    "\n",
    "\n",
    "\n",
    "Wir können zum Beispiel einen Vektor mit einer Zahl multiplizieren:\n",
    "*Für bessere Übersicht schreiben wir den Vektor untereinander*\n",
    "$$3\\cdot\\begin{bmatrix}3 \\\\ 4 \\\\ 0.5\\end{bmatrix}= \\begin{bmatrix}3\\cdot 3 \\\\ 4 \\cdot 3 \\\\ 0.5 \\cdot 3\\end{bmatrix}= \\begin{bmatrix}9 \\\\ 12 \\\\ 1.5\\end{bmatrix} $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bade34bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9. , 12. ,  1.5])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array([3,4,0.5])*3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47439371",
   "metadata": {},
   "source": [
    "Gleiches gilt auch für Addition und Substraktion:\n",
    "$$3+\\begin{bmatrix}3 \\\\ 4 \\\\ 0.5\\end{bmatrix}= \\begin{bmatrix}3+3 \\\\ 4+3 \\\\ 0.5 + 3\\end{bmatrix}= \\begin{bmatrix}6 \\\\ 7 \\\\ 3.5\\end{bmatrix} $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d3338b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6. , 7. , 3.5])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3+np.array([3,4,0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73698ce3",
   "metadata": {},
   "source": [
    "Auch können wir zwei Vektoren addieren:\n",
    "    \n",
    "    \n",
    "$$\\begin{bmatrix}3 \\\\ 4 \\\\ 0.5\\end{bmatrix} + \\begin{bmatrix}0.3 \\\\ 3 \\\\ -0.2\\end{bmatrix} = \\begin{bmatrix}3 +0.3 \\\\ 4+3 \\\\ 0.5-0.2\\end{bmatrix} =  \\begin{bmatrix}3.3 \\\\ 7 \\\\ 0.3\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fa694c4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.3, 7. , 0.3])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([3,4,0.5])+ np.array([0.3,3,-0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dde4019",
   "metadata": {},
   "source": [
    "Interessant werden Vektoren erst, wenn wir mehrere miteinander multiplizieren:\n",
    "\n",
    "$$\\begin{bmatrix}3 \\\\ 4 \\\\ 0.5\\end{bmatrix} \\cdot \\begin{bmatrix}0.3 \\\\ 3 \\\\ -0.2\\end{bmatrix} $$\n",
    "\n",
    "Vorallem das sogennante Skalarprodukt ist vorallem wichtig und wird wie folgt berechnet:\n",
    "$$\\begin{bmatrix}3 \\\\ 4 \\\\ 0.5\\end{bmatrix} \\cdot \\begin{bmatrix}0.3 \\\\ 3 \\\\ -0.2\\end{bmatrix} = (3\\cdot 0.3) + (3 \\cdot 4)+ (0.5\\cdot -0.2) = 12.8  $$\n",
    "\n",
    "\n",
    "Berechnen Sie das Skalarproduct für die beiden Vektoren per Hand: \n",
    "\n",
    "$$\\begin{bmatrix}8 \\\\ 0.25 \\\\ -1\\end{bmatrix} \\cdot \\begin{bmatrix}0.1 \\\\ 12 \\\\ 8\\end{bmatrix} = $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ece16ec",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Lösung. HIER klicken</strong></summary>\n",
    "\n",
    "$$\\begin{bmatrix}8 \\\\ 0.25 \\\\ -1\\end{bmatrix} \\cdot \\begin{bmatrix}0.1 \\\\ 12 \\\\ 8\\end{bmatrix} =(8\\cdot 0.1) + (0.25 \\cdot 12)+ (-1\\cdot 8) = -4.2  $$\n",
    "</details>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cd19c9",
   "metadata": {},
   "source": [
    "\n",
    "In `numpy` benutzen wir `np.dot()`, um das Skalarprodukt zu berechnen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e3ecd4fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.8"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(np.array([3,4,0.5]), np.array([0.3,3,-0.2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4fa878",
   "metadata": {},
   "source": [
    "Wie Ihnen vielleicht schon aufgefallen ist, ähnelt das Skalarprodukt einer linearen Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d89a637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.8"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x    = np.array([3,4,0.5])\n",
    "beta = np.array([0.3,3,-0.2])\n",
    "np.dot(x,beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a92937",
   "metadata": {},
   "source": [
    "`x` ist der Inputvektor der die Informationen für drei Variablen enthät. Zum Beispiel für ein Haus, das `3` Bäder und `4` Schlafzimmer hat. Es wurde auch erst vor einem halben Jahr (`0.5`) renoviert. Der zweite Vektor enthält die Koeffzienten der Regression. Also $\\beta_1, \\beta_2, \\beta_3$. Mit der Regression können wir dann den Wert des Hauses in 100.000 € ermitteln. \n",
    "\n",
    "Effektive führt das Skalarprodukt zu einer vereinfachung der Formel. Anstatt zu schreiben:\n",
    "$$\\hat{y} = \\beta_1x_1 +\\beta_2x_2 +\\beta_3x_3$$\n",
    "können wir die Formel auch so schreiben.\n",
    "\n",
    "$$\\hat{y} = x\\beta$$\n",
    "\n",
    "Hier muss angenommmen werden, dass $x$ und $\\beta$ Vekotren sind. \n",
    "Es feht natürlch immer nocht das $t$ oder auch $\\beta_0$. Also der y-Achsenabschnitt. Wie oben erklärt, können einzelne Werte einfach zu Vektoren addiert werden. \n",
    "\n",
    "Die komplette Formel wird deshalb:\n",
    "\n",
    "$$\\hat{y} = x\\beta+\\beta_0$$\n",
    "\n",
    "Können Sie diese Formel mit `numpy` schreiben. Berechenen Sie $\\hat{y}$ für `x`. Hierbei ist $\\beta_0=-5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d66dfef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.800000000000001"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_0 =-5\n",
    "y_hat = _____________________\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ba9fb9",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Lösung. HIER klicken</strong></summary>\n",
    "\n",
    "```python\n",
    "y_hat = np.dot(x,beta)+beta_0\n",
    "    \n",
    "```\n",
    "</details>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dceecd13",
   "metadata": {},
   "source": [
    "Angenommen, wir wollen nun nicht nur `y_hat` für eine Haus bestimmen sondern für mehrere Häuser gleichzeitig, dann geht das mit genau der selben Formel. \n",
    "\n",
    "`X` enthält nun nicht nur einen Vektor, sondern gleich mehrere. Wie Sie schon gelernt haben, können solche datenstrukturen als 2D-Array gespeichert werden. Ein 2D-Array ist mit einer Matrix in der Mathematik vergleichber. \n",
    "\n",
    "Wenn wir von Matrizen sprechen, benutzen kapitalisierte Variablennamen um zu kennzeichen, dass wir von einer Matrix sprechen.\n",
    "\n",
    "Unten ist `X` gegeben. Sie können sehen, dass `np.dot(X,beta) + beta_0` immer noch das richtige Ergebnis lieftert. Diesmal aber für jeder der 4 Reihen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "98d219b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7.8  , -1.64 ,  2.176,  4.5  ])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[3,4,0.5],\n",
    "              [2,1,1.2],\n",
    "              [4,2,0.12],\n",
    "              [3,3,2]])\n",
    "\n",
    "np.dot(X,beta) + beta_0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4a5fce",
   "metadata": {},
   "source": [
    "---\n",
    "Die Notation $\\beta$s kommt aus der traditionellen Statistik. Im machinellen Lernen werden die Koeffizieten mit   $w$, für \"weights\", gekennzeichnet. Darüber hinaus wird $\\beta_0$, der y-Achesenabschnitt, als $b$ (Bias) bezeichnet.\n",
    "Die Regressionsgleichung ist deshalb:\n",
    "\n",
    "$$Xw+b$$\n",
    "\n",
    "Wir werden ab jetzt diese Notation beibehalten.\n",
    "\n",
    "---\n",
    "\n",
    "Wie Sie gelernt haben, ist die Stärke von Neuronalen Netzwerken, das Ausführen von mehr als nur einer Regression gleichzeitig.\n",
    "Das heißt, wir haben nicht nur eine Reihe von Regressionskoeffizieten sondern mehrerer. Wie viele?\n",
    "Das ist Ihnen selber überlassen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7439e9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "W =  np.array([beta,\n",
    "              [6,0,-2],\n",
    "              [1,0,3],\n",
    "              [0,0,-1],\n",
    "              [1,2,-1]])\n",
    "b = np.array([beta_0,3,2,0.5,-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8aa79a",
   "metadata": {},
   "source": [
    "`W` enthält nun die Gewichte für insgesamt fünf lineare Regressionen. Die erste Reihe enthält  noch unsere `beta` Koeffizienten aus der initialne Regression.  Jede weitere Reihe enthält neue Koeffizienten/Gewichte für eine weiter Regression. An Hand der Anzahl der Reihen, können wir also erkennen wieviele Regressionen wir machen. \n",
    "Auch `b` enthält fünf Werte. Für jede Regressionen enhält er den y-Achsenabschnitt\n",
    "\n",
    "In einem neuronalen Netzwerk bedeutet das auch wieviele Nodes wir in der Hidden Layer haben werden!\n",
    "\n",
    "Wollen wir jetzt mit diesen beiden Matrizen rechnen, passiert folgendes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "12cddbbd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (4,3) and (5,3) not aligned: 3 (dim 1) != 5 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2344102/2383651950.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (4,3) and (5,3) not aligned: 3 (dim 1) != 5 (dim 0)"
     ]
    }
   ],
   "source": [
    "np.dot(X,W)+b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b892e71a",
   "metadata": {},
   "source": [
    "Eine Fehlermeldung:\n",
    "\n",
    "```shapes (4,3) and (5,3) not aligned: 3 (dim 1) != 5 (dim 0)```\n",
    "\n",
    "Tatsächlich können wir aus der Fehlermeldung schließen, was das Problem ist. \n",
    "Zunächst werden uns die Dionensionen (Anzahl der Reihen und Spalten) ausgegeben. \n",
    "`X` `4` Reihen und `3` Spalten. `W` hat `5` Reihen und `3` Spalten. \n",
    "\n",
    "Darauf folgt: `3 (dim 1) != 5 (dim 0)`. Also, `3 (dim 1)`, die Anzahl der Spalten (`3 (dim 1)` der ersten Matrix sind ungleich (`!=`) der Anzahl an Reihen der zweiten Spalte (`5 (dim 0)`).  \n",
    "\n",
    "**Die Anzahl der Spalten der ersten Matrix sollten gleich der Anzahl der Reihen in der zweite Spalte sein.**\n",
    "\n",
    "Wenn wir zum Beispiel, die `W` Matrix umdrehen also Reihen als Spalten und Spalten als Reihen, dann würden Anzhal der Spalten und Reihen gleich sein.\n",
    "\n",
    "Das kovertieren von Spalten zu Reihen und umgekehrt, nennt sich das *Transpose* einer Matrix.\n",
    "`W.tranpose()` fürt diese Transformation aus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "05c09581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.3  3.  -0.2]\n",
      " [ 6.   0.  -2. ]\n",
      " [ 1.   0.   3. ]\n",
      " [ 0.   0.  -1. ]\n",
      " [ 1.   2.  -1. ]] \n",
      "\n",
      "[[ 0.3  6.   1.   0.   1. ]\n",
      " [ 3.   0.   0.   0.   2. ]\n",
      " [-0.2 -2.   3.  -1.  -1. ]]\n"
     ]
    }
   ],
   "source": [
    "print(W, \"\\n\")\n",
    "print(W.transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638e4fc1",
   "metadata": {},
   "source": [
    "Wie Sie sehen, werden aus den Reihen Spalten. Das führt auch dazu, dass sich die Dimension der Matrix ändern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5a71f733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 3) \n",
      "\n",
      "(3, 5)\n"
     ]
    }
   ],
   "source": [
    "print(W.shape, \"\\n\")\n",
    "print(W.transpose().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1c1d09",
   "metadata": {},
   "source": [
    "Mit dem Tarnspose der Matrix `W` sollte die Multiplikation der beiden Matrizen funktionieren, da jetzt die Anzahl der Spalten/Reihen identisch ist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "24fed361",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.8  , 20.   ,  6.5  ,  0.   ,  8.5  ],\n",
       "       [-1.64 , 12.6  ,  7.6  , -0.7  ,  0.8  ],\n",
       "       [ 2.176, 26.76 ,  6.36 ,  0.38 ,  5.88 ],\n",
       "       [ 4.5  , 17.   , 11.   , -1.5  ,  5.   ]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(X,W.transpose())+b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f09f89",
   "metadata": {},
   "source": [
    "Tatsächlich klappt es. Schauen Sie sich zum Beispiel, die erste Spalte an. Diese Werte sind nämlich die Ergebnisse der ersten Regression die wir berechnet haben: `np.dot(X, beta)+beta_0`.\n",
    "Tatsächlich enthält jede Reihe die fünf Regressionsergebnisse für jeweils eins der vier Häuser.\n",
    "\n",
    "Aber wie kann es sein, dass die Regression funktioniert, obwohl wir die `W` Matrix umgedreht haben.\n",
    "\n",
    "Das liegt daran wie eine Matrixmultiplikation definiert ist. Es werden nicht Skalarprodukt zwischen korrespondierenden Reihen beerechnet. Sondern, Skalarprodukt werden zwischen den Reihen der ersten Matrix und den Spalten der zweiten Matrix berechnet. \n",
    "\n",
    "\n",
    "![Matthew Scroggs](https://www.mscroggs.co.uk/img/full/multiply_matrices.gif)\n",
    "<center>Credit: Matthew Scroggs - 2020 | www.mscroggs.co.uk/blog/73 |</center>\n",
    "\n",
    "Tatsächlich ist das auch schon fast alles was was für den Forward Pass gebraucht wird.\n",
    "\n",
    "---\n",
    "\n",
    "Bis jetzt haben wir immer `np.dot()` für eine Matrixmultiplikation benutzt. Tatsächlich gibt es extar eine Funktion `np.matmul()`. Für große Matrizen ist `np.matmul` schneller und wir werden deswegen auch diese Funktion benutzen. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "99be14e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.8  , 20.   ,  6.5  ,  0.   ,  8.5  ],\n",
       "       [-1.64 , 12.6  ,  7.6  , -0.7  ,  0.8  ],\n",
       "       [ 2.176, 26.76 ,  6.36 ,  0.38 ,  5.88 ],\n",
       "       [ 4.5  , 17.   , 11.   , -1.5  ,  5.   ]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(X,W.transpose())+b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef442f1",
   "metadata": {},
   "source": [
    "# Ableitungen\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ba91b1",
   "metadata": {},
   "source": [
    "Um zu verstehen wie neuronale Netzwerke lernen sollten Sie zumindest in groben Zügen verstehen was Ableitung aussagen und wie man sie berechnen kann.\n",
    "\n",
    "Die Ableitung einer Funktion beschreibt die Steigung der ursprünglichen Funktion. \n",
    "Angenommen es gibt eine Funktion $f(x)=x^2$. Dann ist die dazugehörige Ableitung $\\frac{df}{dx}=2x$ (spich: *Ableitung von f nach x*). \n",
    "\n",
    "Im Bild sind sowohl $f(x)$ (*blau*) also auch die Ableitung $\\frac{df}{dx}$ (*orange*) eingezeichnet. Zum Beispiel für $x=-5$ ist $f(-5) = 25$. Die Steiung an diesem Punkt ist: $\\frac{df(-5)}{dx}=2\\cdot -5= -10$. Das heißt die Steigung der FUnktion $f(x)=x^2$ ist $-10$ wenn $x=-5$ ist.\n",
    "\n",
    "<img src=\"Img/lin_alg/ableitung_1.png\"></img>\n",
    "\n",
    "Es gitb einige Regeln zu Ableitung. Wir werden nur zwei davon besprechen. \n",
    "        $$f(x) = x^n \\rightarrow \\frac{df}{dx} = n \\cdot x^{n-1}$$\n",
    "        $$f(x) = x^2 \\rightarrow \\frac{df}{dx} = 2 \\cdot x^{2-1}=2x^1= 2x $$\n",
    "        \n",
    "\n",
    "Grundsätzlich fallen Konstanten immer in Ableitungen weg.\n",
    "\n",
    "Das heißt:\n",
    "Die Ableitung von $f(x)=x^2 + 5$ ist trozudem nur $2x$, da Konstanten die Funktion nur verschieben aber nicht in ihre Steigung beeinflußen. \n",
    "\n",
    "Anders werden Koeffizienten gehandhabt:\n",
    "\n",
    "$$f(x) = ax^n \\rightarrow \\frac{df}{dx} = (n \\cdot a)\\cdot x^{n-1}$$\n",
    "\n",
    "Ein Beispiel:\n",
    "\n",
    "$$f(x) = 4x^3 \\rightarrow \\frac{df}{dx} = 12x^2$$ \n",
    "\n",
    "\n",
    "**Probieren Sie folgende Funktionen abzuleiten (wahrscheinlich einfacher auf einem Papier):**\n",
    "\n",
    "$$g(x)= 7x^5 - 3$$\n",
    "\n",
    "$$h(x)= 0.5x^2 + 3x +12$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f604bd",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Lösung. HIER klicken</strong></summary>\n",
    "\n",
    "$$\\frac{dg}{dx}35x^4 $$\n",
    "$$\\frac{dh}{dx}x +3$$\n",
    "\n",
    "</details>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd549dd",
   "metadata": {},
   "source": [
    "# Kettenregel \n",
    "\n",
    "Die wichtigste Regel für Neuronale Netzwerke ist die Kettenregel. An Hand der Formel ist sie schwierig zu verstehen, doch an Hand eines Beispiel sollte es relativ einfach sein. \n",
    "\n",
    "Zuvor hieß es, dass die Ableitung die Steigung der origininalen Funktion beschreibt. Man kann die Ableitung $\\frac{df}{dx}$ auch wei folgt interpretieren: *Um wie viel verändert sich $f(x)$ wenn ich $x$ verändern*. Hierbei ist natürlich die Stärke der Veränderung abhängig von $x$ selber. Im Beispiel $x^2$ haben kleiner Veränderung in $x$, größeren Effekt für Werte um $x=5$ als für Werte um $x=1$. \n",
    "\n",
    "Wenn wir die Gewichte eines Netzwerkes optimieren wollen, müssen wir auch wissen wie eine Veränderung der Gewcihte einen Veränderung im Loss herbeiführt. \n",
    "\n",
    "\n",
    "Hier ist nochmal ein schematisches Beispiel des Neuronalen Netzwerkes.\n",
    "\n",
    "<img src=\"Img/lin_alg/ableitung_3.png\"></img>\n",
    "\n",
    "Für das folgende Beispiel schauen wir uns nur den letzten Teil genauer an. Die Berrechnung von $\\hat{y}$ erfolgt in zwei Schritten. Zunächst wird $Z_2$ berechnet, dann wird eine nicht-lineare Funktion darauf angewandt was uns $\\hat{y}$.\n",
    "\n",
    "<img src=\"Img/lin_alg/ableitung_4.png\"></img>\n",
    "\n",
    "**Für dieses Beispiel schauen wir uns ein Beispiel mit nur einem Wert an**\n",
    "\n",
    "Also $a_1$ ist, für diesen Moment, kein Vektor sondern nur ein einzelner Wert, das gleiche gilt für $w_2$ und $b_2$.\n",
    "\n",
    "<img src=\"Img/lin_alg/ableitung_5.png\"></img>\n",
    "\n",
    "Die Frage ist: Welchen Einfluss hat $w_2$/$b_2$ auf den Loss $J$. Oder wie verändert sich der Loss, wenn wir $w_2$/$b_2$ verändern?\n",
    "\n",
    "Mathematische können wir das als Ableitung von $J$ nach $w_1$ bezeichnen. \n",
    "Wir benutzen jetzte $\\partial$ anstatt von $d$, da wir über Funktionen mit mehr Parametern sprechen ($b_2$).\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial w_2}$$\n",
    "\n",
    "Allerdings, gibt es keinen direkten Einfluss von $w_2$ auf den Loss. $w_2$ beinflusst $z_2$ und $z_2$ hat einen Effekt auf $\\hat{y}$. Und schlussendlich hat $\\hat{y}$ Einfluss auf den Loss.\n",
    "\n",
    "Die Kettenregel erlaubt es uns genau so $\\frac{\\partial J}{\\partial w_2}$ zu berechnen.\n",
    "\n",
    "Zunänchst berechnen wir den Effekt von $w_2$ auf $z_2$:\n",
    "$$\\frac{\\partial J}{\\partial w_2} = \\frac{\\partial z_2}{\\partial w_2}.... $$\n",
    "\n",
    "Als nächstest kommt der Effekt von $z_2$ auf $\\hat{y}$ dazu:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial w_2} = \\frac{\\partial z_2}{\\partial w_2}\\frac{\\partial \\hat{y}}{\\partial z_2} $$\n",
    "\n",
    "Als letztes noch der Effekt von $\\hat{y}$ auf $J$\n",
    "\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial w_2} = \\frac{\\partial z_2}{\\partial w_2}\\frac{\\partial \\hat{y}}{\\partial z_2}\\frac{\\partial J}{\\partial \\hat{y}} $$\n",
    "\n",
    "\n",
    "Die Kettenregel erlaubt es uns diese Effekte einfach zu multiplizieren, um die gewünschte Ableitung zur Erhalten.\n",
    "Diese Kette kann beliebig lang werden, deswegen kann auch ein Netzwerk beliebig groß werden. \n",
    "Denn wie Sie sich errinnern können, gibt es auch noch ein $w_1$ und $b_1$, auch deren Effekt auf $J$ kann berechnet werden. Hier wird die \"Kette\" nur noch länger.\n",
    "\n",
    "\n",
    "## Beispiel:\n",
    "\n",
    "$$e_1 = 2x+3$$\n",
    "$$e_2 = 0.5e_1^3$$\n",
    "\n",
    "Berechnen Sie $$\\frac{de_2}{dx}$$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d879fedb",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Lösung. HIER klicken</strong></summary>\n",
    "\n",
    "$$\\frac{de_2}{dx}= \\frac{de_1}{dx}\\frac{de_2}{de_1} $$\n",
    "$$\\frac{de_2}{dx}= 2(1.5e_1^2) $$\n",
    "    \n",
    "Da wir wissen, dass $e_1 = 2x+3$ ist, können wir diese auch in die Ableitung einsetzen.\n",
    "$$\\frac{de_2}{dx}= 2(1.5(2x+3)^2) $$ \n",
    "$$\\frac{de_2}{dx}= 2(1.5(4x^2+12x+9)) $$     \n",
    "$$\\frac{de_2}{dx}= 2(6x^2+18x+13.5) $$ \n",
    "$$\\frac{de_2}{dx}= 12x^2+36x+27) $$   \n",
    "</details>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed75e91e",
   "metadata": {},
   "source": [
    "# Übungsaufgabe 2\n",
    "\n",
    "In dieser Übungsaufgabe berechenen Sie auch wie in einem Neuronalen Netzwerk, den Gradienten für $w$. \n",
    "Natürlich vereinfacht und auch nur für einen Wert von $w$. In diesem Beispiel benutzen wir eine simple Lossfunktion und auch keine echte nicht-lineare Funktion. Die Lossfunktion würde in der tatsächlichen Applikation nicht funktionieren. Das gleiche gilt für die nicht-lineare Funktion, sie ist nämlich linear. Als Übungsaufgabe wäre es zu schwierig eine nicht-lineare Funktion und eine tatsächliche Loss Funktion abzuleiten. \n",
    "\n",
    "Bitte versuchen Sie diese Übung nach ihrem Vermögen zulösen. Wie schon öfter gesagt, ist es uns nicht wichtig, dass Sie das richtige Ergebnis erhalten, sondern das Sie sich mit der Materie befasst haben. Manchen fällt Mathe leichter als anderen, das ist uns bewusst. \n",
    "\n",
    "\n",
    "Zürück zu unserem Fake Netzwerk.\n",
    "Angenommen die letzte Layer unseres Netzwerk funktioniert wie folgt:\n",
    "\n",
    "$$z_2 = a_1w_2+b_2$$\n",
    "$$\\hat{y} = z_2^3-3$$\n",
    "$$J = \\hat{y}^2- y^2$$\n",
    "\n",
    "\n",
    "Berechnen Sie $\\frac{\\partial J}{\\partial w_2}$, also den \"Einfluss\" von $w_2$ auf $J$ (Loss).\n",
    "Hierfür geben wir die Werte:\n",
    "\n",
    "$$a_1 = 2 | b_2=1.4 | w_2 =0.6 | y=1 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "1ad66994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1182.4051199999992\n"
     ]
    }
   ],
   "source": [
    "# Berechnen sie zunächst z_2, y_hat, und J. Also quasi der Forwardpass \n",
    "weight =0.6\n",
    "\n",
    "z_2 = ___*weight+___\n",
    "\n",
    "y_hat = (z_2**__)-___\n",
    "\n",
    "J = ____-____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4901934",
   "metadata": {},
   "source": [
    "Sie haben den Forwardpass ausgeführt jetzt kommt die Berechnung der Gradienten. Dafür müssen wir zunächst nur die einzelnen Ableitung berechnen.\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial w_2} = \\frac{\\partial z_2}{\\partial w_2}\\frac{\\partial \\hat{y}}{\\partial z_2}\\frac{\\partial J}{\\partial \\hat{y}} $$\n",
    "\n",
    "Als erstes berechen Sie $\\frac{\\partial z_2}{\\partial w_2}$ welches wir `dw_2` nennen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d42cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dw_2 = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c11ccf6",
   "metadata": {},
   "source": [
    "Als nächstes berechnen Sie $frac{\\partial \\hat{y}}{\\partial z_2}$ welches wir `dz_2` nennen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb1a884",
   "metadata": {},
   "outputs": [],
   "source": [
    "dz_2 = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b58640",
   "metadata": {},
   "source": [
    "Als letzes berechnen Sie $\\frac{\\partial J}{\\partial \\hat{y}}$ welches wir `dy_hat` nennen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bbe10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dy_hat = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e748f735",
   "metadata": {},
   "source": [
    "Um den Gradienten zu berechnen müssen Sie nun nur diese drei Miteinander multiplizieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e0da51",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient = dw_2*dz_2*dy_hat\n",
    "gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00478d59",
   "metadata": {},
   "source": [
    "Das war es auch schon! Sie haben den Gradienten berechnet.\n",
    "Folgends müssen Sie nicht mehr abgeben Sie können sich aber daran probieren. Wenn wir diese Ableitungen in einen `for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696dbed9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a627ec2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight =0.6\n",
    "z_2 = 2*weight+1.4\n",
    "y_hat = (z_2**3)-3\n",
    "J = (y_hat**2)-(1**2)\n",
    "gradient = 2*(3*z_2**2)*2*y_hat\n",
    "print(gradient)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4aadee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "972f6ee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1182.4051199999997"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w= 0.6\n",
    "384*w**5+1344*w**4+1881.6*w**3+1173.12*w**2+259.392*w-6.02112"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "5e741cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211.4597759999998\n",
      "0.6\n",
      "103.10409815794709\n",
      "0.48175948800000007\n",
      "63.654416216972265\n",
      "0.4133630718387573\n",
      "43.5926701285659\n",
      "0.3655205614685663\n",
      "31.714582691388173\n",
      "0.3291293389146817\n",
      "24.01252737213741\n",
      "0.30005218812293577\n",
      "18.702162699654384\n",
      "0.27604367001457547\n",
      "14.874028047537482\n",
      "0.25574644406008173\n",
      "12.019163997382265\n",
      "0.23827735774220044\n",
      "9.832370434431656\n",
      "0.22302997653080864\n"
     ]
    }
   ],
   "source": [
    "weight =0.6\n",
    "for i in range(10):\n",
    "    z_2 = 2*weight+1.4\n",
    "    y_hat = z_2**3-3\n",
    "    J = y_hat**2-1**2\n",
    "    print(J)\n",
    "\n",
    "    gradient = 2*(3*z_2**2)*2*y_hat\n",
    "    print(weight)\n",
    "    weight -=  0.0001* gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8d40fd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from matplotlib import pyplot as plt\n",
    "x = np.arange(-6,6,0.2)\n",
    "y = x**2\n",
    "y_2 = 2*x\n",
    "\n",
    "\n",
    "x_line_1 = [-6,-4]\n",
    "y_line_1 = [35,15]\n",
    "\n",
    "x_line_2 = [-3,-1]\n",
    "y_line_2 = [8,0]\n",
    "\n",
    "x_line_3 = [3,5]\n",
    "y_line_3 = [8,24]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "2a3e1cc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.axhline(y=0, color='black')\n",
    "plt.axvline(x=0, color='black')\n",
    "plt.ylim([-35,35])\n",
    "plt.plot(x,y)\n",
    "plt.plot(x,y_2)\n",
    "\n",
    "# \n",
    "plt.plot(x_line_1,y_line_1, \"--\", color =\"black\", linewidth=3)\n",
    "plt.plot(-5,5**2, \"o\", color=\"r\")\n",
    "plt.plot(-5,-10, \"o\", color=\"r\")\n",
    "\n",
    "\n",
    "#\n",
    "\n",
    "plt.plot(x_line_2,y_line_2, \"--\", color =\"black\", linewidth=3)\n",
    "plt.plot(-2,2**2, \"o\", color=\"r\")\n",
    "plt.plot(-2,-4, \"o\", color=\"r\")\n",
    "#\n",
    "\n",
    "plt.plot(x_line_3,y_line_3, \"--\", color =\"black\", linewidth=3)\n",
    "plt.plot(4,4**2, \"o\", color=\"r\")\n",
    "plt.plot(4,8, \"o\", color=\"r\")\n",
    "\n",
    "plt.savefig(\"/nfs/home/jmenke2/Documents/GitHub/JanoschMenke/ableitung_1.png\", dpi =450)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffb5ed2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
