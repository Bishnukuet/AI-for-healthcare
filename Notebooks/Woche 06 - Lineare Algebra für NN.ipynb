{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lineare Algebra\n",
    "\n",
    "Heute werden wir die essenziellen mathematischen Grundlagen für Neuronale Netzwerke erklären.\n",
    "\n",
    "Das erste mathematische Konzept notwendig ist der **Vektor**\n",
    "Ein Vektor ist Sammlung von mehrere Werte und wird wie folgt definiert\n",
    "\n",
    "$$\\begin{bmatrix}3 & 4 & 0.5\\end{bmatrix}$$ \n",
    "Dieser Vektor enthält genau drei Werte. Mit Vektoren können wir einzelne Datenpunkte beschrieben. Zum Beispiel könnten wir die Daten eines Hauses in diesem Vektor speichern. Der erste Wert gibt an viele Bäder das Haus hat, der zweite wie viele Schlafzimmer, und der dritte Wert gibt das Alter der Heizung in Jahren an.\n",
    "\n",
    "Ihnen ist bestimmt aufgefallen, dass ein Vektor erstaunlich Ähnlichkeiten zu einem 1-dimensionalen `array` hat.\n",
    "`array([3,4,0.5])`. Tatsächlich, sollen `np.arrays` die gleichen Funktionen wie Vektoren haben. Die mathematischen Regeln die für Vektoren gelten, gelten auch für die `arrays`.\n",
    "\n",
    "\n",
    "Wir können zum Beispiel einen Vektor mit einer Zahl multiplizieren:\n",
    "*Für bessere Übersicht schreiben wir den Vektor untereinander*\n",
    "$$3\\cdot\\begin{bmatrix}3 \\\\ 4 \\\\ 0.5\\end{bmatrix}= \\begin{bmatrix}3\\cdot 3 \\\\ 4 \\cdot 3 \\\\ 0.5 \\cdot 3\\end{bmatrix}= \\begin{bmatrix}9 \\\\ 12 \\\\ 1.5\\end{bmatrix} $$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9. , 12. ,  1.5])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array([3,4,0.5])*3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gleiches gilt auch für Addition und Substraktion:\n",
    "$$3+\\begin{bmatrix}3 \\\\ 4 \\\\ 0.5\\end{bmatrix}= \\begin{bmatrix}3+3 \\\\ 4+3 \\\\ 0.5 + 3\\end{bmatrix}= \\begin{bmatrix}6 \\\\ 7 \\\\ 3.5\\end{bmatrix} $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6. , 7. , 3.5])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3+np.array([3,4,0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auch können wir zwei Vektoren addieren:\n",
    "    \n",
    "    \n",
    "$$\\begin{bmatrix}3 \\\\ 4 \\\\ 0.5\\end{bmatrix} + \\begin{bmatrix}0.3 \\\\ 3 \\\\ -0.2\\end{bmatrix} = \\begin{bmatrix}3 +0.3 \\\\ 4+3 \\\\ 0.5-0.2\\end{bmatrix} =  \\begin{bmatrix}3.3 \\\\ 7 \\\\ 0.3\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.3, 7. , 0.3])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([3,4,0.5])+ np.array([0.3,3,-0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interessant werden Vektoren erst, wenn wir mehrere miteinander multiplizieren.\n",
    "\n",
    "Vor allem das sogenannte Skalarprodukt ist für usn wichtig und wird wie folgt berechnet:\n",
    "$$\\begin{bmatrix}3 \\\\ 4 \\\\ 0.5\\end{bmatrix} \\cdot \\begin{bmatrix}0.3 \\\\ 3 \\\\ -0.2\\end{bmatrix} = (3\\cdot 0.3) + (3 \\cdot 4)+ (0.5\\cdot -0.2) = 12.8  $$\n",
    "\n",
    "\n",
    "Berechnen Sie das Skalarprodukt für die beiden Vektoren per Hand: \n",
    "\n",
    "$$\\begin{bmatrix}8 \\\\ 0.25 \\\\ -1\\end{bmatrix} \\cdot \\begin{bmatrix}0.1 \\\\ 12 \\\\ 8\\end{bmatrix} = $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Lösung. HIER klicken</strong></summary>\n",
    "\n",
    "$$\\begin{bmatrix}8 \\\\ 0.25 \\\\ -1\\end{bmatrix} \\cdot \\begin{bmatrix}0.1 \\\\ 12 \\\\ 8\\end{bmatrix} =(8\\cdot 0.1) + (0.25 \\cdot 12)+ (-1\\cdot 8) = -4.2  $$\n",
    "</details>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In `numpy` benutzen wir `np.dot()`, um das Skalarprodukt zu berechnen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.8"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(np.array([3,4,0.5]), np.array([0.3,3,-0.2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie Ihnen vielleicht schon aufgefallen ist, ähnelt das Skalarprodukt einer linearen Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.8"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x    = np.array([3,4,0.5])\n",
    "beta = np.array([0.3,3,-0.2])\n",
    "np.dot(x,beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`x` ist der Inputvektor der die Informationen für drei Variablen enthält. Zum Beispiel für ein Haus, das `3` Bäder und `4` Schlafzimmer hat. Es hat vor einem halben Jahr (`0.5`) eine neue Heizung bekommen. Der zweite Vektor enthält die Koeffizienten der Regression. Also $\\beta_1, \\beta_2, \\beta_3$. Mit der Regression können wir dann den Wert des Hauses in 100.000 € ermitteln. \n",
    "\n",
    "Effektive führt das Skalarprodukt zu einer Vereinfachung der Formel. Anstatt zu schreiben:\n",
    "$$\\hat{y} = \\beta_1x_1 +\\beta_2x_2 +\\beta_3x_3$$\n",
    "können wir die Formel auch so schreiben.\n",
    "\n",
    "$$\\hat{y} = x\\beta$$\n",
    "\n",
    "Hier muss angenommen werden, dass $x$ und $\\beta$ Vektoren sind. \n",
    "Es fehlt natürlich immer noch das $t$ oder auch $\\beta_0$. Also der y-Achsenabschnitt. Wie oben erklärt, können einzelne Werte einfach zu Vektoren addiert werden. \n",
    "\n",
    "Die komplette Formel wird deshalb:\n",
    "\n",
    "$$\\hat{y} = x\\beta+\\beta_0$$\n",
    "\n",
    "Können Sie diese Formel mit `numpy` schreiben? Berechnen Sie $\\hat{y}$ für `x`. Hierbei ist $\\beta_0=-5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.800000000000001"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_0 =-5\n",
    "y_hat = _____________________\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Lösung. HIER klicken</strong></summary>\n",
    "\n",
    "```python\n",
    "y_hat = np.dot(x,beta)+beta_0\n",
    "    \n",
    "```\n",
    "</details>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Angenommen, wir wollen nun nicht nur `y_hat` für ein Haus bestimmen, sondern für mehrere Häuser gleichzeitig, dann geht das mit genau derselben Formel. \n",
    "\n",
    "`X` enthält nun nicht nur einen Vektor, sondern gleich mehrere. Wie Sie schon gelernt haben, können solche Datenstrukturen als 2D-Array gespeichert werden. Ein 2D-Array ist mit einer Matrix in der Mathematik vergleichbar. \n",
    "\n",
    "Wenn wir von Matrizen sprechen, benutzen kapitalisierte Variablennamen, um zu kennzeichnen, dass wir von einer Matrix sprechen.\n",
    "\n",
    "Unten ist `X` gegeben. Sie können sehen, dass `np.dot(X,beta) + beta_0` immer noch das richtige Ergebnis liefert. Diesmal aber für jeder der 4 Reihen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-ca4ddf55acbe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m X = np.array([[3,4,0.5],\n\u001b[0m\u001b[0;32m      2\u001b[0m               \u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1.2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m               \u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.12\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m               [3,3,2]])\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "X = np.array([[3,4,0.5],\n",
    "              [2,1,1.2],\n",
    "              [4,2,0.12],\n",
    "              [3,3,2]])\n",
    "\n",
    "np.dot(X,beta) + beta_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Die Notation $\\beta$s kommt aus der traditionellen Statistik. Im maschinellen Lernen werden die Koeffizienten mit   $w$, für \"weights\", gekennzeichnet. Darüber hinaus wird $\\beta_0$, der y-Achsenabschnitt, als $b$ (Bias) bezeichnet.\n",
    "Die Regressionsgleichung ist deshalb:\n",
    "\n",
    "$$Xw+b$$\n",
    "\n",
    "Wir werden ab jetzt diese Notation beibehalten.\n",
    "\n",
    "---\n",
    "\n",
    "Wie Sie gelernt haben, ist die Stärke von Neuronalen Netzwerken, das Ausführen von mehr als nur einer Regression gleichzeitig.\n",
    "Das heißt, wir haben nicht nur eine Reihe von Regressionskoeffizienten, sondern mehrerer. Wie viele?\n",
    "Das ist Ihnen selber überlassen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-ab76b05a7d9d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m W =  np.array([beta,\n\u001b[0m\u001b[0;32m      2\u001b[0m               \u001b[1;33m[\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m               \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m               \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m               [1,2,-1]])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "W =  np.array([beta,\n",
    "              [6,0,-2],\n",
    "              [1,0,3],\n",
    "              [0,0,-1],\n",
    "              [1,2,-1]])\n",
    "b = np.array([beta_0,3,2,0.5,-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`W` enthält nun die Gewichte für insgesamt fünf lineare Regressionen. Die erste Reihe enthält  noch unsere `beta` Koeffizienten aus der initiale Regression.  Jede weitere Reihe enthält neue Koeffizienten/Gewichte für eine weite Regression. Anhand der Anzahl der Reihen können wir also erkennen, wie viele Regressionen wir machen. \n",
    "Auch `b` enthält fünf Werte. Für jede Regressionen enthält er den y-Achsenabschnitt\n",
    "\n",
    "In einem neuronalen Netzwerk bedeutet, dass auch wie viele Nodes wir in der Hidden Layer haben werden!\n",
    "\n",
    "Wollen wir jetzt mit diesen beiden Matrizen rechnen, passiert Folgendes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (4,3) and (5,3) not aligned: 3 (dim 1) != 5 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2344102/2383651950.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (4,3) and (5,3) not aligned: 3 (dim 1) != 5 (dim 0)"
     ]
    }
   ],
   "source": [
    "np.dot(X,W)+b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eine Fehlermeldung:\n",
    "\n",
    "```shapes (4,3) and (5,3) not aligned: 3 (dim 1) != 5 (dim 0)```\n",
    "\n",
    "Tatsächlich können wir aus der Fehlermeldung schließen, was das Problem ist. \n",
    "Zunächst werden uns die Dimensionen (Anzahl der Reihen und Spalten) ausgegeben. \n",
    "`X` `4` Reihen und `3` Spalten. `W` hat `5` Reihen und `3` Spalten. \n",
    "\n",
    "Darauf folgt: `3 (dim 1) != 5 (dim 0)`. Also, `3 (dim 1)`, die Anzahl der Spalten (`3 (dim 1)`) der ersten Matrix sind ungleich (`!=`) der Anzahl an Reihen der zweiten Spalte (`5 (dim 0)`).  \n",
    "\n",
    "**Die Anzahl der Spalten der ersten Matrix sollten gleich der Anzahl der Reihen in der zweiten Spalte sein.**\n",
    "\n",
    "Wenn wir zum Beispiel, die `W` Matrix umdrehen, also Reihen als Spalten und Spalten als Reihen, dann würden Anzahl der Spalten und Reihen gleich sein.\n",
    "\n",
    "Das Konvertieren von Spalten zu Reihen und umgekehrt, nennt sich das *Transpose* einer Matrix.\n",
    "`W.tranpose()` führt diese Transformation aus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.3  3.  -0.2]\n",
      " [ 6.   0.  -2. ]\n",
      " [ 1.   0.   3. ]\n",
      " [ 0.   0.  -1. ]\n",
      " [ 1.   2.  -1. ]] \n",
      "\n",
      "[[ 0.3  6.   1.   0.   1. ]\n",
      " [ 3.   0.   0.   0.   2. ]\n",
      " [-0.2 -2.   3.  -1.  -1. ]]\n"
     ]
    }
   ],
   "source": [
    "print(W, \"\\n\")\n",
    "print(W.transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie Sie sehen, werden aus den Reihen Spalten. Das führt auch dazu, dass sich die Dimension der Matrix ändern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 3) \n",
      "\n",
      "(3, 5)\n"
     ]
    }
   ],
   "source": [
    "print(W.shape, \"\\n\")\n",
    "print(W.transpose().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit dem Transpose der Matrix `W` sollte die Multiplikation der beiden Matrizen funktionieren, da jetzt die Anzahl der Spalten/Reihen identisch ist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.8  , 20.   ,  6.5  ,  0.   ,  8.5  ],\n",
       "       [-1.64 , 12.6  ,  7.6  , -0.7  ,  0.8  ],\n",
       "       [ 2.176, 26.76 ,  6.36 ,  0.38 ,  5.88 ],\n",
       "       [ 4.5  , 17.   , 11.   , -1.5  ,  5.   ]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(X,W.transpose())+b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tatsächlich klappt es. Schauen Sie sich zum Beispiel, die erste Spalte an. Diese Werte sind nämlich die Ergebnisse der ersten Regression, die wir berechnet haben: `np.dot(X, beta)+beta_0`.\n",
    "Tatsächlich enthält jede Reihe die fünf Regressionsergebnisse für jeweils eins der vier Häuser.\n",
    "\n",
    "Aber wie kann es sein, dass die Regression funktioniert, obwohl wir die `W` Matrix umgedreht haben.\n",
    "\n",
    "Das liegt daran, wie eine Matrixmultiplikation definiert ist. Es werden nicht Skalarprodukt zwischen korrespondierenden Reihen berechnet. Sondern, Skalarprodukt werden zwischen den Reihen der ersten Matrix und den Spalten der zweiten Matrix berechnet. \n",
    "\n",
    "\n",
    "![Matthew Scroggs](https://www.mscroggs.co.uk/img/full/multiply_matrices.gif)\n",
    "<center>Credit: Matthew Scroggs - 2020 | www.mscroggs.co.uk/blog/73 |</center>\n",
    "\n",
    "Tatsächlich ist das auch schon fast alles, was für den Forward Pass gebraucht wird.\n",
    "\n",
    "---\n",
    "\n",
    "Bis jetzt haben wir immer `np.dot()` für eine Matrixmultiplikation benutzt. Tatsächlich gibt es extra eine Funktion `np.matmul()`. Für große Matrizen ist `np.matmul` schneller und wir werden deswegen auch diese Funktion benutzen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.8  , 20.   ,  6.5  ,  0.   ,  8.5  ],\n",
       "       [-1.64 , 12.6  ,  7.6  , -0.7  ,  0.8  ],\n",
       "       [ 2.176, 26.76 ,  6.36 ,  0.38 ,  5.88 ],\n",
       "       [ 4.5  , 17.   , 11.   , -1.5  ,  5.   ]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(X,W.transpose())+b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ableitungen\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um zu verstehen, wie neuronale Netzwerke lernen, sollten Sie zumindest in groben Zügen verstehen, was Ableitung aussagen und wie man sie berechnen kann.\n",
    "\n",
    "Die Ableitung einer Funktion beschreibt die Steigung der ursprünglichen Funktion. \n",
    "Angenommen es gibt eine Funktion $f(x)=x^2$. Dann ist die dazugehörige Ableitung $\\frac{df}{dx}=2x$ (spich: *Ableitung von f nach x*). \n",
    "\n",
    "Im Bild sind sowohl $f(x)$ (*blau*) also auch die Ableitung $\\frac{df}{dx}$ (*orange*) eingezeichnet. Zum Beispiel für $x=-5$ ist $f(-5) = 25$. Die Steigung an diesem Punkt ist: $\\frac{df(-5)}{dx}=2\\cdot -5= -10$. Das heißt, die Steigung der Funktion $f(x)=x^2$ ist $-10$ wenn $x=-5$ ist.\n",
    "\n",
    "<img src=\"Img/lin_alg/ableitung_1.png\"></img>\n",
    "\n",
    "Es gibt einige Regeln zu Ableitung. Wir werden nur zwei davon besprechen. \n",
    "        $$f(x) = x^n \\rightarrow \\frac{df}{dx} = n \\cdot x^{n-1}$$\n",
    "        $$f(x) = x^2 \\rightarrow \\frac{df}{dx} = 2 \\cdot x^{2-1}=2x^1= 2x $$\n",
    "        \n",
    "\n",
    "Grundsätzlich fallen Konstanten immer in Ableitungen weg.\n",
    "\n",
    "Das heißt:\n",
    "Die Ableitung von $f(x)=x^2 + 5$ ist trotzdem nur $2x$, da Konstanten die Funktion nur verschieben, aber nicht in ihre Steigung beeinflussen. \n",
    "\n",
    "Anders werden Koeffizienten gehandhabt:\n",
    "\n",
    "$$f(x) = ax^n \\rightarrow \\frac{df}{dx} = (n \\cdot a)\\cdot x^{n-1}$$\n",
    "\n",
    "Ein Beispiel:\n",
    "\n",
    "$$f(x) = 4x^3 \\rightarrow \\frac{df}{dx} = 12x^2$$ \n",
    "\n",
    "\n",
    "**Probieren Sie folgende Funktionen abzuleiten (wahrscheinlich einfacher auf einem Papier):**\n",
    "\n",
    "$$g(x)= 7x^5 - 3$$\n",
    "\n",
    "$$h(x)= 0.5x^2 + 3x +12$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Lösung. HIER klicken</strong></summary>\n",
    "\n",
    "$$\\frac{dg}{dx}35x^4 $$\n",
    "$$\\frac{dh}{dx}x +3$$\n",
    "\n",
    "</details>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kettenregel \n",
    "\n",
    "Die wichtigste Regel für neuronale Netzwerke ist die Kettenregel. Anhand der Formel ist sie schwierig zu verstehen, doch anhand eines Beispieles sollte es relativ einfach sein. \n",
    "\n",
    "Zuvor hieß es, dass die Ableitung die Steigung der originalen Funktion beschreibt. Man kann die Ableitung $\\frac{df}{dx}$ auch wie folgt interpretieren: *Um wie viel verändert sich $f(x)$, wenn ich $x$ verändern*. Hierbei ist natürlich die Stärke der Veränderung abhängig von $x$ selber. Im Beispiel $x^2$ haben kleiner Veränderung in $x$, größeren Effekt für Werte um $x=5$ als für Werte um $x=1$. \n",
    "\n",
    "Wenn wir die Gewichte eines Netzwerkes optimieren wollen, müssen wir auch wissen, wie eine Veränderung der Gewichte eine Veränderung im Loss herbeiführt. \n",
    "\n",
    "\n",
    "Hier ist nochmal ein schematisches Beispiel des Neuronalen Netzwerkes.\n",
    "\n",
    "<img src=\"Img/lin_alg/ableitung_3.png\"></img>\n",
    "\n",
    "Für das folgende Beispiel schauen wir uns nur den letzten Teil genauer an. Die Berechnung von $\\hat{y}$ erfolgt in zwei Schritten. Zunächst wird $Z_2$ berechnet, dann wird eine nicht-lineare Funktion darauf angewandt, was uns $\\hat{y}$.\n",
    "\n",
    "<img src=\"Img/lin_alg/ableitung_4.png\"></img>\n",
    "\n",
    "**Für dieses Beispiel schauen wir uns ein Beispiel mit nur einem Wert an**\n",
    "\n",
    "Also $a_1$ ist, für diesen Moment, kein Vektor, sondern nur ein einzelner Wert, das Gleiche gilt für $w_2$ und $b_2$.\n",
    "\n",
    "<img src=\"Img/lin_alg/ableitung_5.png\"></img>\n",
    "\n",
    "Die Frage ist: Welchen Einfluss hat $w_2$/$b_2$ auf den Loss $J$. Oder wie verändert sich der Loss, wenn wir $w_2$/$b_2$ verändern?\n",
    "\n",
    "Mathematische können wir das als Ableitung von $J$ nach $w_1$ bezeichnen. \n",
    "Wir benutzen jetzt $\\partial$ anstatt von $d$, da wir über Funktionen mit mehr Parametern sprechen ($w_2$und $b_2$).\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial w_2}$$\n",
    "\n",
    "Allerdings, gibt es keinen direkten Einfluss von $w_2$ auf den Loss. $w_2$ beeinflusst $z_2$ und $z_2$ hat einen Effekt auf $\\hat{y}$. Und schlussendlich hat $\\hat{y}$ Einfluss auf den Loss.\n",
    "\n",
    "Die Kettenregel erlaubt es uns genau so $\\frac{\\partial J}{\\partial w_2}$ zu berechnen.\n",
    "\n",
    "Zunächst berechnen wir den Effekt von $w_2$ auf $z_2$:\n",
    "$$\\frac{\\partial J}{\\partial w_2} = \\frac{\\partial z_2}{\\partial w_2}.... $$\n",
    "\n",
    "Als Nächstes kommt der Effekt von $z_2$ auf $\\hat{y}$ dazu:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial w_2} = \\frac{\\partial z_2}{\\partial w_2}\\frac{\\partial \\hat{y}}{\\partial z_2} $$\n",
    "\n",
    "Als Letztes noch der Effekt von $\\hat{y}$ auf $J$\n",
    "\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial w_2} = \\frac{\\partial z_2}{\\partial w_2}\\frac{\\partial \\hat{y}}{\\partial z_2}\\frac{\\partial J}{\\partial \\hat{y}} $$\n",
    "\n",
    "\n",
    "Die Kettenregel erlaubt es uns diese Effekte einfach zu multiplizieren, um die gewünschte Ableitung zur erhalten.\n",
    "Diese Kette kann beliebig lang werden, deswegen kann auch ein Netzwerk beliebig groß werden. \n",
    "Denn wie Sie sich erinnern können, gibt es auch noch ein $w_1$ und $b_1$, auch deren Effekt auf $J$ kann berechnet werden. Hier wird die \"Kette\" nur noch länger.\n",
    "\n",
    "\n",
    "## Beispiel:\n",
    "\n",
    "$$e_1 = 2x+3$$\n",
    "$$e_2 = 0.5e_1^3$$\n",
    "\n",
    "Berechnen Sie $$\\frac{de_2}{dx}$$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Lösung. HIER klicken</strong></summary>\n",
    "\n",
    "$$\\frac{de_2}{dx}= \\frac{de_1}{dx}\\frac{de_2}{de_1} $$\n",
    "$$\\frac{de_2}{dx}= 2(1.5e_1^2) $$\n",
    "    \n",
    "Da wir wissen, dass $e_1 = 2x+3$ ist, können wir diese auch in die Ableitung einsetzen.\n",
    "$$\\frac{de_2}{dx}= 2(1.5(2x+3)^2) $$ \n",
    "$$\\frac{de_2}{dx}= 2(1.5(4x^2+12x+9)) $$     \n",
    "$$\\frac{de_2}{dx}= 2(6x^2+18x+13.5) $$ \n",
    "$$\\frac{de_2}{dx}= 12x^2+36x+27) $$   \n",
    "</details>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Übungsaufgabe 2\n",
    "\n",
    "In dieser Übungsaufgabe berechenen Sie auch wie in einem Neuronalen Netzwerk, den Gradienten für $w$. \n",
    "Natürlich vereinfacht und auch nur für einen Wert von $w$. In diesem Beispiel benutzen wir eine simple Lossfunktion und auch keine echte nicht-lineare Funktion. Die Lossfunktion würde in der tatsächlichen Applikation nicht funktionieren. Das gleiche gilt für die nicht-lineare Funktion, sie ist nämlich linear. Als Übungsaufgabe wäre es zu schwierig eine nicht-lineare Funktion und eine tatsächliche Loss Funktion abzuleiten. \n",
    "\n",
    "Bitte versuchen Sie diese Übung nach ihrem Vermögen zulösen. Wie schon öfter gesagt, ist es uns nicht wichtig, dass Sie das richtige Ergebnis erhalten, sondern das Sie sich mit der Materie befasst haben. Manchen fällt Mathe leichter als anderen, das ist uns bewusst. \n",
    "\n",
    "\n",
    "Zürück zu unserem Fake Netzwerk.\n",
    "Angenommen die letzte Layer unseres Netzwerk funktioniert wie folgt:\n",
    "\n",
    "$$z_2 = a_1w_2+b_2$$\n",
    "$$\\hat{y} = z_2^3-3$$\n",
    "$$J = \\hat{y}^2- y^2$$\n",
    "\n",
    "\n",
    "Berechnen Sie $\\frac{\\partial J}{\\partial w_2}$, also den \"Einfluss\" von $w_2$ auf $J$ (Loss).\n",
    "Hierfür geben wir die Werte:\n",
    "\n",
    "$$a_1 = 2 | b_2=1.4 | w_2 =0.6 | y=1 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1182.4051199999992\n"
     ]
    }
   ],
   "source": [
    "# Berechnen sie zunächst z_2, y_hat, und J. Also quasi der Forwardpass \n",
    "weight =0.6\n",
    "\n",
    "z_2 = ___*weight+___\n",
    "\n",
    "y_hat = (z_2**__)-___\n",
    "\n",
    "J = ____-____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sie haben den Forwardpass ausgeführt, jetzt kommt die Berechnung der Gradienten. Dafür müssen wir zunächst nur die einzelnen Ableitungen berechnen.\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial w_2} = \\frac{\\partial z_2}{\\partial w_2}\\frac{\\partial \\hat{y}}{\\partial z_2}\\frac{\\partial J}{\\partial \\hat{y}} $$\n",
    "\n",
    "Als Erstes berechnen Sie $\\frac{\\partial z_2}{\\partial w_2}$ welches wir `dw_2` nennen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dw_2 = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als nächstes berechnen Sie $\\frac{\\partial \\hat{y}}{\\partial z_2}$ welches wir `dz_2` nennen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dz_2 = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als letzes berechnen Sie $\\frac{\\partial J}{\\partial \\hat{y}}$ welches wir `dy_hat` nennen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dy_hat = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um den Gradienten zu berechnen, müssen Sie nun nur diese drei Miteinander multiplizieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient = dw_2*dz_2*dy_hat\n",
    "gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das war es auch schon! Sie haben den Gradienten berechnet.\n",
    "Folgendes müssen Sie nicht mehr abgeben, Sie können sich aber daran probieren. Wenn wir diese Ableitungen in einen `for-loop` packen, und das Gewicht entgegen des Gradienten ein klein wenig verändern, können wir sehen, dass der Loss langsam kleiner wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-7-827356c2b103>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-7-827356c2b103>\"\u001b[1;36m, line \u001b[1;32m6\u001b[0m\n\u001b[1;33m    dw_2 =\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "weight =0.6\n",
    "for i in range(10):\n",
    "    z_2 = ___*weight+___\n",
    "    y_hat = (z_2**__)-___\n",
    "    J = ____-____\n",
    "    dw_2 = \n",
    "    dz_2 = \n",
    "    dy_hat = \n",
    "    gradient = dw_2*dz_2*dy_hat\n",
    "    weight -=  0.0001* gradient # updaten des weights\n",
    "    print(J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
