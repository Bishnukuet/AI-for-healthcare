{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"https://venturebeat.com/wp-content/uploads/2019/06/pytorch-e1576624094357.jpg?w=1200&strip=all\" style=\"width: 800px;\">\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch\n",
    "\n",
    "---\n",
    "\n",
    "### Lernziele\n",
    "* Sie können ein einfaches Neuronales Netz mit PyTorch programmieren.\n",
    "* Sie können fortgeschrittene Schichten von Neuronen in Ihr Netzwerk einbauen (Dropout, Batchnorm).\n",
    "* Sie lernen weitere moderne Optimierungen kennen (Momentum, adam).\n",
    "---\n",
    "\n",
    "Letzte Woche haben Sie ein einfaches neuronales Netzwerk selber programmiert. Wie schon erwähnt, ist es aber nicht nötig jedes Netzwerk selber zu programmieren. Bestimmte Softwarepackete übernehmen viele der Unannehmlichkeiten, die beim Erstellen und Trainieren von Netzwerken auftreten.\n",
    "\n",
    "Grundsätzlich gibt es zwei Libraries, die benutzt werden können, PyTorch und TensorFlow. TensorFlow wird von Google entwickelt. TensorFlow ist die populärere Wahl, vor allem in der Industrie. PyTorch hingegen, hat sich vor allem in der wissenschaftlichen Welt durchgesetzt. Grundsätzlich gilt PyTorch als das leichter zu lernende Framework ist insgesamt benutzerfreundlicher. \n",
    "\n",
    "Während es früher noch große Unterschiede gab, werden die beiden Libraries heute immer ähnlicher in ihrer Funktionalität.\n",
    "\n",
    "Als Letztes gibt es noch Keras und PyTorch Lightning. Beide haben das Ziel das Erstellen von Neuronale Netzwerke noch einfacher zu machen. Keras benutzt im Hintergrund TensorFlow, macht es aber vor allem Anfänger leichter Netzwerke zu trainieren. Dasselbe gilt für PyTorch Lightning und PyTorch.\n",
    "\n",
    "In der Chemieinformatik bietet sich aber PyTorch an, da es spezielle Libraries für Graph Neural Networks nur für PyTorch gibt/gegeben hat.\n",
    "\n",
    "\n",
    "Ein essenzieller Bestandteil von PyTorch ist **autograd**. Autograd ist eine Library, die, wie der Name erahnen lässt, die Gradienten automatisch berechnen und sammeln kann. Dadurch müssen die Ableitungen nicht mehr selber berechnen werden.\n",
    "Auch gibt es viele Funktionen, wie z. B. Activationfunctions oder lineare Transformationen, die schon in PyTorch implementiert sind. \n",
    "\n",
    "*TensorFlow hat diese Funktionalitäten natürlich auch.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors\n",
    "\n",
    "Während Sie bis jetzt mit NumPy Arrays gerechnet haben, benutzen wir heute `Tensors`, genauer gesagt PyTorch `tensors`. \n",
    "\n",
    "**Was ist der Unterschied?**\n",
    "\n",
    "Erst einmal keiner. Arrays und Tensors sind sich in vielen Punkten ähnlich. Beide speichern Zahlen/Werte in einer strukturierten Form. So können Sie in NumPy Matrizen in einem 2D-Array speichern, Sie können aber die gleich Matrix auch in einem 2D-Tensor speichern.\n",
    "Auch lassen sich Tensors in Arrays und Arrays in Tensors umwandeln.\n",
    "\n",
    "Der Unterschied zwischen den beiden „Speichermöglichkeiten“ ist, dass PyTorch Tensor von PyTorch entwickelt worden sind. Und NumPy Arrays wurden von den Entwicklern von NumPy entwickelt. \n",
    "Viele Funktionen, die NumPy bietet, gibt es auch von PyTorch für deren Tensors (heißen dann aber eventuell anders). \n",
    "PyTorch hat ihre Tensors entwickelt, um mathematische Operationen schneller durchführen zu können. Zudem kann man Tensors auch auf die Grafikkarte „laden“, was die Geschwindigkeit von Operation um ein Vielfaches verbessert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit Tensors zu rechnen ist fast identisch zu Rechnung mit Arrays. Aber die Funktionen tragen eventuell andere Namen. `torch.mm()` ist zum Beispiel die Funktion für Matrixmultiplikation und `.t()` ist das Transpose einer Matrix. Ähnlich wie mit `np.array()` Arrays erstellt werden, so werden mit `torch.tensor()` Tensors erstellt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # lädt PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([[1,2,3],\n",
    "                [4,5,6]])\n",
    "\n",
    "W = torch.tensor([[8,9,10],\n",
    "                 [11,12,313]])\n",
    "\n",
    "b = torch.tensor([1,2])\n",
    "\n",
    "torch.mm(X,W.t())+b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das ist die lineare Transformation, die Sie auch schon kennen.<br>\n",
    "PyTorch vereinfacht diesen Schritt aber. \n",
    "So gibt es ein Modul in PyTorch mit dem Namen `nn`, diese enthält viele Funktionen, die hilfreich sind für das Erstellen von Neuronalen Netzwerken.\n",
    "\n",
    "Wir können das Module `nn` mit `from torch import nn` laden. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuronales Netzwerk mit PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch `nn` stellt unter anderem die Funktion `nn.Linear` zur Verfügung. Diese führt die lineare Transformation $xW^T +b $ aus.\n",
    "Als Input nimmt die Funktion: \n",
    "* `in_features`  die Anzahl der Features, die der Input hat, vor der Transformation hat, oder auch die Größe der Input Layer. Gestern hatten die Bilder 784 Pixel, also 784 Features\n",
    "* `out_features`  wie viele Features der Input nach der Transformation haben soll. `out_features` legt also die Größe der Hidden Layer fest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_1=nn.Linear(in_features = 784, out_features=300, bias=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fehlt den nicht der Input für die Layer?\n",
    "\n",
    "Stimmt, bis jetzt haben Sie auch keine lineare Transformation durchgeführt, sondern nur eine Variable  `layer_1` erstellt. Diese kann dann die lineare Transformation für uns durchführen.\n",
    "Praktisch ist vor allem, dass die Weights dieser Transformation automatisch von PyTorch initialisiert werden. Das nimmt Ihnen schon ein wenig Arbeit ab.\n",
    "Die Weights $W$ dieser Layer können auch angeschaut werden.\n",
    "\n",
    "Dafür benutzen Sie `list(layer_1.parameters())[0]`\n",
    "Wenn Sie die genaue Größe der Weightmatrix in Erfahrung bringen wollen, können Sie wie bei NumPy `.shape` benutzen: `list(layer_1.parameters())[0].shape` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(layer_1.parameters())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(layer_1.parameters())[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sie sehen, die Weightmatrix hat die gleichen Dimensionen wie die Matrix von letzter Woche. Sie können auch sehen, dass die Matrix tatsächlich Weights beinhaltet.\n",
    "Sie brauchen jetzt nur einen Input (Bilder), die Sie mit dieser linearen Transformation verändern wollen. \n",
    "Dazu wird der Trainingsdatensatz von letzter Woche mit NumPy geladen.\n",
    "Zusätzlich müssen die Bilder noch in einen Tensor umgewandelt werden. Das geht mit `torch.tensor()`\n",
    "\n",
    "Sie müssen natürlich auch wieder die Daten skalieren. Dafür benutzen Sie den Min-Max Scaler.\n",
    "In PyTorch muss genauer auf die Datentypen geachtet werden. \n",
    "Deswegen legen wir den Datentyp `dtype` festgelegt. Die Datenart für unsere Bilder ist `float32`. `float` kennt ihr noch von gestern, die `32` legt fest wie genau diese Zahl sein kann.\n",
    "`long` sagt Ihnen vielleicht nichts, ist aber einfach einer der `torch` Begriffe für Integers.\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "<details>\n",
    "<summary><strong>Besonders Interessierte HIER KLICKEN:</strong></summary>\n",
    "\n",
    "Im letzten Notebook wurde angeschnitten, warum wir die Weightmatrix genau so initialisieren.\n",
    "Tatsächlich werden in TensorFlow die Weightmatrizen so erstellt, dass eine Matrixmultiplikation auch ohne `transpose` auskommt.\n",
    "    \n",
    "Hier ist die Beschreibung des Codes für den Forwardpass von Tensorflow.\n",
    "    \n",
    "    \n",
    "```\n",
    "  `Dense` implements the operation:\n",
    "  `output = activation(dot(input, kernel) + bias)`\n",
    "  where `activation` is the element-wise activation function\n",
    "  passed as the `activation` argument, `kernel` is a weights matrix\n",
    "  created by the layer, and `bias` is a bias vector created by the layer\n",
    "  (only applicable if `use_bias` is `True`). These are all attributes of\n",
    "  `Dense`.\n",
    "```\n",
    "Für PyTorch ist diese [hier](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) zu finden.\n",
    "    \n",
    "PyTorch übernimmt hier die Notation, die eine größere Ähnlichkeit zu der mathematischen Notation hat.\n",
    "\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def min_max(x):\n",
    "    return (x - np.min(x)) / (np.max(x) - np.min(x))\n",
    "\n",
    "\n",
    "train_data = np.genfromtxt('../data/mnist/mnist_train.csv', delimiter=',', skip_header =False) #genfromtxt reads .txt files if we chose delimiter =\",\" the function can read also .csv files  (comma seperated values)\n",
    "\n",
    "train_images = min_max(train_data[:,1:])\n",
    "train_images = torch.tensor(train_images, dtype = torch.float32)\n",
    "train_labels=torch.tensor(train_data[:,0].astype(int), dtype = torch.long) \n",
    "\n",
    "\n",
    "test_data = np.genfromtxt('../data/mnist/mnist_test.csv', delimiter=',', skip_header =False) #genfromtxt reads .txt files if we chose delimiter =\",\" the function can read also .csv files  (comma seperated values)\n",
    "\n",
    "test_images = min_max(test_data[:,1:])\n",
    "test_images = torch.tensor(test_images, dtype = torch.float32)\n",
    "test_labels=torch.tensor(test_data[:,0].astype(int), dtype = torch.long) \n",
    "\n",
    "train_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Datensatz umfasst 60000 Bilder mit jeweils 784 Pixeln.\n",
    "Diese können Sie jetzt als Input für die lineare Transformation benutzen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_1=layer_1(train_images)\n",
    "print(z_1)\n",
    "z_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die `layer_1` gibt den Output (`z_1`) aus. Dieser hat die `shape` `[60000,200]`. Also immer noch 60000 Bilder, aber diesmal hat jedes nur jeweils 200 Features (Größe der Hidden Layer). So wie es auch bei der Definition der `layer_1` bestimmt wurde.\n",
    "\n",
    "Was Ihnen auffallen sollte, am Ende des Tensors `z_1` steht `grad_fn=<AddmmBackward>`. Dies zeigt an, dass *autograd* die Gradienten für diese Transformation erfasst hat. Man kann auch erkennen, dass wir eine Matrixmultiplikation `mm` und eine Addition `Add` durchgeführt haben. Es wird immer die letzte durchgeführte Transformation des Tensors angezeigt.\n",
    "\n",
    "Was Ihnen jetzt noch fehlt, ist die Activationfunction. Auch hier kann PyTorch helfen. \n",
    "PyTorchs `nn`Library hat ein Submodul `functional`. Hier sind viele zusätzliche mathematische Funktionen enthalten, unter anderem auch die `relu` und `sigmoid` Funktionen. Da die ReLU Funktion in der Praxis noch bessere Ergebnisse liefert als die Sigmoid Funktion verwenden wir jetzt diese.<br>\n",
    "`functional` lässt sich wie folgt importieren: `from torch.nn import functional as F`. Wir nennen `functional` zu `F` um, quasi eine Norm, wenn man mit PyTorch arbeitet.\n",
    "\n",
    "`F.relu()` kann jetzt benutzt werden um die ReLU Funktion anzuwenden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "a_1 = F.relu(z_1)\n",
    "print(a_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenn Sie `a_1` mit `z_1` vergleichen können Sie sehen, dass alle Werte die vorher negative waren Null wurden und alle Werte die positive waren unverändert sind.\n",
    "Auch kann man im `grad fn` sehen, dass eine ReLU angewandt wurde. Also auch das hat *autograd* mit aufgezeichnet. \n",
    "\n",
    "Der erste Teil der Forwardpropagation ist geschafft. \n",
    "Für den zweiten Schritt können wir einfach eine weitere Layer erstellen, die `a_1` als Input nimmt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_2 = nn.Linear(300,10) # 10 ist die Anzahl der out_features, da wir 10 Ziffern haben.\n",
    "z_2=layer_2(a_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auch hier brauchen Sie wieder eine Activationfunction. Diesmal aber die `softmax`-Funktion um die Wahrscheinlichkeiten zu erhalten.\n",
    "`nn.functional` hat auch eine `softmax()` Funktion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = F.softmax(z_2,dim=1)# der dim Parameter legt fest ob über Spalten oder Reihen die Softmax funktion angewandt wird.\n",
    "y_hat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch kann man auch verschiedene Layers zusammenfassen. Mit `nn.Sequential` können Sie die lineare Transformation und Activationfunction direkt hintereinander schalten. Der Input wird automatisch durch jede der Layers weitergeleitet.\n",
    "Das macht Ihren Code übersichtlicher und einfacher zu schreiben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netzwerk = nn.Sequential(nn.Linear(784,300), \n",
    "                         nn.ReLU(), \n",
    "                         nn.Linear(300,10))\n",
    "netzwerk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie Sie sehen können, wurde ein Netzwerk mit einer Hiddenlayer erstellt. Was Ihnen ausfallen sollte ist, dass anstatt `F.relu` `nn.ReLU` benutzt wurde. Wenn eine `relu` Funktion innerhalb von `Sequential()` benutzt werden soll, müssen Sie immer `nn.ReLU` benutzen. \n",
    "\n",
    "Das `netzwerk` könnte nun die Bilder klassifizieren:\n",
    "Mit `netzwerk(input)` kann der Input z. B. unsere Bilder durch das Netzwerk geführt werden.\n",
    "Anhand der Tensorgröße des Outputs können Sie sehen, dass tatsächlich am Ende 60000 Bilder mit jeweils 10 Features (den 10 Ziffern bekommen) als Output herausgegeben werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=netzwerk(train_images)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eine weitere Veränderung ist, dass Sie nicht mehr die letzte Activationfunktion benutzt haben. PyTorch wählt die Activationfunktion automatisch. Die Entscheidung welche Activationfunction in der letzten Layer benutzt werden soll hängt von der Wahl der Lossfunktion ab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Funktion\n",
    "\n",
    "Auch mit der Lossfunktion kann `nn` helfen. Die häufigsten Lossfunktionen sind nämlich schon in PyTorch enthalten.\n",
    "Sie können einfach eine neue Variable erstellen und dieser die `nn.CrossEntropyLoss()` Funktion zuordnen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_funktion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`loss_funktion` kann nun den Loss berechnen und wendet dabei automatisch die Softmax-Funktion an.\n",
    "Dazu müssen Sie einfach nur `y_hat` und die `train_labels` in die Funktion geben. Hier zeigt sich ein weiterer Vorteil, Sie brauchen die Labels nicht noch `one-hot` codieren, auch das macht PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=loss_funktion(output, train_labels)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back Propagation\n",
    "\n",
    "Als letzten Schritt müssen Sie noch die Back Propagation durchführen. Dank *autograd* geht das einfach mit dem Befehl `loss.backward()`. Dieser berechnet die Gradienten für alle Weightmatrizen.\n",
    "\n",
    "Danach müssen die Weightmatrizen nur noch geupdated werden. Wie Sie sich denken können, nimmt Ihnen auch der PyTorch ab.\n",
    "PyTorch stellt sogar eine Vielzahl von Algorithmen zu Verfügung. Viele dieser funktionieren besser als der einfache Gradient Descent.\n",
    "Für das Updaten der Weights wird ein neues Modul von PyTorch benötigt.\n",
    "Dafür laden Sie `from torch import optim`. `optim` enthält Optimizer, also Funktionen, die für uns das Netzwerk optimieren - die Weights updaten.\n",
    "\n",
    "Ähnlich wie bei der Loss Funktion, können Sie auch hier einfach eine Variable erstellen und ihr Updatefunktion zuordnen. \n",
    "Sie können jetzt den `optim.SGD()` zum updaten benutzen. SGD = Stochastic Gradient Descent.  In der Funktion selber definieren Sie, welche Parameter (Weights) verändert werden sollen. Auch legen Sie hier die Lernrate fest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward() #sammelt die Gradienten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "weights_updaten=optim.SGD(netzwerk.parameters(), lr=0.01) # Sie legen fest welche Parameter und mit welche Lernrate diese verändert werden sollen\n",
    "\n",
    "weights_updaten.step()  # mit- step() updaten Sie die Weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt haben Sie alles, was man für ein Netzwerk braucht.\n",
    "\n",
    "Sie können wieder einen for-loop benutzen, um das Training zu automatisieren.\n",
    "\n",
    "Es fällt euch eventuell auf, dass wir noch `updaten.zero_grad()` benutzen. Diese Funktion wird benutzt, um die gesammelten Gradienten zu löschen. Wird das nicht gemacht, so würde der Optimizer alle Gradienten aus allen Epochs zusammen zählen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Definieren von Netzwerk, LossFunktion und Update Algorithmus\n",
    "netzwerk = nn.Sequential(nn.Linear(784,300), \n",
    "                         nn.ReLU(), \n",
    "                         nn.Linear(300,10))\n",
    "\n",
    "loss_funktion = nn.CrossEntropyLoss()\n",
    "updaten = optim.SGD(netzwerk.parameters(), lr=0.3)\n",
    "EPOCHS = 50\n",
    "\n",
    "## Trainings Loop\n",
    "for i in range(50):\n",
    "    updaten.zero_grad()\n",
    "    output = netzwerk(train_images) # Forward Propagation\n",
    "    \n",
    "    loss   = loss_funktion(output, train_labels)\n",
    "    loss.backward()\n",
    "    acc=((output.max(dim=1)[1]==train_labels).sum()/float(output.shape[0])).item()\n",
    "    print(i, \n",
    "        \"Training Loss: %.2f Training Accuracy: %.2f\"\n",
    "        % (loss.item(), acc)\n",
    "    )\n",
    "    \n",
    "    updaten.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sie können sehen, dass Sie mit viel weniger Aufwand ein neuronales Netzwerk trainieren können. Sie können auch ohne großen Aufwand ein zweite oder dritte Hiddenlayer zu Ihrem Model hinzufügen.\n",
    "Einfach eine `nn.ReLU` und eine `nn.Linear` Layer in `Sequential` hinzufügen und *autograd* kann auch für diese Layer die Gradienten berechnen. Alles andere bleibt gleich. Denken Sie nur daran, dass die Dimensionen von einer zu nächsten Layer passen müssen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Definieren von Netzwerk, Loss Funktion und Update Algorithmus\n",
    "netzwerk = nn.Sequential(nn.Linear(784,300), \n",
    "                         nn.ReLU(), \n",
    "                         nn.Linear(300,300),# <----- EXTRA LAYER\n",
    "                         nn.ReLU(), \n",
    "                         nn.Linear(300,10))\n",
    "print(netzwerk)\n",
    "loss_funktion = nn.CrossEntropyLoss()\n",
    "updaten = optim.SGD(netzwerk.parameters(), lr=0.3)\n",
    "EPOCHS = 50\n",
    "\n",
    "## Trainings Loop\n",
    "for i in range(50):\n",
    "    updaten.zero_grad()\n",
    "    output = netzwerk(train_images) # Forward Propagation\n",
    "    \n",
    "    loss   = loss_funktion(output, train_labels)\n",
    "    loss.backward()\n",
    "    \n",
    "    acc=((output.max(dim=1)[1]==train_labels).sum()/float(output.shape[0])).item()\n",
    "    print(i,\n",
    "        \"Training Loss: %.2f Training Accuracy: %.2f\"\n",
    "        % (loss.item(), acc)\n",
    "    )\n",
    "    \n",
    "    updaten.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sie haben vielleicht schon gemerkt, dass wir als Optimizer (Weights updaten) den Stochastic Gradient Descent benutzen. Bisher wurde nur von dem Gradient Descent gesprochen.\n",
    "Tatsächlich wird der Gradient Descent, so wie gestern erklärt eigentlich nicht mehr benutzt, sondern als Alternative der Stochastic Gradient Descent. \n",
    "\n",
    "Der Unterschied:<br>\n",
    "Im Stochastic Gradient Descent wird der Datensatz nicht auf einmal durch das Netz geschickt, sondern der Datensatz wird in kleineren Teilen (**minibatch**) durch das Netzwerk gebracht. <br>\n",
    "In diesem Datensatz gibt es insgesamt 60000 Bilder. Beim Gradient Descent, werden wird der Forwardpass mit 60000 Bilder berechnet, für die 60000 Bilder der Loss berechnet. Danach werden die Bilder einmal upgedatet.\n",
    "Dann wiederholt sich der Schritt. \n",
    "\n",
    "Es wäre doch effizienter, wenn nicht erst nach allen 60000 Bilder ein Update passiert. Sondern schon nach 200 oder auch nur 100.  Dadurch kann das Netzwerk viel schneller lernen\n",
    "Genau das macht Stochastic Gradient Descent.  Nicht alle Bilder, sondern nur z. B. 32 Bilder werden auf einmal durch das Netzwerk geschickt. Für diese 32 wird dann der Loss berechnet und die Updates durchgeführt.\n",
    "Dann wird das wiederholt, aber diesmal mit 32 neuen Bilder. So können innerhalb eines Epochs viel öfter die Weights upgedatet werden.\n",
    "Die Batchsize gibt an, wie groß ein Minibatch (der kleine Teil der Daten, der durchs Netzwerk geht) sein soll und kann auch die Performance des Models beeinflussen.\n",
    "<center>\n",
    "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcT0TVrYkk0A0FfPvnzYTe747F0qPLG2rU2Bmg&usqp=CAU\" style=\"width: 600px;\">\n",
    "</center>\n",
    "<h8><center>Source: Insu Han and Jongheon Jeong, http://alinlab.kaist.ac.kr/resource/Lec2_SGD.pdf</center></h8>\n",
    "\n",
    "\n",
    "Vorteile:<br>\n",
    "* schneller\n",
    "* braucht weniger Speicherplatz (auf der Graphikkarte)\n",
    "\n",
    "Nachteile: <br>\n",
    "* kann nicht das Optimum finden (kann aber auch gut sein, um overfitting zu verhindern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Damit wir die Vorteile vom Stochastic Gradient Descent nutzen können, müssen wir zuerst die Daten in Minibatches einteilen. Auch dafür können Sie auch PyTorch benutzen, die entsprechenden Funktionen gibt es im Submodul `torch.utils.data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `torch.utils.data` gibt es zwei Funktionen, die Sie brauchen:\n",
    "\n",
    "* `data.TensorDataset(input,labels)` erstellt ein PyTorch Dataset aus den Daten\n",
    "* `data.DataLoader(Dataset, batch_size)`  erstellt Minibatches der angegebenen Größe aus einem Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=data.TensorDataset(train_images, train_labels) # input sind unsere Tensors die einmal die Bilder und einmal die Labels beinhalten\n",
    "loader = data.DataLoader(train_data, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Variable `loader` enthält nun 1875 Minibatches mit jeweils 32 Bilder und deren 32 Labels. In der nächsten Zelle können Sie den Inhalt des ersten Batches sehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(loader)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um alles zusammenzubringen, brauchen Sie einen zweiten `for-loop`, der innerhalb des ersten `for-loops` die Minibatches nach einander auswählt und durch das Netzwerk führt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Definieren von Netzwerk, LossFunktion und Update Algorithmus\n",
    "netzwerk = nn.Sequential(nn.Linear(784,300), \n",
    "                         nn.ReLU(), \n",
    "                         nn.Linear(300,300),\n",
    "                         nn.ReLU(), \n",
    "                         nn.Linear(300,10))\n",
    "loss_funktion = nn.CrossEntropyLoss()\n",
    "updaten = optim.SGD(netzwerk.parameters(), lr=0.3)\n",
    "EPOCHS = 2\n",
    "\n",
    "## Trainings Loop\n",
    "for i in range(EPOCHS):\n",
    "    loss_list = [] # diese Liste speichter den Loss jedes Minibatches\n",
    "                   # damit können wir am Ende den Durschnittslost innerhalb des Epochs berechnen\n",
    "    for minibatch in loader: # for-loop geht durch alle Minibatches\n",
    "        images, labels = minibatch # minibatch wird in Bilder und Labels geteilt\n",
    "        \n",
    "        updaten.zero_grad()\n",
    "        output = netzwerk(images) # Forward Propagation\n",
    "    \n",
    "        loss   = loss_funktion(output, labels)\n",
    "        loss.backward()\n",
    "        loss_list.append(loss.item())\n",
    "        updaten.step()\n",
    "        \n",
    "    output = netzwerk(train_images)\n",
    "    acc=((output.max(dim=1)[1]==train_labels).sum()/float(output.shape[0])).item()\n",
    "    print(\n",
    "        \"Training Loss: %.2f Training Accuracy: %.2f\"\n",
    "        % (np.mean(loss_list), acc)\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nach nur zwei Epochs ist die Accuracy viel niedriger als jemals zuvor. Dafür schicken wir noch einmal das ganze Dataset durch das Netzwerk und berechnen die Genauigkeit. Der einzelne Epoch dauert viel länger, aber die Trainingszeit verkürzt sich insgesamt.\n",
    "Um die Genauigkeit auszurechnen wird, nachdem all Minibatches durch das Netzwerk gegangen sind, noch einmal der ganze Datensatz durch das Netzwerk geschickt (ohne die Weights zu verändern) und die Genauigkeit berechnet. Der Epochloss ist der durchschnittliche Loss der Minibatches.\n",
    "\n",
    "Tipp: Wenn man den Wert eines Tensors und nicht den Tensor weitergeben will, kann man `tensor.item()` benutzen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fortgeschrittene Layers\n",
    "\n",
    "Wir werden uns im Folgenden mit den fortgeschrittenen Layern beschäftigen.\n",
    "\n",
    "## Dropout\n",
    "\n",
    "Dropout wird während des Trainings benutzt, um einzelne Neuronen zufällig aus dem Netzwerk zu *droppen*. Sie also kurzzeitig aus dem Netzwerk zuwerfen. Mathematisch bedeutet das, dass ihr Output einfach auf null gesetzt wird.\n",
    "Jedes Mal, wenn ein Batch durch das Netzwerk geführt wird, wird erneut *gelost, welche Neuronen* für diesen Batch entfernt werden.\n",
    "\n",
    "Dieses zufällige Löschen von Neuronen zwingt das Netzwerk dazu, sich nicht auf einzelne Neuronen verlassen zu können. Ähnlich wie beim Random Forest, wo zufällige Variablen entfernt werden, wird mittels Dropout overfitting verhindert.\n",
    "Zum Evaluieren des Netzwerkes werden aber keine der Neuronen mehr entfernt. Das heißt, die Dropout-Layer wird übergangen.\n",
    "\n",
    "Wie viele Neuronen gelöscht werden, ist eine Hyperparameter, der so wie die Learningrate von euch selber gewählt werden kann. Der Dropout wird in Prozenten angeben. Also ein Dropout von `0.8` bedeutet, dass in dieser Layerr 80 % der Neuronen entfernt werden. Ein Standardwert für den Dropout ist `0.2`. Oft wird auch direkt nach dem Input eine Dropout-Layer benutzt.\n",
    "\n",
    "In PyTorch ist die Dropout-Layer wie folgt definiert: `nn.Dropout(0.2)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1235)\n",
    "\n",
    "beispiel_x = torch.tensor([[1.,2.,3.,4.,5.]] )\n",
    "do = nn.Dropout(0.5)\n",
    "do(beispiel_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drei der Werte wurden auf `0.` gesetzt, die anderen Werte aber haben sich verdoppelt. **Warum ist das passiert?**\n",
    "\n",
    "Das liegt daran, dass wir mit Dropout trainieren, aber ohne Dropout evaluieren werden. Das bedeutet, dass, bei einem Dropout von `0.5`, nur die Hälfte der Neuronen genutzte werden bzw. eine Hälfte auf null gesetzt. In der Forwardpropagation werden die Werte als gewichtete Summe weitergegeben. Wenn die Hälfte der Neuronen den Wert `0` hat, ist die Summe natürlich viel kleiner als bei einem Netzwerk, bei dem kein Dropout verwendet wird. Die Weights passen sich dementsprechend auch in einer Weise an, die nur den Input von 50 % der Neuronen erhält. \n",
    "\n",
    "Ohne Dropout: $$z = \\beta_0 + \\beta_11 + \\beta_22 +\\beta_33 +\\beta_44 +\\beta_55$$\n",
    "\n",
    "Mit Dropout: $$\\begin{align}z&= \\beta_0 + \\beta_10 + \\beta_22 +\\beta_30 +\\beta_44 +\\beta_50 \\\\\n",
    "&=\\beta_0 + \\beta_22+\\beta_44\\end{align}$$\n",
    "\n",
    "\n",
    "\n",
    "Die Summe $z$ mit Dropout ist immer kleiner als die ohne. Das Skalieren der Weights sorgt also dafür, dass die Summe ähnlich bleibt.\n",
    "\n",
    "Während der Evaluierung, in der kein Dropout verwendet wird, haben wir auf einmal doppelt so viele Inputs, aber unsere Weights erwarten nur die Hälfte. Um diese Diskrepanz zwischen Training und Evaluation zu verhindern, skaliert der Dropout automatisch während des Trainings die Werte.\n",
    "\n",
    "Sie können eine `Layer` oder ein komplettes Netzwerk vom Trainings- zum Evaluierungsmodus wechseln, indem  Sie `netzwerk.eval()` benutzen. Um es in den Trainingsmodus zu konvertieren wird `.train()` benutzt. WICHTIG ist hierbei, dass per Default jede Layer und jedes Netzwerk sich im `train()` Modus befindet.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "do.eval()\n",
    "do(beispiel_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im `.eval()` Modus wird der Dropout nicht angewendet.\n",
    "\n",
    "## Batchnorm\n",
    "\n",
    "Batchnorm Layers sind eine weiter beliebte Layer in neuronalen Netzwerken. Wie der Name suggeriert, werden hier die Batches bzw. die Activations Minibatches normalisiert.\n",
    "Erinnern Sie sich noch daran, dass wir unseren Input skalieren. Das Gleiche macht Batchnorm, allerdings im  Netzwerk selber, sodass auch später im Netzwerk die Werte ungefähr die gleiche Skala haben. Grundsätzlich werden Variablen wie folgt normalisiert:\n",
    "\n",
    "$$x_s = \\frac{x-\\bar{x}}{sd_x}$$\n",
    "\n",
    "Hier steht $\\bar{x}$ für den Mittelwert und $sd_x$ ist die Standardabweichung (standard deviation) von $x$.<br><br>\n",
    "In einem klassischen neuronalen Netzwerk werden die Actvations einer Layer in zwei Schritten berechnet. Zunächst wird die lineare Transformation durchgeführt:\n",
    "\n",
    "$$Z = XW^T+b$$\n",
    "*Hier ist $X$ ein Minibatch, also zum Beispiel nur 32 Bilder*\n",
    "\n",
    "Darauf folgt eine nicht-lineare Aktivierungsfunktion:\n",
    "\n",
    "$$A = \\sigma(Z)$$\n",
    "\n",
    "Mit Batchnorm werden die Werte vor der Aktivierungsfunktion noch einmal normalisiert.\n",
    "\n",
    "$$Z_s = (\\frac{Z-\\bar{Z}}{sd_Z})  \\cdot \\gamma + \\beta $$\n",
    "\n",
    "Diese wird für jede Spalte, oder auch jees Neuron unabhängig voneinander getan. Der Mittelwert $\\bar{Z}$ und die Standardabweichung $sd_Z$ werden hier auch nur per Minibatch berechnet. Was neu ist, ist das $\\gamma$ und $\\beta$. Das sind nur zwei einzelne Parameter, die die Normalverteilung $N(0,1)$ verschieben und skalieren können. Diese beiden Parameter sind auch Lernbar, das heißt Sie werden während des Trainings auch verändert.\n",
    "\n",
    "Wichtig ist auch zu beachten, dass während des Trainings die Mittelwerte der Minibatches als neue Mittelwerte  zusammengefasst werden. Dieser Mittelwert der Mittelwerte wird dann in der Testphase benutzt, um die Batches zu normalisieren.\n",
    "\n",
    "*Ob man erst die Aktivierungsfunktion anwendet und dann Batchnorm oder andersrum ist eine Frage, die Ihnen keiner beantworten kann. Es gibt solche und solche Antworten.*\n",
    "\n",
    "Vervollständigen Sie die `layer_one`. Die Größe der Hidden Layer und des Dropouts ist Ihnen überlassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_x, batch_y = next(iter(loader)) # select the first batch\n",
    "\n",
    "layer_one = nn.Sequential(nn.Linear(____,___),\n",
    "                         nn.BatchNorm1d(_____),\n",
    "                         nn.ReLU(),\n",
    "                         nn.Dropout(____))\n",
    "layer_one(batch_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>Lösung:</b></summary>\n",
    "\n",
    "```python\n",
    "layer_one = nn.Sequential(nn.Linear(784,300),\n",
    "                         nn.BatchNorm1d(300),\n",
    "                         nn.ReLU(),\n",
    "                         nn.Dropout(0.2))\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erweitern Sie nun das komplette Netzwerk von vorhin.\n",
    "Wichtig: nach der letzten Linear Layer wird weder Batchnorm noch Dropout verwendet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netzwerk = nn.Sequential(nn.Linear(784,300), \n",
    "                         ______________,\n",
    "                         ______________,\n",
    "                         ______________,\n",
    "                         nn.Linear(300,300),\n",
    "                         ______________,\n",
    "                         ______________,\n",
    "                         ______________,\n",
    "                         nn.Linear(300,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>Lösung:</b></summary>\n",
    "\n",
    "```python\n",
    "netzwerk = nn.Sequential(nn.Linear(784,300), \n",
    "                         nn.BatchNorm1d(300),\n",
    "                         nn.ReLU(),\n",
    "                         nn.Dropout(0.2),\n",
    "                         nn.Linear(300,300),\n",
    "                         nn.BatchNorm1d(300),\n",
    "                         nn.ReLU(),\n",
    "                         nn.Dropout(0.2),\n",
    "                         nn.Linear(300,10))\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers\n",
    "\n",
    "Optimizers bestimmen wie genau die Weight des Netzwerks geupdated werden. Bis jetzt haben Sie immer den `SGD` benutzt. Allerdings wird dieser simple Algorithmus nur noch selten verwendet. In der PyTorch Implementation gibt es weitere Parameter für `SGD`, die das Training effektiver gestalten können. Einer ist das `momentum`. Wenn Sie Momentum zum Trainieren benutzten, werden Weights nicht nur nach den Gradients des jetzigen Minibatches upgedatet. Sondern, die Gradients der vorherigen Minibatches haben auch einen Einfluss. Der `momentum` parameter gibt an, wie stark oder schwach der Einfluss vorheriger Minibatches ist.\n",
    "\n",
    "Durch das Momentum soll der Loss geradliniger optimiert werden. Als Beispiel kann man sich einen Ball vorstellen, der einen Berg herunterrollt. Je länger in dieselbe Richtung rollt, desto schneller wird er. Und je schneller er wird, desto weniger Einflüsse haben kleine Richtungsänderung, die durch Veränderung des Terrains (Gradienten) ausgelöst werden. \n",
    "\n",
    "So ist es auch mit dem Loss, wenn die Gradients eine Zeit lang in dieselbe Richtung gehen, dann sollte nicht ein Minibatch diese Richtung auf einmal ändern. \n",
    "\n",
    "Einer der meistbenutzten Algorithmen zum Trainieren von Netzwerken ist der `ADAM` Algorithmus. Er kombiniert viele Verbesserungen des SGD, unter anderem auch eine Version des Momentums.  \n",
    "\n",
    "**ADAM muss aber nicht immer besser sein, als SGD**\n",
    "\n",
    "Eine Übersicht alle verfügbaren Algorithmen können sie [hier](https://pytorch.org/docs/stable/optim.html#algorithms) finden. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss_funktion = nn.CrossEntropyLoss()\n",
    "updaten = optim.Adam(netzwerk.parameters(), lr=0.1)\n",
    "EPOCHS = 10\n",
    "\n",
    "## Trainings Loop\n",
    "for i in range(EPOCHS):\n",
    "    loss_list = [] # diese Liste speichter den Loss jedes Minibatches\n",
    "                   # damit können wir am Ende den Durschnittslost innerhalb des Epochs berechnen\n",
    "    netzwerk.train()\n",
    "    for minibatch in loader: # for-loop geht durch alle minibatches\n",
    "        images, labels = minibatch # minibatch wird in Bilder und Labels geteilt\n",
    "        \n",
    "        updaten.zero_grad()\n",
    "        output = netzwerk(images) # Forward Propagation\n",
    "    \n",
    "        loss   = loss_funktion(output, labels)\n",
    "        loss.backward()\n",
    "        loss_list.append(loss.item())\n",
    "        updaten.step()\n",
    "    netzwerk.eval()    \n",
    "    output = netzwerk(train_images)\n",
    "    acc=((output.max(dim=1)[1]==train_labels).sum()/float(output.shape[0])).item()\n",
    "    \n",
    "    print(i,\n",
    "        \"Training Loss: %.2f Training Accuracy: %.2f\"\n",
    "        % (np.mean(loss_list), acc)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Übungsaufgabe\n",
    "\n",
    "Für die Übungsaufgabe werden Sie ein Netzwerk trainieren, diesmal aber mit den Toxizitätsdaten aus Woche 5.\n",
    "Zunächst laden Sie wieder alle notwendigen Libraries und Daten. Benutzen Sie diemal auch Batchnorm, Dropout und den ADAM Algorithmus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "%run ../utils/utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_tox = pd.read_csv(\"../data/toxicity/sr-mmp.tab\", sep = \"\\t\")\n",
    "data_tox = data_tox.iloc[:,1:] #alle Spalten bis auf die erste (index 0) werden ausgewählt\n",
    "data_tox.columns = [\"smiles\", \"activity\"]\n",
    "data_tox.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als Nächstes berechnen Sie die Fingerprints. Dafür steht ihnen wie in Woche 05 die Funktion `get_fingerprints` zur Verfügung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fps = get_fingerprints(data_tox)\n",
    "fps[\"activity\"] = data_tox.activity\n",
    "fps.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bevor Sie diese in Pytorch verwenden können, müssen Sie die sowohl die Fingerprints als auch die `activity` in `tensors` umwandeln. Beachten Sie, dass beide im DataFrame `fps` sind. \n",
    "\n",
    "`.values` konvertiert einen Datenframe in ein `np.array`.\n",
    "\n",
    "Danach teilen Sie die Daten in Training und Testset auf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fps = torch.tensor(___.values, dtype=torch.float32) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test=train_test_split(fps,test_size= 0.2 , train_size= 0.8, random_state=1234)\n",
    "\n",
    "\n",
    "train_x = train[:,:-1]\n",
    "train_y = train[:,-1]\n",
    "test_x = test[:,:-1]\n",
    "test_y = test[:,-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auch jetzt wollen wir wieder Minibatches benutzten. Deswegen müssen wir unsere Trainingsdaten noch in zu einem `DataLoader` konvertieren. Warum nur die Trainingsdaten? Das Benutzen von Batches ist erst einmal nur relevant für das Training. Solange ihr Computer fähig ist den Testdatensatz auf einmal durch das Netzwerk zu führen, müssen wir den Testdatensatz nicht in Batches unterteilen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=data.TensorDataset(______, _____) # input sind unsere Tensors die einmal die Fingerprints die activities\n",
    "loader=data.DataLoader(train_data, batch_size = 32)\n",
    "len(loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passen Sie das Netzwerk so an, dass der Input und Output die richtige Größe haben. Also die Länge der Fingerprints und die Anzahl der Klassen, die wir vorhersagen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netzwerk = nn.Sequential(nn.Linear(), \n",
    "                         nn.BatchNorm1d(),\n",
    "                         nn.ReLU(), \n",
    "                         nn.Dropout(),\n",
    "                         nn.Linear(),\n",
    "                         nn.BatchNorm1d(),\n",
    "                         nn.ReLU(), \n",
    "                         nn.Dropout(),\n",
    "                         nn.Linear())\n",
    "\n",
    "loss_funktion = nn.BCEWithLogitsLoss()\n",
    "updaten = ___________________, lr=0.1)    \n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als letztes füllen Sie den `for loop`. \n",
    "\n",
    "`.squeeze` konvertiert den `(n,1)` `output` tensor zu einem 1-dimensionalen Tensor der Länge `n`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(EPOCHS):\n",
    "    loss_list = [] # diese Liste speichter den Loss jedes Minibatches\n",
    "    for minibatch in loader: # for-loop geht durch alle minibatches\n",
    "        updaten.__________\n",
    "        molecules, activity = minibatch # minibatch wird in Bilder und Labels geteilt\n",
    "        output = netzwerk(____________) # Forward Propagation\n",
    "        loss   = loss_funktion(output.squeeze(), ____________)\n",
    "        loss._______\n",
    "        loss_list.append(loss.item())\n",
    "        updaten.________\n",
    "    # Hier wird die Accuracy für den Testsatz berechnet\n",
    "    output = netzwerk(test_x)\n",
    "    acc = torch.sum((output>0).squeeze().int() == test_y)/float(test_y.shape[0])\n",
    "   \n",
    "    print(\n",
    "        \"Training Loss: %.2f Test Accuracy: %.2f\"\n",
    "        % (np.mean(loss_list), acc.item())\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
