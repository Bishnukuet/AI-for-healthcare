{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network\n",
    "\n",
    "Diese Woche beschäftigen wir uns mit Convolutional Neuroal Network. Convolution Neural Networks (CNN) werden vorallem, aber nicht außschließlich, für die Bilderkennung benutzt.\n",
    "\n",
    "Anders als Neuronale Netzwerke die wir bis jetzt kennen gelernt haben, können CNNs Bilder als eine Matrix einlesen. Das bedeutet lokale Zusammenhängen werden nicht durch das `flatten` der Bilder gebrochen.  \n",
    "\n",
    "![](https://miro.medium.com/max/1280/1*h01T_cugn22R2zbKw5a8hA.gif)\n",
    "\n",
    "<centering><h7> Otavio Good. 2017 \"A Visual and Intuitive Understanding of Deep Learning\" *O'Reilly AI Conference* </h7></centering>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils import data\n",
    "\n",
    "def min_max(x):\n",
    "    return (x - np.min(x)) / (np.max(x) - np.min(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zunächst laden Sie wieder die Trainingsdaten ein und konvertieren Sie zu einem Tensor. Insgesamt sind es 60,000 Bilder und 784 Pixel + eine Spalte für die Labels der Bilder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 784]) torch.Size([60000])\n"
     ]
    }
   ],
   "source": [
    "train_data = np.genfromtxt('../data/mnist/mnist_train.csv', delimiter=',', skip_header =False)\n",
    "\n",
    "train_x = torch.tensor(min_max(train_data[:,1:]), dtype=torch.float32)\n",
    "train_y = torch.tensor(train_data[:,0], dtype=torch.long)\n",
    "\n",
    "print(train_x.shape, train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bisher haben wir Bilder immer als 1D Input in unsere Neuronales Netzwerk eingeführt. Wir wollen diesmal aber die 2D Struktur benutzen. Dafür müssen wir aus einem Vektor der Länge `784` eine Matrix mit den Maßen `28 x 28` machen.\n",
    "\n",
    "Hierfür können Sir die Funktion `vektor.view(28,28)` benutzen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 28])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[0,:].view(28,28).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir können uns dieses Bild anschauen, können aber nicht viel erkennen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[28, 28]' is invalid for input of size 4096",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1960182/3399191638.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[28, 28]' is invalid for input of size 4096"
     ]
    }
   ],
   "source": [
    "train_x[0,:].view(28,28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doch mithilfe von `matplotlib` können wir Arrays als Bild darstellen. `cmap = \"greys\"` gibt hierbei an, dass wir unser Farbspektrum nur in Schwarz-Weiß haben wollen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd564074750>"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAN80lEQVR4nO3df6hcdXrH8c+ncf3DrBpTMYasNhuRWBWbLRqLSl2RrD9QNOqWDVgsBrN/GHChhEr6xyolEuqP0qAsuYu6sWyzLqgYZVkVo6ZFCF5j1JjU1YrdjV6SSozG+KtJnv5xT+Su3vnOzcyZOZP7vF9wmZnzzJnzcLife87Md879OiIEYPL7k6YbANAfhB1IgrADSRB2IAnCDiRxRD83ZpuP/oEeiwiPt7yrI7vtS22/aftt27d281oAesudjrPbniLpd5IWSNou6SVJiyJia2EdjuxAj/XiyD5f0tsR8U5EfCnpV5Ku6uL1APRQN2GfJekPYx5vr5b9EdtLbA/bHu5iWwC61M0HdOOdKnzjND0ihiQNSZzGA03q5si+XdJJYx5/R9L73bUDoFe6CftLkk61/V3bR0r6kaR19bQFoG4dn8ZHxD7bSyU9JWmKpAci4o3aOgNQq46H3jraGO/ZgZ7ryZdqABw+CDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUii4ymbcXiYMmVKsX7sscf2dPtLly5tWTvqqKOK686dO7dYv/nmm4v1u+66q2Vt0aJFxXU///zzYn3lypXF+u23316sN6GrsNt+V9IeSfsl7YuIs+toCkD96jiyXxQRH9TwOgB6iPfsQBLdhj0kPW37ZdtLxnuC7SW2h20Pd7ktAF3o9jT+/Ih43/YJkp6x/V8RsWHsEyJiSNKQJNmOLrcHoENdHdkj4v3qdqekxyTNr6MpAPXrOOy2p9o++uB9ST+QtKWuxgDUq5vT+BmSHrN98HX+PSJ+W0tXk8zJJ59crB955JHF+nnnnVesX3DBBS1r06ZNK6577bXXFutN2r59e7G+atWqYn3hwoUta3v27Cmu++qrrxbrL7zwQrE+iDoOe0S8I+kvauwFQA8x9AYkQdiBJAg7kARhB5Ig7EASjujfl9om6zfo5s2bV6yvX7++WO/1ZaaD6sCBA8X6jTfeWKx/8sknHW97ZGSkWP/www+L9TfffLPjbfdaRHi85RzZgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJxtlrMH369GJ948aNxfqcOXPqbKdW7XrfvXt3sX7RRRe1rH355ZfFdbN+/6BbjLMDyRF2IAnCDiRB2IEkCDuQBGEHkiDsQBJM2VyDXbt2FevLli0r1q+44opi/ZVXXinW2/1L5ZLNmzcX6wsWLCjW9+7dW6yfccYZLWu33HJLcV3UiyM7kARhB5Ig7EAShB1IgrADSRB2IAnCDiTB9ewD4JhjjinW200vvHr16pa1xYsXF9e9/vrri/W1a9cW6xg8HV/PbvsB2zttbxmzbLrtZ2y/Vd0eV2ezAOo3kdP4X0i69GvLbpX0bEScKunZ6jGAAdY27BGxQdLXvw96laQ11f01kq6uty0Adev0u/EzImJEkiJixPYJrZ5oe4mkJR1uB0BNen4hTEQMSRqS+IAOaFKnQ287bM+UpOp2Z30tAeiFTsO+TtIN1f0bJD1eTzsAeqXtabzttZK+L+l429sl/VTSSkm/tr1Y0u8l/bCXTU52H3/8cVfrf/TRRx2ve9NNNxXrDz/8cLHebo51DI62YY+IRS1KF9fcC4Ae4uuyQBKEHUiCsANJEHYgCcIOJMElrpPA1KlTW9aeeOKJ4roXXnhhsX7ZZZcV608//XSxjv5jymYgOcIOJEHYgSQIO5AEYQeSIOxAEoQdSIJx9knulFNOKdY3bdpUrO/evbtYf+6554r14eHhlrX77ruvuG4/fzcnE8bZgeQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJxtmTW7hwYbH+4IMPFutHH310x9tevnx5sf7QQw8V6yMjIx1vezJjnB1IjrADSRB2IAnCDiRB2IEkCDuQBGEHkmCcHUVnnnlmsX7PPfcU6xdf3Plkv6tXry7WV6xYUay/9957HW/7cNbxOLvtB2zvtL1lzLLbbL9ne3P1c3mdzQKo30RO438h6dJxlv9LRMyrfn5Tb1sA6tY27BGxQdKuPvQCoIe6+YBuqe3XqtP841o9yfYS28O2W/8zMgA912nYfybpFEnzJI1IurvVEyNiKCLOjoizO9wWgBp0FPaI2BER+yPigKSfS5pfb1sA6tZR2G3PHPNwoaQtrZ4LYDC0HWe3vVbS9yUdL2mHpJ9Wj+dJCknvSvpxRLS9uJhx9sln2rRpxfqVV17ZstbuWnl73OHir6xfv75YX7BgQbE+WbUaZz9iAisuGmfx/V13BKCv+LoskARhB5Ig7EAShB1IgrADSXCJKxrzxRdfFOtHHFEeLNq3b1+xfskll7SsPf/888V1D2f8K2kgOcIOJEHYgSQIO5AEYQeSIOxAEoQdSKLtVW/I7ayzzirWr7vuumL9nHPOaVlrN47eztatW4v1DRs2dPX6kw1HdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgnH2SW7u3LnF+tKlS4v1a665plg/8cQTD7mnidq/f3+xPjJS/u/lBw4cqLOdwx5HdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgnH2w0C7sexFi8abaHdUu3H02bNnd9JSLYaHh4v1FStWFOvr1q2rs51Jr+2R3fZJtp+zvc32G7ZvqZZPt/2M7beq2+N63y6ATk3kNH6fpL+PiD+X9FeSbrZ9uqRbJT0bEadKerZ6DGBAtQ17RIxExKbq/h5J2yTNknSVpDXV09ZIurpHPQKowSG9Z7c9W9L3JG2UNCMiRqTRPwi2T2ixzhJJS7rsE0CXJhx229+W9Iikn0TEx/a4c8d9Q0QMSRqqXoOJHYGGTGjozfa3NBr0X0bEo9XiHbZnVvWZknb2pkUAdWh7ZPfoIfx+Sdsi4p4xpXWSbpC0srp9vCcdTgIzZswo1k8//fRi/d577y3WTzvttEPuqS4bN24s1u+8886WtccfL//KcIlqvSZyGn++pL+V9LrtzdWy5RoN+a9tL5b0e0k/7EmHAGrRNuwR8Z+SWr1Bv7jedgD0Cl+XBZIg7EAShB1IgrADSRB2IAkucZ2g6dOnt6ytXr26uO68efOK9Tlz5nTSUi1efPHFYv3uu+8u1p966qli/bPPPjvkntAbHNmBJAg7kARhB5Ig7EAShB1IgrADSRB2IIk04+znnntusb5s2bJiff78+S1rs2bN6qinunz66acta6tWrSque8cddxTre/fu7agnDB6O7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQRJpx9oULF3ZV78bWrVuL9SeffLJY37dvX7FeuuZ89+7dxXWRB0d2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUjCEVF+gn2SpIcknSjpgKShiPhX27dJuknS/1ZPXR4Rv2nzWuWNAehaRIw76/JEwj5T0syI2GT7aEkvS7pa0t9I+iQi7ppoE4Qd6L1WYZ/I/Owjkkaq+3tsb5PU7L9mAXDIDuk9u+3Zkr4naWO1aKnt12w/YPu4FusssT1se7i7VgF0o+1p/FdPtL8t6QVJKyLiUdszJH0gKST9k0ZP9W9s8xqcxgM91vF7dkmy/S1JT0p6KiLuGac+W9KTEXFmm9ch7ECPtQp729N425Z0v6RtY4NefXB30EJJW7ptEkDvTOTT+Ask/Yek1zU69CZJyyUtkjRPo6fx70r6cfVhXum1OLIDPdbVaXxdCDvQex2fxgOYHAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJ9HvK5g8k/c+Yx8dXywbRoPY2qH1J9NapOnv7s1aFvl7P/o2N28MRcXZjDRQMam+D2pdEb53qV2+cxgNJEHYgiabDPtTw9ksGtbdB7Uuit071pbdG37MD6J+mj+wA+oSwA0k0Enbbl9p+0/bbtm9toodWbL9r+3Xbm5uen66aQ2+n7S1jlk23/Yztt6rbcefYa6i322y/V+27zbYvb6i3k2w/Z3ub7Tds31Itb3TfFfrqy37r+3t221Mk/U7SAknbJb0kaVFEbO1rIy3YflfS2RHR+BcwbP+1pE8kPXRwai3b/yxpV0SsrP5QHhcR/zAgvd2mQ5zGu0e9tZpm/O/U4L6rc/rzTjRxZJ8v6e2IeCcivpT0K0lXNdDHwIuIDZJ2fW3xVZLWVPfXaPSXpe9a9DYQImIkIjZV9/dIOjjNeKP7rtBXXzQR9lmS/jDm8XYN1nzvIelp2y/bXtJ0M+OYcXCarer2hIb7+bq203j309emGR+YfdfJ9OfdaiLs401NM0jjf+dHxF9KukzSzdXpKibmZ5JO0egcgCOS7m6ymWqa8Uck/SQiPm6yl7HG6asv+62JsG+XdNKYx9+R9H4DfYwrIt6vbndKekyjbzsGyY6DM+hWtzsb7ucrEbEjIvZHxAFJP1eD+66aZvwRSb+MiEerxY3vu/H66td+ayLsL0k61fZ3bR8p6UeS1jXQxzfYnlp9cCLbUyX9QIM3FfU6STdU92+Q9HiDvfyRQZnGu9U042p43zU+/XlE9P1H0uUa/UT+vyX9YxM9tOhrjqRXq583mu5N0lqNntb9n0bPiBZL+lNJz0p6q7qdPkC9/ZtGp/Z+TaPBmtlQbxdo9K3ha5I2Vz+XN73vCn31Zb/xdVkgCb5BByRB2IEkCDuQBGEHkiDsQBKEHUiCsANJ/D+f1mbt6t55/AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_x[0,:].view(28,28), cmap= \"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir haben bis jetzt nur ein Bild in das richtige Format gebracht, um das für all Bilder zu machen, können wir auch `.view()` benutzen. Der Tensor von oben hatte den Format `(Höhe,Breite)`. Damit wir alle Bilder konvertieren können, müssen wir den `tensor` um eine extra Dimension erweitern.  Der neue `tensor` sollte die folgenden Dimensionen `(Anzahl Bilder, Höhe, Breite)` haben. Wir haben also insgesamt drei Dimensionen.\n",
    "\n",
    "Allerdings würde hier PyTorch einen Strich durch die Rechnung machen. Denn PyTorch kann sowohl mit Schwarz-Weiß (s/w),  als auch mit farbigen Bildern arbeiten. In PyTorch werden farbige Bilder über drei Matrizen dargestellt. Eine für Rot, eine für Grün und eine für Blau. Diese werden auch als Channel bezeichnet. Ein farbiges Bild würde in PyTorch die Dimensionen `(3, Höhe, Breite)` haben. Die Dimension, die wir gerade noch für die `Anzahl Bilder` verwendet haben, ist also mit der `Anzahl Channels` belegt.\n",
    "\n",
    "\n",
    "![](https://miro.medium.com/max/700/1*icINeO4H7UKe3NlU1fXqlA.jpeg)\n",
    "\n",
    "<center><h7>Source: Mathanraj Sharma, 2019 </h7></center>\n",
    "\n",
    "PyTorch erwartet diese \"Channel Dimension\" auch für s/w Bilder. \n",
    "Deshalb stellen wir ein s/w Bild wie folgt dar: `(1, Höhe, Breite)`. \n",
    "\n",
    "Daraus folgt, dass alle Bilder vom MNIST Datensatz diesem Format entsprechen müssen: `(Anzahl Bilder, 1, Höhe, Breite)`. Also insgesamt hat unser Input Tensor 4 Dimensionen.\n",
    "\n",
    "\n",
    "\n",
    "Konvertieren Sie `train_x` in dieses Format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x.view(_____,1,____,____)\n",
    "train_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Lösung:</b></summary>\n",
    "    \n",
    "```python \n",
    "train_x = train_x.view(60000,1,28,28)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sie haben jetzt alle Bilder in das Format `(1,28,28)` konvertiert.\n",
    "Bilder können Sie sich immer noch mit `plt.imshow` anzeigen lassen.\n",
    "\n",
    "Beachten Sie, wie jetzt der Tensor indiziert ist. `[0,0,:,:]`. Wir wählen das erste Bild aus, und auch den ersten und einzigen Channel. Wir wählen die gesamte Höhe und Breite aus, um das Bild komplett darzustellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(train_x[0,0,:,:], cmap= \"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie letzte Woche können Sie sich auch diesmal einen `DataLoader` benutzen. Dafür müssen Sie erst ein TensorDataset erstellen. Mit `next(iter())` können Sie sich den ersten Batch des Dataloaders ausgeben lassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_train = data.TensorDataset(_____, ____)\n",
    "train_loader = data.DataLoader(______, batch_size=32)\n",
    "\n",
    "batch_x, batch_y =next(iter(train_loader))\n",
    "print(batch_x.shape, batch_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Lösung:</b></summary>\n",
    "    \n",
    "```python \n",
    "torch_train = data.TensorDataset(train_x,train_y)\n",
    "train_loader = data.DataLoader(torch_train, batch_size=32)\n",
    "\n",
    "batch_x, batch_y =next(iter(train_loader))\n",
    "print(batch_x.shape, batch_y.shape)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie Sie sehen können, hat der `batch_x` die Dimensionen `[32, 1, 28, 28]`. Also `32` Bilder, die Größe unseres Batches, `1`Channel, `28` Pixel in der Höhe und `28` in der Breite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNNs in PyTorch erstellen.\n",
    "\n",
    "Wir haben so weit unsere Daten im richtigen Format, jetzt beschäftigen wir uns mit dem Erstellen von `CNN` in PyTorch. Sowie es `linear` layers in PyTorch gibt, gibt es auch Convolutional Layers im `nn` Modul.\n",
    "\n",
    "`nn.Conv2d()` ist so eine Layer. Bevor wir Sie benutzen, besprechen wir kurz die wichtigsten Parameter.\n",
    "\n",
    "- `in_channels` die Anzahl der Channels, die das Bild vor der Convolution hat \n",
    "- `out_channels` wie viel Channels soll, das Bild nach der Convolution haben oder auch wie viele Filter lassen wir über das Bild laufen.\n",
    "- `kernel_size` wie groß ist der Kernel, also die Höhe/Breite in Pixeln\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = conv1 = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = conv1(batch_x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sie sehen, der Batch hat sich in der Größe verändert. Zunächst haben wir immer noch `32` Bilder, allerdings wie spezifiziert, haben wir jetzt `3` Channel. Auch die Höhe und Breite unseres Bildes hat sich verändert. Wir haben jeweils 2 Pixel pro Dimension verloren. Das liegt daran, wie Convolutions funktionieren.\n",
    "\n",
    "![](https://miro.medium.com/max/700/1*L4T6IXRalWoseBncjRr4wQ@2x.gif)\n",
    "<center><h7>Source: Arden Dertat, 2017 </h7></center>\n",
    "\n",
    "Hier sehen an einem Beispiel, warum bei einer Kernel Size von 3 unser Output Bild um zwei Pixel kleiner wird. Links ist das Inputbild und rechts der Output. Da wir den Kernel nicht über den Rand des Bildes schieben können, „verlieren“ wir den äußern Rand des Bildes.\n",
    "\n",
    "Um zu verhindern, dass diese Information verloren geht, können wir das Bild *padden*. Dadurch vergrößern wir das Bild, zum Beispiel mit Pixel, die den Wert Null haben.\n",
    "![](https://miro.medium.com/max/700/1*W2D564Gkad9lj3_6t9I2PA@2x.gif)\n",
    "<center><h7>Source: Arden Dertat, 2017 </h7></center>\n",
    "\n",
    "Durch das Padding, kann der Kernel einmal über das ganze Bild geschoben werden.\n",
    "Wir können die Breite des Paddings auch als Parameter in `Conv2d` mitgeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 64, 64])"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1 = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=3, padding =1)\n",
    "out = conv1(batch_x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durch das Padding schrumpft nun das Bild nicht mehr. Dadurch, dass wir jetzt `3` Channel haben, können wir das Bild immer noch mit `plt.imshow` darstellen.  Hierzu müssen wir ein Bild aus dem Batch auswählen und mit dem Befehl `detach()` die Gradienten, die durch `autograd` gespeichert werden, entfernen.\n",
    "\n",
    "*Ein solches Bild, kann man nur als Beispiel benutzen um die Transformation zu verdeutlichen. Die tatsächlichen Farben und Intensitäten sind hier aber irrelevant, da Sie von dem Netzwerk arbiträr gewählt sind. Zum Beispiel, gibt die Reihenfolge der Channels an, welcher Channel für welche Farbe zuständig ist. Ein Convolution ist sich natürlich nicht bewusst, dass es so eine Ordnung in den Channels gibt.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd54d7ed690>"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(min_max(out.detach().numpy()[0].transpose((1, 2, 0))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sie können immer noch eine 5 erkennen, allerdings diesmal in Farbe. Wie oben schon beschrieben, eigenen sich die Farben nicht zum Interpretieren. Es soll lediglich die Diversifizierung des Inputs darstellen. \n",
    "\n",
    "Der weitere neue Layer, die Sie heute benutzen werden, ist `nn.MaxPool2d()`. \n",
    "\n",
    "Diese Layer wird als  **Pooling** Layer bezeichnet.\n",
    "\n",
    "\n",
    "![](https://miro.medium.com/max/700/1*ReZNSf_Yr7Q1nqegGirsMQ@2x.png)\n",
    "<center><h7>Source: Arden Dertat, 2017 </h7></center>\n",
    "\n",
    "Pooling Layers führen zu einer gewollten Reduzierung der Bildgröße. Dadurch brauchen weniger Parameter (Weights), was dazu führt, dass unsere Netzwerke schneller trainieren. Wenn Sie ein Bild (größer als 28 x 28 Pixel) anschauen, dann erkennen Sie nicht jeden einzelnen Pixel, sondern Pixel in einer gewissen Proximität  schmelzen zusammen. Pooling funktioniert ähnlich. Hier werden mehrer Pixel über den maximal Wert zusammengefasst.\n",
    "Weniger Parameter bedeutet auch, eine geringere Chance zu overfitten. \n",
    "\n",
    "Die meistbenutzte Pooling Layer ist die Max Pooling Layer. Hierbei wird der größte Wert im Kernel, als neuer Wert für den Output gewählt. Es gibt natürlich eine Vielzahl von anderen [Pooling](https://pytorch.org/docs/stable/nn.html#pooling-layers) Layers.\n",
    "\n",
    "Neben der Kernel Size, die Größe Quadrates das gepoolt werden soll, geben wir diesmal auch den `stride` an. Der Stride definiert, um wie viele Pixel wir den Kernel verschieben. \n",
    "\n",
    "![](https://www.oreilly.com/library/view/machine-learning-for/9781786469878/assets/09ad7edc-334f-4c54-944b-af21139b0587.png)\n",
    "<center><h7>Source: Rodolfo Bonnin - Machine Learning for Developers </h7></center>\n",
    "\n",
    "\n",
    "[Hier](https://ezyang.github.io/convolution-visualizer/index.html) ist eine Website in der Sie den Effekt von verschiedenen Parametern auf die Convolution visualisiert bekommen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pool1 = nn.MaxPool2d(kernel_size = 2, stride = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sie können jetzt den Output der 2DConv (`out`) als Input für die Pooling Layer benutzen.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out2 = pool1(____)\n",
    "out2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Lösung:</b></summary>\n",
    "    \n",
    "```python \n",
    "out2 = pool1(out)\n",
    "```\n",
    "</details>\n",
    "\n",
    "Da sich nichts an der Anzahl der Channels geändert hat, können wir dieses Bild immernoch visualsieren.\n",
    "Es ist zu erkennen, dass sich das Bild verkleinert hat, dennoch könnnen wir noch eine 5 erkennen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(min_max(out2.detach().numpy()[0].transpose((1, 2, 0))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit `nn.Sequential` können Sie auch wieder mehrere Layers hintereinander schalten. Wichtig, wir brauchen auch wieder eine nicht-lineare Aktivierungsfunktion, diese wird normalerweise nach dem Convolution eingefügt.\n",
    "\n",
    "Füllen Sie den Code aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = nn.Sequential(nn.Conv2d(_,3,3,1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(2,2),\n",
    "                   nn.Conv2d(__,6,3,1),\n",
    "                   nn.______,\n",
    "                   nn.MaxPool2d(2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Lösung:</b></summary>\n",
    "    \n",
    "```python \n",
    "cnn = nn.Sequential(nn.Conv2d(1,3,3,1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(2,2),\n",
    "                   nn.Conv2d(3,6,3,1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(2,2))\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt können wir den Batch `batch_x` einmal durch das Netzwerk führen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1])"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn(batch_x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dieser Output eignet sich aber noch nicht, um Vorhersagen zu treffen. Um diese zu treffen, müssen wir die Bilder wieder in ein traditionelles Neuronales Netzwerk führen. Doch diese akzeptieren nur Input in Form eines Vektors. Deshalb konvertieren wir jedes Bild zurück in einen Vektor. \n",
    "\n",
    "Also unser Tensor hat die `shape` `[32, 6, 5, 5]` und soll zu einem Tensor `[32, 6 x 5 x 5]` = `[32, 150]`.\n",
    "\n",
    "Dafür können wir die \"Layer\" `nn.Flatten(starting_dim)` benutzen. Hierbei müssen wir nur den Parameter `starting_dim` festlegen. Dieser bestimmt, ab welche Dimension wir beginnen, die Dimensionen zusammenzuführen. Da wir für jedes Bild einen eigenen Vektor wollen, benutzen wir eine `starting_dim = 1`. Mit cnn.add_module(), können wir noch extra Layers zu unserem Modul hinzufügen.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add_module(\"flatten\",nn.Flatten(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn(batch_x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Größe des Batches ist nun `(32,150)`. `32` ist immer noch die Anzahl der Bilder im Batch (Dimension 0), aber unsere zweite Dimension hat jetzt die Größe `150`. Das heißt, jedem Bild im Batch ist ein Vektor zugeordnet. Jetzt können wir auch noch eine traditionelle Linear Layer einbauen. Vor der `nn.Linear` Layer fügen wir aber noch zusätzlich eine BatchNorm und eine Dropout Layer ein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add_module(\"bn\", nn.BatchNorm1d(____))\n",
    "cnn.add_module(\"dp\", nn._________(0.2))\n",
    "cnn.add_module(\"fc\", nn.Linear(____,___))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Lösung:</b></summary>\n",
    "    \n",
    "```python \n",
    "cnn.add_module(\"bn\", nn.BatchNorm1d(150))\n",
    "cnn.add_module(\"dp\", nn.Dropout(0.2))\n",
    "cnn.add_module(\"fc\", nn.Linear(150,10))\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun können Sie die Loss Funktion und Optimizer bestimmen. Durch das Benutzen von PyTorchs `loaders` und der `nn` library, können Sie denselben `for-loop` von letzter Woche ohne Änderung kopieren. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_funktion = nn.CrossEntropyLoss()\n",
    "updaten =  torch.optim.Adam(_____________, lr =0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Lösung:</b></summary>\n",
    "    \n",
    "```python \n",
    "loss_funktion = nn.CrossEntropyLoss()\n",
    "updaten =  torch.optim.Adam(cnn.parameters(), lr =0.001)\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 2\n",
    "for i in range(EPOCHS):\n",
    "    loss_list = [] # diese Liste speichter den Loss jedes Minibatches\n",
    "    cnn.train() \n",
    "    for minibatch in train_loader: # for-loop geht durch alle minibatches\n",
    "        images, labels = minibatch # minibatch wird in Bilder und Labels geteilt\n",
    "\n",
    "        updaten.zero_grad()\n",
    "        output = cnn(images) # Forward Propagation\n",
    "        loss   = loss_funktion(output, labels)\n",
    "        loss.backward()\n",
    "        loss_list.append(loss.item())\n",
    "        updaten.step()\n",
    "    cnn.eval()    \n",
    "    output = cnn(train_x)\n",
    "    train_acc=((output.max(dim=1)[1]==train_y).sum()/float(output.shape[0])).item()\n",
    "    print(\n",
    "        \"Training Loss: %.2f Training Accuracy: %.2f\"\n",
    "        % (np.mean(loss_list), train_acc)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zuletzt evaluieren wir das Netzwerk auf dem Testdatensatz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.genfromtxt('../data/mnist/mnist_test.csv', delimiter=',', skip_header =False)\n",
    "test_x = torch.tensor(min_max(test_data[:,1:]), dtype=torch.float32)\n",
    "test_y = torch.tensor(test_data[:,0], dtype=torch.long)\n",
    "test_x = test_x.reshape(test_x.shape[0],1,28,28)\n",
    "print(test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = cnn(test_x)\n",
    "acc=((output.max(dim=1)[1]==test_y).sum()/float(output.shape[0])).item()\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Übungsaufgabe\n",
    "\n",
    "Wieder verwenden wir für die Übungsaufgabe die Toxicity Daten. Diesmal aber sind die Moleküle nicht in einem SMILES Format, sondern die Struktur gespeichert. **Sie werden wieder die Toxizität vorhersagen, diesmal aber anhand des Bildes**. \n",
    "\n",
    "Tatsächlich wurde dies auch [schon probiert.](https://www.sciencedirect.com/science/article/abs/pii/S0169743919303417). \n",
    "\n",
    "Die Bilder bestehen aus `64 x 64` Pixel. Sie werden sehen, dass das kaum ausreicht, um die Molekülstruktur auszumachen.  Allerdings, sind wir an das, von der Uni zur Verfügung gestellten, Speicherlimit gebunden.\n",
    "Tatsächlich aber ändert, in diesem Fall, eine höhere Auflösung nichts an dem Problem.\n",
    "\n",
    "## Restarten Sie den Kernel bevor Sie mit der Aufgabe beginnen!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision.transforms as T\n",
    "def min_max(x):\n",
    "    return (x - np.min(x)) / (np.max(x) - np.min(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teilen Sie zunächst den Datensatz in Traings- und Testset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1796, 4096]) torch.Size([1796])\n"
     ]
    }
   ],
   "source": [
    "mol_img_data = torch.tensor(np.genfromtxt('../data/toxicity/molasimg.csv', delimiter=',', skip_header =False),dtype=torch.float32)\n",
    "train, test=train_test_split(_______________,______________,_________, random_state=1234)\n",
    "\n",
    "train_x = train[______]\n",
    "train_y = train[______]\n",
    "test_x = test[______]\n",
    "test_y = test[______]\n",
    "\n",
    "\n",
    "print(train_x.shape, train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So verpixelt sehen die Bilder aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd5b82b0d10>"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_x[10,:].view(64,64), cmap= \"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als nächstes konvertieren Sie das Test- und Trainingsset. Denken Sie daran, dass die Dimensionen wie folgt ausehen sollen. `Anzahl Bilder, Anzahl Channel, Höhe, Breite`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x.view(__________________________)\n",
    "test_x = test_x.view(___________________________)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 64, 64]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "torch_train = data.TensorDataset(______________________)\n",
    "train_loader = data.DataLoader(__________________, batch_size=32)\n",
    "\n",
    "batch_x, batch_y =next(iter(train_loader))\n",
    "print(batch_x.shape, batch_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenn Sie bis jetzt alles richtig gemacht haben, sollte `batch_x` die Dimensionen `[32, 1, 64, 64]` und `batch_y` haben. Fügen Sie dem Netzwerk mindestens 2 weitere Convolution Layers hinzu. Achten Sie darauf, dass Sie auch Pooling Layers und nicht-lineare Aktivierungsfunktionen verwenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = nn.Sequential(nn.Conv2d(1,3,3,1),\n",
    "                   \n",
    "                   \n",
    "                   \n",
    "                    \n",
    "                   \n",
    "                   \n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 12, 6, 6])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn(batch_x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fügen Sie nun eine `Flatten` Layer hinzu. Ab welcher Dimension fangen wir an die Werte zusammen zu fügen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add_module(\"flatten\",nn.Flatten(_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 432])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn(batch_x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als letztes fügen Sie eine `BatchNorm`, `Dropout` und `Linear` hinzu. Achten Sie hierb auf die richitigen Input/ Output Dimensionen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add_module(\"bn\", _______________)\n",
    "cnn.add_module(\"dp\", _______________)\n",
    "cnn.add_module(\"fc\", _______________)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1])"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn(batch_x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die `shape` solte jetzt `[32, 1]` sein. Füllen Sie den Rest des Trainingsloops aus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.70 Training Accuracy: 0.59 | Test Loss: 0.68  Test Accuracy: 0.58\n",
      "Training Loss: 0.67 Training Accuracy: 0.63 | Test Loss: 0.67  Test Accuracy: 0.61\n",
      "Training Loss: 0.65 Training Accuracy: 0.64 | Test Loss: 0.67  Test Accuracy: 0.63\n",
      "Training Loss: 0.64 Training Accuracy: 0.64 | Test Loss: 0.65  Test Accuracy: 0.61\n",
      "Training Loss: 0.63 Training Accuracy: 0.67 | Test Loss: 0.64  Test Accuracy: 0.64\n",
      "Training Loss: 0.62 Training Accuracy: 0.64 | Test Loss: 0.89  Test Accuracy: 0.66\n",
      "Training Loss: 0.62 Training Accuracy: 0.66 | Test Loss: 0.63  Test Accuracy: 0.65\n",
      "Training Loss: 0.61 Training Accuracy: 0.67 | Test Loss: 0.64  Test Accuracy: 0.65\n",
      "Training Loss: 0.60 Training Accuracy: 0.69 | Test Loss: 0.62  Test Accuracy: 0.66\n",
      "Training Loss: 0.60 Training Accuracy: 0.69 | Test Loss: 0.62  Test Accuracy: 0.66\n",
      "Training Loss: 0.59 Training Accuracy: 0.69 | Test Loss: 0.62  Test Accuracy: 0.65\n",
      "Training Loss: 0.61 Training Accuracy: 0.68 | Test Loss: 0.63  Test Accuracy: 0.64\n",
      "Training Loss: 0.59 Training Accuracy: 0.65 | Test Loss: 0.64  Test Accuracy: 0.64\n",
      "Training Loss: 0.59 Training Accuracy: 0.70 | Test Loss: 0.62  Test Accuracy: 0.67\n",
      "Training Loss: 0.59 Training Accuracy: 0.70 | Test Loss: 0.61  Test Accuracy: 0.67\n",
      "Training Loss: 0.57 Training Accuracy: 0.71 | Test Loss: 0.61  Test Accuracy: 0.66\n",
      "Training Loss: 0.57 Training Accuracy: 0.71 | Test Loss: 0.61  Test Accuracy: 0.66\n",
      "Training Loss: 0.57 Training Accuracy: 0.73 | Test Loss: 0.61  Test Accuracy: 0.67\n",
      "Training Loss: 0.56 Training Accuracy: 0.73 | Test Loss: 0.61  Test Accuracy: 0.66\n",
      "Training Loss: 0.56 Training Accuracy: 0.72 | Test Loss: 0.61  Test Accuracy: 0.68\n",
      "Training Loss: 0.56 Training Accuracy: 0.64 | Test Loss: 0.74  Test Accuracy: 0.58\n",
      "Training Loss: 0.56 Training Accuracy: 0.71 | Test Loss: 0.62  Test Accuracy: 0.65\n",
      "Training Loss: 0.55 Training Accuracy: 0.70 | Test Loss: 0.63  Test Accuracy: 0.63\n",
      "Training Loss: 0.55 Training Accuracy: 0.72 | Test Loss: 0.61  Test Accuracy: 0.66\n",
      "Training Loss: 0.54 Training Accuracy: 0.73 | Test Loss: 0.61  Test Accuracy: 0.66\n",
      "Training Loss: 0.55 Training Accuracy: 0.73 | Test Loss: 0.61  Test Accuracy: 0.67\n",
      "Training Loss: 0.54 Training Accuracy: 0.74 | Test Loss: 0.61  Test Accuracy: 0.67\n",
      "Training Loss: 0.54 Training Accuracy: 0.67 | Test Loss: 0.72  Test Accuracy: 0.60\n",
      "Training Loss: 0.52 Training Accuracy: 0.75 | Test Loss: 0.61  Test Accuracy: 0.66\n",
      "Training Loss: 0.52 Training Accuracy: 0.75 | Test Loss: 0.61  Test Accuracy: 0.67\n"
     ]
    }
   ],
   "source": [
    "loss_funktion = ________________________\n",
    "updaten =  torch.optim.Adam(_______________, lr =0.0003)\n",
    "EPOCHS = 30\n",
    "\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    loss_list = [] # diese Liste speichter den Loss jedes Minibatches\n",
    "    \n",
    "    ___.train() \n",
    "    for minibatch in train_loader: # for-loop geht durch alle minibatches\n",
    "        images, labels = minibatch # minibatch wird in Bilder und Labels geteilt\n",
    "        ________.zero_grad()\n",
    "        output = cnn(_______) # Forward Propagation\n",
    "        loss   = loss_funktion(output.squeeze(), labels)\n",
    "        loss.backward()\n",
    "        loss_list.append(loss.item())\n",
    "        updaten.step()\n",
    "    ___.eval()    \n",
    "    \n",
    "    # Trainings Evaluation\n",
    "    output = cnn(train_x)\n",
    "    train_acc = torch.sum((output>0).squeeze().int() == train_y)/train_y.shape[0]\n",
    "    # Test Evaluation\n",
    "    output = cnn(test_x)\n",
    "    loss   = loss_funktion(output.squeeze(), test_y)\n",
    "    test_acc = torch.sum((output>0).squeeze().int() == test_y)/test_y.shape[0]\n",
    "    \n",
    "    print(\n",
    "        \"Training Loss: %.2f Training Accuracy: %.2f | Test Loss: %.2f  Test Accuracy: %.2f\"\n",
    "        % (np.mean(loss_list), train_acc, loss.item(),test_acc )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sie sehen, das funktioniert nur so mäßig. Mit Fingerprints hat es auf jeden Fall besser funktioniert.\n",
    "Grundsätzlich ist es schwieriger CNNs zu trainieren als einfacher Neuronales Netzwerke. \n",
    "Hinzu kommt, dass die grafische Darstellung von Moleküle im Vergleich zu SMILES oder Fingerprints/Deskriptoren eine sehr ineffiziente ist. \n",
    "\n",
    "In unserem Fall könnte man argumentieren, dass wenn wir größere, mehr und farbige Bilder hätten, dann würde auch unser Computer besser lernen können. Dem ist wahrscheinlich auch so. Aber selbst in der Publikation von oben, konnten deren CNNs, vergleichbare Algorithmen, die Fingerprints benutzen nicht schlagen.\n",
    "Man kann sagen, dass Bilder keine adäquate Repräsentation von Molekülen ist. Zumindest für das maschinelle Lernen.\n",
    "\n",
    "\n",
    "Das soll nicht heißen, dass es nicht sinnvoll sein kann, CNN auf Bilder von Molekülen zu trainieren. \n",
    "Zum Beispiel Netzwerke, die Struktur erkennen und den dazugehörigen SMILES ausgeben. So können schnell Patente und chemische Veröffentlichung gescreent werden.\n",
    "\n",
    "Wie zum Beispiel hier:\n",
    "https://jcheminf.biomedcentral.com/articles/10.1186/s13321-021-00538-8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
