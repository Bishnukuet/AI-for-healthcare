{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network\n",
    "\n",
    "Diese Woche beschäftigen wir uns mit Convolutional Neural Networks. Convolutional Neural Networks (CNN) werden hauptsächlich, aber nicht ausschließlich, für die Bilderkennung eingesetzt.\n",
    "\n",
    "Anders als die neuronalen Netze, die wir bisher gesehen haben, können CNNs Bilder als Matrix lesen. Das bedeutet, dass lokale Zusammenhänge nicht durch das `flatten` des Bildes verloren gehen.  \n",
    "\n",
    "![](https://miro.medium.com/max/1280/1*h01T_cugn22R2zbKw5a8hA.gif)\n",
    "\n",
    "<centering><h7> Otavio Good. 2017 \"A Visual and Intuitive Understanding of Deep Learning\" *O'Reilly AI Conference* </h7></centering>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils import data\n",
    "\n",
    "def min_max(x):\n",
    "    return (x - np.min(x)) / (np.max(x) - np.min(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zunächst laden Sie die Trainingsdaten erneut und wandeln sie in einen `tensor` um. Insgesamt gibt es 60,000 Bilder mit 784 Pixel + eine Spalte für die Labels der Bilder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 784]) torch.Size([60000])\n"
     ]
    }
   ],
   "source": [
    "train_data = np.genfromtxt('../data/mnist/mnist_train.csv', delimiter=',', skip_header =False)\n",
    "\n",
    "train_x = torch.tensor(min_max(train_data[:,1:]), dtype=torch.float32)\n",
    "train_y = torch.tensor(train_data[:,0], dtype=torch.long)\n",
    "\n",
    "print(train_x.shape, train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bisher haben wir immer Bilder als 1D-Input in unser neuronales Netz eingegeben. Diesmal wollen wir aber die 2D-Struktur verwenden. Dazu müssen wir aus einem Vektor der Länge `784` eine Matrix mit der Größe `28 x 28` machen.\n",
    "\n",
    "Dazu können wir die Funktion `vector.view(28,28)` verwenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 28])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[0,:].view(28,28).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir können uns dieses Bild ansehen, aber wir können nicht viel erkennen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0118, 0.0706, 0.0706, 0.0706, 0.4941, 0.5333,\n",
       "         0.6863, 0.1020, 0.6510, 1.0000, 0.9686, 0.4980, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1176,\n",
       "         0.1412, 0.3686, 0.6039, 0.6667, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
       "         0.8824, 0.6745, 0.9922, 0.9490, 0.7647, 0.2510, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1922, 0.9333,\n",
       "         0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9843,\n",
       "         0.3647, 0.3216, 0.3216, 0.2196, 0.1529, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.8588,\n",
       "         0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765, 0.7137, 0.9686, 0.9451,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3137,\n",
       "         0.6118, 0.4196, 0.9922, 0.9922, 0.8039, 0.0431, 0.0000, 0.1686, 0.6039,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0549, 0.0039, 0.6039, 0.9922, 0.3529, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.5451, 0.9922, 0.7451, 0.0078, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0431, 0.7451, 0.9922, 0.2745, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.1373, 0.9451, 0.8824, 0.6275, 0.4235, 0.0039,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.3176, 0.9412, 0.9922, 0.9922, 0.4667,\n",
       "         0.0980, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1765, 0.7294, 0.9922, 0.9922,\n",
       "         0.5882, 0.1059, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0627, 0.3647, 0.9882,\n",
       "         0.9922, 0.7333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9765,\n",
       "         0.9922, 0.9765, 0.2510, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1804, 0.5098, 0.7176, 0.9922,\n",
       "         0.9922, 0.8118, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.1529, 0.5804, 0.8980, 0.9922, 0.9922, 0.9922,\n",
       "         0.9804, 0.7137, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0941, 0.4471, 0.8667, 0.9922, 0.9922, 0.9922, 0.9922, 0.7882,\n",
       "         0.3059, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0902,\n",
       "         0.2588, 0.8353, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765, 0.3176, 0.0078,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.6706, 0.8588,\n",
       "         0.9922, 0.9922, 0.9922, 0.9922, 0.7647, 0.3137, 0.0353, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.2157, 0.6745, 0.8863, 0.9922, 0.9922,\n",
       "         0.9922, 0.9922, 0.9569, 0.5216, 0.0431, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.5333, 0.9922, 0.9922, 0.9922, 0.8314,\n",
       "         0.5294, 0.5176, 0.0627, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[0,:].view(28,28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aber mit `matplotlib` können wir Arrays als Bild darstellen. Hier gibt `cmap = \"greys\"` an, dass wir unser Farbspektrum nur in Schwarz und Weiß haben wollen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f656fa05d10>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_x[0,:].view(28,28), cmap= \"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir haben bisher nur ein Bild in das richtige Format gebracht, um dies für alle Bilder zu tun, können wir auch `.view()` verwenden. Der `tesnor` von eben hatte das Format `(Höhe,Breite)`. Um alle Bilder konvertieren zu können, müssen wir dem `tensor` eine zusätzliche Dimension hinzufügen.  Der neue `tensor` sollte die folgenden Dimensionen haben: `(Anzahl der Bilder, Höhe, Breite)`. Wir haben also insgesamt drei Dimensionen.\n",
    "\n",
    "PyTorch würde uns hier jedoch einen Strich durch die Rechnung machen. Denn PyTorch kann sowohl mit schwarz-weißen (s/w) als auch mit farbigen Bildern arbeiten. In PyTorch werden farbige Bilder durch drei Matrizen dargestellt. Eine für Rot, eine für Grün und eine für Blau. Diese werden auch Channels genannt. Ein farbiges Bild hätte in PyTorch die Dimensionen `(3, Höhe, Breite)`. Die Dimension, die wir gerade für die \"Anzahl der Bilder\" verwendet haben, wird also durch die \"Anzahl der Channels\" belegt.\n",
    "\n",
    "\n",
    "![](https://miro.medium.com/max/700/1*icINeO4H7UKe3NlU1fXqlA.jpeg)\n",
    "\n",
    "<center><h7>Source: Mathanraj Sharma, 2019 </h7></center>\n",
    "\n",
    "PyTorch erwartet diese \"Channel Dimension\" auch für s/w Bilder. \n",
    "Deshalb stellen wir ein s/w Bild wie folgt da: `(1, Höhe, Breite)`. \n",
    "\n",
    "Daraus folgt, dass alle Bilder vom MNIST Datensatz diesem Format entsprechen müssen: `(Anzahl Bilder, 1, Höhe, Breite)`. Also insgesamt hat unser Input `tensor` 4 Dimensionen.\n",
    "\n",
    "\n",
    "\n",
    "Konvertieren Sie `train_x` zu diesem Format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x.view(_____,1,____,____)\n",
    "train_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Lösung:</b></summary>\n",
    "    \n",
    "```python \n",
    "train_x = train_x.view(60000,1,28,28)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sie haben nun alle Bilder in das Format `(1,28,28)` konvertiert.\n",
    "Sie können immer noch Bilder mit `plt.imshow` anzeigen.\n",
    "\n",
    "Beachten Sie, wie der `tensor` jetzt indiziert ist. `[0,0,:,:]`. Wir wählen das erste Bild und auch den ersten und einzigen Channel aus. Wir wählen zusätzlich die gesamte Höhe und Breite aus, um das Bild vollständig anzuzeigen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_310999/609908672.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m\"gray\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "plt.imshow(train_x[0,0,:,:], cmap= \"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie letzte Woche können Sie einen `DataLoader` verwenden. Dazu müssen Sie zuerst ein PyTorch Dataset erstellen. Mit `next(iter())` können Sie den ersten Minibatches des `DataLoaders` ausgeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_train = data.TensorDataset(_____, ____)\n",
    "train_loader = data.DataLoader(______, batch_size=32)\n",
    "\n",
    "batch_x, batch_y =next(iter(train_loader))\n",
    "print(batch_x.shape, batch_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Lösung:</b></summary>\n",
    "    \n",
    "```python \n",
    "torch_train = data.TensorDataset(train_x,train_y)\n",
    "train_loader = data.DataLoader(torch_train, batch_size=32)\n",
    "\n",
    "batch_x, batch_y =next(iter(train_loader))\n",
    "print(batch_x.shape, batch_y.shape)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie Sie sehen können, hat der `batch_x` die Dimensionen `[32, 1, 28, 28]`. Also `32` Bilder, die Größe unseres Batches, `1` Channel, `28` Pixel in der Höhe und `28` in der Breite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNNs in PyTorch erstellen.\n",
    "\n",
    "\n",
    "Soweit haben wir unsere Daten im richtigen Format, jetzt geht es um die Erstellung von `CNN` in PyTorch. Genauso wie es in PyTorch `nn.Linear` Layers gibt, gibt es auch Convolutional Layers im `nn` Modul.\n",
    "\n",
    "`nn.Conv2d()` ist eine solche Layer. Bevor wir sie benutzen, besprechen wir kurz die wichtigsten Parameter.\n",
    "\n",
    "- `in_channels` die Anzahl der Channel, die das Bild vor der Convolution hat \n",
    "- `out_channels` wie viele Channel das Bild nach der Convolution haben soll. Oder wie viele Filter wir über das Bild laufen lassen.\n",
    "- `kernel_size` wie groß der Kernel ist, also die Höhe/Breite in Pixeln.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 26, 26])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = conv1(batch_x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie Sie sehen können, hat sich die Größe des Minibatches geändert. Ursprünglich haben wir immer noch 32 Bilder, aber wie angegeben, haben wir jetzt 3 Channels. Die Höhe und Breite unseres Bildes haben sich ebenfalls geändert. Wir haben 2 Pixel pro Dimension verloren. Das liegt an der Funktionsweise der Convolution.\n",
    "\n",
    "![](https://miro.medium.com/max/700/1*L4T6IXRalWoseBncjRr4wQ@2x.gif)\n",
    "<center><h7>Source: Arden Dertat, 2017 </h7></center>\n",
    "\n",
    "\n",
    "Hier ist ein Beispiel dafür, warum eine Kernelgröße von 3 unser Outputbild um zwei Pixel kleiner macht. Links ist das Inputbild und rechts das Outputbild zu sehen. Da wir den Kernel nicht über den Rand des Bildes schieben können, \"verlieren\" wir den äußeren Rand des Bildes.\n",
    "\n",
    "Um zu verhindern, dass diese Informationen verloren gehen, können wir das Bild *padden*. Auf diese Weise vergrößern wir das Bild, zum Beispiel mit Pixeln, die den Wert Null haben.\n",
    "\n",
    "![](https://miro.medium.com/max/700/1*W2D564Gkad9lj3_6t9I2PA@2x.gif)\n",
    "<center><h7>Source: Arden Dertat, 2017 </h7></center>\n",
    "\n",
    "Durch das Padding, kann der Kernel einmal über das gesamte Bild geschoben werden.\n",
    "Wir können die Breite des Paddings auch als Parameter in `Conv2d` mitgeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 28, 28])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1 = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=3, padding =1)\n",
    "out = conv1(batch_x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durch das Padding schrumpft das Bild nicht mehr. Da wir jetzt `3` Channels haben, können wir das Bild immer noch mit `plt.imshow` anzeigen.  Dazu müssen wir ein Bild aus dem Minibatch auswählen und den Befehl `detach()` verwenden, um die von `autograd` gespeicherten Gradienten zu entfernen.\n",
    "\n",
    "*Ein Bild wie dieses kann nur als Beispiel verwendet werden, um die Transformation zu veranschaulichen. Die tatsächlichen Farben und Intensitäten sind hier irrelevant, da diese vom Netzwerk willkürlich festgelegt sind.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x24b34af1490>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(min_max(out.detach().numpy()[0].transpose((1, 2, 0))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sie können immer noch eine 5 erkennen, aber dieses Mal in Farbe. Wie oben beschrieben, sind die Farben nicht zur Interpretation geeignet. Sie dienen nur dazu, die Diversifizierung des Inputs zu verdeutlichen. \n",
    "\n",
    "Die zweite neue Layer, die Sie heute verwenden werden, ist `nn.MaxPool2d()`. \n",
    "\n",
    "Diese Ebene wird als **Pooling**-Layer bezeichnet.\n",
    "\n",
    "\n",
    "![](https://miro.medium.com/max/700/1*ReZNSf_Yr7Q1nqegGirsMQ@2x.png)\n",
    "<center><h7>Source: Arden Dertat, 2017 </h7></center>\n",
    "\n",
    "Das Pooling Layers führen zu einer bewussten Reduzierung der Bildgröße tiefer im Netzwerk. Dies bedeutet, dass weniger Parameter (Weights) benötigt werden, was dazu führt, dass unsere Netzwerke schneller trainieren. Wenn Sie ein Bild (größer als 28 x 28 Pixel) betrachten, erkennen Sie nicht jeden einzelnen Pixel, sondern Pixel in einer bestimmten Nähe verschmelzen miteinander. Pooling funktioniert auf ähnliche Weise. Hier werden mehrere Pixel mit Hilfe des Maximalwertes kombiniert.\n",
    "Weniger Parameter bedeuten auch eine geringere Wahrscheinlichkeit auf Overfitting. \n",
    "\n",
    "Die am häufigsten verwendete Pooling Layer ist die Max-Pooling Layer. Hier wird der größte Wert in der Region als neuer Wert für den Output gewählt. Es gibt natürlich eine Vielzahl anderer [Pooling]-Layers(https://pytorch.org/docs/stable/nn.html#pooling-layers).\n",
    "Neben der Kernelgröße, der Größe des Quadrats, das wir zusammenfassen wollen, geben wir diesmal auch den `stride` an. Der Stride legt fest, um wie viele Pixel wir den Pooling Kernel verschieben. \n",
    "\n",
    "![](https://www.oreilly.com/library/view/machine-learning-for/9781786469878/assets/09ad7edc-334f-4c54-944b-af21139b0587.png)\n",
    "<center><h7>Source: Rodolfo Bonnin - Machine Learning for Developers </h7></center>\n",
    "\n",
    "\n",
    "[Hier](https://ezyang.github.io/convolution-visualizer/index.html) ist eine Website mit der Sie den Effekt von verschiedenen Parametern auf die Convolution visualisieren können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pool1 = nn.MaxPool2d(kernel_size = 2, stride = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sie können nun den Output des 2DConv (`out`) als Input für die Pooling-Layer verwenden.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out2 = pool1(____)\n",
    "out2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Lösung:</b></summary>\n",
    "    \n",
    "```python \n",
    "out2 = pool1(out)\n",
    "```\n",
    "</details>\n",
    "\n",
    "Da sich an der Anzahl der Channels nichts geändert hat, können wir dieses Bild immer noch visualisieren.\n",
    "Wir können sehen, dass das Bild geschrumpft ist, dennoch kann man die 5 noch sehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x24b2bf4ba00>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(min_max(out2.detach().numpy()[0].transpose((1, 2, 0))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit `nn.Sequential` kann man auch mehrere Convolution/Pooling Layers hintereinander schalten. Wichtig, wir brauchen auch wieder eine nichtlineare Aktivierungsfunktion, diese wird normalerweise nach der Convolution eingefügt.\n",
    "\n",
    "Füllen Sie den fehlenden Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = nn.Sequential(nn.Conv2d(in_channels=_, out_channels=3, kernel_size=3, padding =1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size = 2,stride = 2),\n",
    "                   nn.Conv2d(in_channels= _ , out_channels=6, kernel_size=3, padding =1),\n",
    "                   nn.______,\n",
    "                   nn.MaxPool2d(kernel_size = 2,stride = 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Lösung:</b></summary>\n",
    "    \n",
    "```python \n",
    "cnn = nn.Sequential(nn.Conv2d(in_channels=1, out_channels=3, kernel_size=3, padding =1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size = 2,stride = 2),\n",
    "                   nn.Conv2d(in_channels= 3 , out_channels=6, kernel_size=3, padding =1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size = 2,stride = 2))\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt können wir den Batch `batch_x` einmal durch das Netzwerk führen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 6, 5, 5])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn(batch_x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diese Output ist jedoch noch nicht für Vorhersagen geeignet. Dazu müssen wir die Bilder wieder in ein herkömmliches neuronales Netz einspeisen. Diese akzeptieren jedoch nur Inputs in Form von Vektoren. Also konvertieren wir jedes Bild zurück in einen Vektor. \n",
    "\n",
    "Der Output `tensor` hat die `shape` `[32, 6, 5, 5]` und sollte zu einem `tensor` der Größe `[32, 6 x 5 x 5]` = `[32, 150]` werden.\n",
    "\n",
    "Dazu können wir die Layer `nn.Flatten(starting_dim)` verwenden. Hier müssen wir nur den Parameter `starting_dim` definieren. Dieser bestimmt, ab welcher Dimension wir die Dimensionen zusammenführen. Da wir für jedes Bild einen eigenen Vektor wollen, verwenden wir `starting_dim = 1`. Mit `cnn.add_module()` können wir zusätzliche Layers zu unserem Netzwerk hinzufügen.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add_module(\"flatten\",nn.Flatten(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 150])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn(batch_x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Größe des Batches ist jetzt `(32,150)`. `32` ist immer noch die Anzahl der Bilder im Batch (Dimension 0), aber unsere zweite Dimension ist jetzt `150`. Das heißt, jedem Bild im Stapel ist ein Vektor zugeordnet. Jetzt können wir auch eine traditionelle lineare Layer hinzufügen. Vor der Layer `nn.Linear` fügen wir jedoch eine zusätzliche BatchNorm und eine Dropout-Layer hinzu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add_module(\"bn\", nn.BatchNorm1d(____))\n",
    "cnn.add_module(\"dp\", nn._________(0.2))\n",
    "cnn.add_module(\"fc\", nn.Linear(____,___))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Lösung:</b></summary>\n",
    "    \n",
    "```python \n",
    "cnn.add_module(\"bn\", nn.BatchNorm1d(150))\n",
    "cnn.add_module(\"dp\", nn.Dropout(0.2))\n",
    "cnn.add_module(\"fc\", nn.Linear(150,10))\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun können Sie die Loss Funktion und Optimizer festlegen. Durch das Benutzen von PyTorchs `loaders` und dem `nn` Modul, können Sie denselben `for-loop` von letzter Woche ohne Änderung kopieren. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_funktion = nn.CrossEntropyLoss()\n",
    "updaten =  torch.optim.Adam(_____________, lr =0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Lösung:</b></summary>\n",
    "    \n",
    "```python \n",
    "loss_funktion = nn.CrossEntropyLoss()\n",
    "updaten =  torch.optim.Adam(cnn.parameters(), lr =0.001)\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cnn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_310999/541107612.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mloss_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# diese Liste speichtert den Loss jedes Minibatches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mminibatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# for-loop geht durch alle mMinibatches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminibatch\u001b[0m \u001b[0;31m# Minibatch wird in Bilder und Labels geteilt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cnn' is not defined"
     ]
    }
   ],
   "source": [
    "EPOCHS = 2\n",
    "for i in range(EPOCHS):\n",
    "    loss_list = [] # diese Liste speichtert den Loss jedes Minibatches\n",
    "    cnn.train() \n",
    "    for minibatch in train_loader: # for-loop geht durch alle mMinibatches\n",
    "        images, labels = minibatch # Minibatch wird in Bilder und Labels geteilt\n",
    "\n",
    "        updaten.zero_grad()\n",
    "        output = cnn(images) # Forward Propagation\n",
    "        loss   = loss_funktion(output, labels)\n",
    "        loss.backward()\n",
    "        loss_list.append(loss.item())\n",
    "        updaten.step()\n",
    "    cnn.eval()    \n",
    "    output = cnn(train_x)\n",
    "    train_acc=((output.max(dim=1)[1]==train_y).sum()/float(output.shape[0])).item()\n",
    "    print(\n",
    "        \"Training Loss: %.2f Training Accuracy: %.2f\"\n",
    "        % (np.mean(loss_list), train_acc)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zuletzt evaluieren wir das Netzwerk auf dem Testdatensatz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.genfromtxt('../data/mnist/mnist_test.csv', delimiter=',', skip_header =False)\n",
    "test_x = torch.tensor(min_max(test_data[:,1:]), dtype=torch.float32)\n",
    "test_y = torch.tensor(test_data[:,0], dtype=torch.long)\n",
    "test_x = test_x.reshape(test_x.shape[0],1,28,28)\n",
    "print(test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = cnn(test_x)\n",
    "acc=((output.max(dim=1)[1]==test_y).sum()/float(output.shape[0])).item()\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Übungsaufgabe\n",
    "\n",
    "Wieder verwenden wir die Toxizitätsdaten für die Übungsaufgabe. Diesmal liegen die Moleküle jedoch nicht im SMILES-Format vor, sondern die Strukturen sind als Bild gespeichert. **Sie werden wieder die Toxizität vorhersagen, aber dieses Mal auf der Grundlage des Bildes**. \n",
    "\n",
    "In der Tat ist dies [bereits versucht worden](https://www.sciencedirect.com/science/article/abs/pii/S0169743919303417). \n",
    "\n",
    "Die Bilder bestehen aus `64 x 64` Pixeln. Sie werden sehen, dass dies kaum ausreicht, um die Molekularstruktur zu erkennen.  Wir sind jedoch an den von der Universität zur Verfügung gestellten Speicherplatz gebunden.\n",
    "Tatsächlich ändert in diesem Fall aber eine höhere Auflösung nichts am Problem.\n",
    "\n",
    "## Starten Sie den Kernel neu, bevor Sie die Aufgabe starten!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision.transforms as T\n",
    "def min_max(x):\n",
    "    return (x - np.min(x)) / (np.max(x) - np.min(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teilen Sie zunächst den Datensatz in Traings- und Testset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1796, 4096]) torch.Size([1796])\n"
     ]
    }
   ],
   "source": [
    "mol_img_data = torch.tensor(np.genfromtxt('../data/toxicity/molasimg.csv', delimiter=',', skip_header =False),dtype=torch.float32)\n",
    "train, test=train_test_split(_______________,______________,_________, random_state=1234)\n",
    "\n",
    "train_x = train[______]\n",
    "train_y = train[______]\n",
    "test_x = test[______]\n",
    "test_y = test[______]\n",
    "\n",
    "\n",
    "print(train_x.shape, train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So verpixelt sehen die Bilder aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd5b82b0d10>"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_x[10,:].view(64,64), cmap= \"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als nächstes konvertieren Sie das Test- und Trainingsset. Denken Sie daran, dass die Dimensionen wie folgt ausehen sollen. `Anzahl Bilder, Anzahl Channel, Höhe, Breite`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x.view(__________________________)\n",
    "test_x = test_x.view(___________________________)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 64, 64]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "torch_train = data.TensorDataset(______________________)\n",
    "train_loader = data.DataLoader(__________________, batch_size=32)\n",
    "\n",
    "batch_x, batch_y =next(iter(train_loader))\n",
    "print(batch_x.shape, batch_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenn Sie bis jetzt alles richtig gemacht haben, sollte `batch_x` die Dimensionen `[32, 1, 64, 64]` und `batch_y` die Dimensionen `[32]` haben. Füge dem Netz mindestens 2 weitere Convolutionlayers hinzu. Stellen Sie sicher, dass Sie auch Pooling-Layers und nicht-lineare Aktivierungsfunktionen verwenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = nn.Sequential(nn.Conv2d(in_channels= , out_channels , kernel_size , padding ),\n",
    "                   \n",
    "                   \n",
    "                    \n",
    "                   \n",
    "                   \n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 12, 6, 6])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn(batch_x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fügen Sie nun eine `Flatten` Layer hinzu. Ab welcher Dimension fangen wir an die Werte zusammen zu fügen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add_module(\"flatten\",nn.Flatten(_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 432])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn(batch_x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als letztes fügen Sie eine `BatchNorm`, `Dropout` und `Linear` hinzu. Achten Sie hierbei auf die richtigen Input/ Output Dimensionen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add_module(\"bn\", _______________)\n",
    "cnn.add_module(\"dp\", _______________)\n",
    "cnn.add_module(\"fc\", _______________)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1])"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn(batch_x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die `shape` solte jetzt `[32, 1]` sein. Füllen Sie den Rest des Trainingsloops aus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.70 Training Accuracy: 0.59 | Test Loss: 0.68  Test Accuracy: 0.58\n",
      "Training Loss: 0.67 Training Accuracy: 0.63 | Test Loss: 0.67  Test Accuracy: 0.61\n",
      "Training Loss: 0.65 Training Accuracy: 0.64 | Test Loss: 0.67  Test Accuracy: 0.63\n",
      "Training Loss: 0.64 Training Accuracy: 0.64 | Test Loss: 0.65  Test Accuracy: 0.61\n",
      "Training Loss: 0.63 Training Accuracy: 0.67 | Test Loss: 0.64  Test Accuracy: 0.64\n",
      "Training Loss: 0.62 Training Accuracy: 0.64 | Test Loss: 0.89  Test Accuracy: 0.66\n",
      "Training Loss: 0.62 Training Accuracy: 0.66 | Test Loss: 0.63  Test Accuracy: 0.65\n",
      "Training Loss: 0.61 Training Accuracy: 0.67 | Test Loss: 0.64  Test Accuracy: 0.65\n",
      "Training Loss: 0.60 Training Accuracy: 0.69 | Test Loss: 0.62  Test Accuracy: 0.66\n",
      "Training Loss: 0.60 Training Accuracy: 0.69 | Test Loss: 0.62  Test Accuracy: 0.66\n",
      "Training Loss: 0.59 Training Accuracy: 0.69 | Test Loss: 0.62  Test Accuracy: 0.65\n",
      "Training Loss: 0.61 Training Accuracy: 0.68 | Test Loss: 0.63  Test Accuracy: 0.64\n",
      "Training Loss: 0.59 Training Accuracy: 0.65 | Test Loss: 0.64  Test Accuracy: 0.64\n",
      "Training Loss: 0.59 Training Accuracy: 0.70 | Test Loss: 0.62  Test Accuracy: 0.67\n",
      "Training Loss: 0.59 Training Accuracy: 0.70 | Test Loss: 0.61  Test Accuracy: 0.67\n",
      "Training Loss: 0.57 Training Accuracy: 0.71 | Test Loss: 0.61  Test Accuracy: 0.66\n",
      "Training Loss: 0.57 Training Accuracy: 0.71 | Test Loss: 0.61  Test Accuracy: 0.66\n",
      "Training Loss: 0.57 Training Accuracy: 0.73 | Test Loss: 0.61  Test Accuracy: 0.67\n",
      "Training Loss: 0.56 Training Accuracy: 0.73 | Test Loss: 0.61  Test Accuracy: 0.66\n",
      "Training Loss: 0.56 Training Accuracy: 0.72 | Test Loss: 0.61  Test Accuracy: 0.68\n",
      "Training Loss: 0.56 Training Accuracy: 0.64 | Test Loss: 0.74  Test Accuracy: 0.58\n",
      "Training Loss: 0.56 Training Accuracy: 0.71 | Test Loss: 0.62  Test Accuracy: 0.65\n",
      "Training Loss: 0.55 Training Accuracy: 0.70 | Test Loss: 0.63  Test Accuracy: 0.63\n",
      "Training Loss: 0.55 Training Accuracy: 0.72 | Test Loss: 0.61  Test Accuracy: 0.66\n",
      "Training Loss: 0.54 Training Accuracy: 0.73 | Test Loss: 0.61  Test Accuracy: 0.66\n",
      "Training Loss: 0.55 Training Accuracy: 0.73 | Test Loss: 0.61  Test Accuracy: 0.67\n",
      "Training Loss: 0.54 Training Accuracy: 0.74 | Test Loss: 0.61  Test Accuracy: 0.67\n",
      "Training Loss: 0.54 Training Accuracy: 0.67 | Test Loss: 0.72  Test Accuracy: 0.60\n",
      "Training Loss: 0.52 Training Accuracy: 0.75 | Test Loss: 0.61  Test Accuracy: 0.66\n",
      "Training Loss: 0.52 Training Accuracy: 0.75 | Test Loss: 0.61  Test Accuracy: 0.67\n"
     ]
    }
   ],
   "source": [
    "loss_funktion = ________________________\n",
    "updaten =  torch.optim.Adam(_______________, lr =0.0003)\n",
    "EPOCHS = 30\n",
    "\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    loss_list = [] # diese Liste speichter den Loss jedes Minibatches\n",
    "    \n",
    "    ___.train() \n",
    "    for minibatch in train_loader: # for-loop geht durch alle minibatches\n",
    "        images, labels = minibatch # minibatch wird in Bilder und Labels geteilt\n",
    "        ________.zero_grad()\n",
    "        output = cnn(_______) # Forward Propagation\n",
    "        loss   = loss_funktion(output.squeeze(), labels)\n",
    "        loss.backward()\n",
    "        loss_list.append(loss.item())\n",
    "        updaten.step()\n",
    "    ___.eval()    \n",
    "    \n",
    "    # Trainings Evaluation\n",
    "    output = cnn(train_x)\n",
    "    train_acc = torch.sum((output>0).squeeze().int() == train_y)/train_y.shape[0]\n",
    "    # Test Evaluation\n",
    "    output = cnn(test_x)\n",
    "    loss   = loss_funktion(output.squeeze(), test_y)\n",
    "    test_acc = torch.sum((output>0).squeeze().int() == test_y)/test_y.shape[0]\n",
    "    \n",
    "    print(\n",
    "        \"Training Loss: %.2f Training Accuracy: %.2f | Test Loss: %.2f  Test Accuracy: %.2f\"\n",
    "        % (np.mean(loss_list), train_acc, loss.item(),test_acc )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie Sie sehen können, funktioniert das nur mäßig. Mit FIngerprints funktioniert es definitiv besser.\n",
    "Grundsätzlich ist es schwieriger, CNNs zu trainieren als simplere neuronale Netzwerke. \n",
    "Darüber hinaus ist die grafische Darstellung von Molekülen im Vergleich zu SMILES oder Fingerprints/Deskriptoren sehr ineffizient. \n",
    "\n",
    "In unserem Fall könnte man argumentieren, dass unser Modell auch besser lernen könnte, wenn wir größere und farbige Bilder hätten. Das ist wahrscheinlich richtig. Aber selbst in der oben genannten Veröffentlichung konnten die CNNs einfach Netzwerke mit FIngerpints nicht schlagen.\n",
    "Man kann sagen, dass Bilder keine angemessene Darstellung von Molekülen sind. Zumindest nicht für das maschinelle Lernen.\n",
    "\n",
    "Das soll nicht heißen, dass es nicht nützlich sein kann, CNN auf Bildern von Molekülen zu trainieren. \n",
    "Zum Beispiel Netzwerke, die Strukturen erkennen und die entsprechenden SMILES ausgeben. Dies kann dazu verwendet werden, Patente und chemische Veröffentlichungen schnell zu durchsuchen.\n",
    "\n",
    "\n",
    "Wie zum Beispiel hier:\n",
    "https://jcheminf.biomedcentral.com/articles/10.1186/s13321-021-00538-8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
