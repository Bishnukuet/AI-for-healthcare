{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils import data\n",
    "\n",
    "def min_max(x):\n",
    "    return (x - np.min(x)) / (np.max(x) - np.min(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zunächst laden wir wieder die Trainingsdaten ein und konvertieren Sie zu einem Tensor. Insgesamt sind es 60,000 Bilder und 784 Pixel + eine Spalte für die Labels der Bilder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.genfromtxt('../data/mnist/mnist_train.csv', delimiter=',', skip_header =False)\n",
    "\n",
    "train_x = torch.tensor(min_max(train_data[:,1:]), dtype=torch.float32)\n",
    "train_y = torch.tensor(train_data[:,0], dtype=torch.long)\n",
    "\n",
    "print(train_x.shape, train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bisher haben wir Bilder immer als 1D Input in unsere Neuronales Netzwerk eingeführt. Wir wollen diesmal aber die 2D Struktur benutzen. Dafür müssen wir aus einem Vektor der Länge `784` eine Matrix mit den Maßen `28 x 28` machen.\n",
    "\n",
    "Hierfür können wir die Funktion `vektor.view(28,28)` benutzen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x[0,:].view(28,28).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir können uns dieses Bild anschauen, können aber nicht viel erkennen. Man könnte eventuell erahnen, um welche Zahl es sich handeln soll. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_x[0,:].view(28,28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doch mithilfe von `matplotlib` können wir Pixel Arrays als Bild darstellen. `cmap = \"greys\"` gibt hierbei an, dass wir unser Farbspektrum nur in Schwarz-Weiß haben wollen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(train_x[0,:].view(28,28), cmap= \"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir haben bis jetzt nur ein Bild in das richtige Format gebracht, um alle Bilder auf einmal in das richtige Format zu bringen, können wir aber auch `.view()` benutzen. Der Tensor oben hatte den Format `(Höhe,Breite)`. Damit wir alle Bilder konvertieren können, müssen wir den `tensor` um eine weitere Dimension erweitern.  Wir wollen einen `tensor` mit den Dimensionen `(Anzahl Bilder, Höhe, Breite)`.\n",
    "\n",
    "Allerdings würde hier PyTorch einen Strich durch die Rechnung machen. PyTorch kann sowohl mit Schwarz-Weiß Bildern, als auch mit farbigen Bildern arbeiten. In PyTorch werden farbige Bilder über drei Matrizen dargestellt. Eine für Rot, eine für Grün und eine für Blau. Diese werden auch als Channel bezeichnet. En farbiges Bild hat 3 Channel, ein s/w Bild hat aber nur einen.\n",
    "Ein farbiges Bild würde in PyTorch die Dimensionen `(3, Höhe, Breite)` haben. Deswegen geht PyTorch davon aus, dass alle Bilder aus drei Dimensionen bestehen. Für eine Schwarz/Weiß Bild brauchen wir deshalb auch eine weitere Dimension. Die dritte Dimension hat aber nur die Größe eins, da wir nur einen Channel haben.\n",
    "![](https://miro.medium.com/max/700/1*icINeO4H7UKe3NlU1fXqlA.jpeg)\n",
    "\n",
    "<center><h7>Source: Mathanraj Sharma, 2019 </h7></center>\n",
    "\n",
    "Deshalb stellen wir ein s/w Bild wie folgt dar: `(1, Höhe, Breite)`. Daraus folgt, dass alle Bilder vom MNIST Datensatz diesem Format entsprechen müssen: `(Anzahl Bilder, 1, Höhe, Breite)`\n",
    "\n",
    "\n",
    "\n",
    "Konvertieren Sie `train_x` in dieses Format.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x.view(_____,1,____,____)\n",
    "train_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Lösung:</b></summary>\n",
    "    \n",
    "```python \n",
    "train_x = train_x.view(60000,1,28,28)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sie haben jetzt alle Bilder in das Format `(1,28,28)` konvertiert.\n",
    "Sie können jetzt immer noch die Bilder mit `plt.imshow` anzeigen lassen.\n",
    "\n",
    "Beachten Sie, wie jetzt der Tensor indiziert ist. `[0,0,:,:]`. Wir wählen das erste Bild aus, und auch den ersten und einzigen Channel. Wir wählen natürlich die gesamte Höhe und Breite aus, um das Bild komplett darzustellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(train_x[0,0,:,:], cmap= \"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie letzte Woche können Sie sich auch diesmal einen `DataLoader` benutzen. Dafür müssen wir erst ein TensorDataset erstellen. Mit `next(iter())` können wir uns den ersten Batch des Dataloaders ausgeben lassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_train = data.TensorDataset(_____, ____)\n",
    "train_loader = data.DataLoader(______, batch_size=32)\n",
    "\n",
    "batch_x, batch_y =next(iter(train_loader))\n",
    "print(batch_x.shape, batch_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Lösung:</b></summary>\n",
    "    \n",
    "```python \n",
    "torch_train = data.TensorDataset(train_x,train_y)\n",
    "train_loader = data.DataLoader(torch_train, batch_size=32)\n",
    "\n",
    "batch_x, batch_y =next(iter(train_loader))\n",
    "print(batch_x.shape, batch_y.shape)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie Sie sehen können, hat der `batch_x` die Dimensionen `[32, 1, 28, 28]`. Also `32` Bilder, die Größe unseres Batches, `1`Channel, `28` Pixel in der Höhe und `28` in der Breite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNNs in PyTorch erstellen.\n",
    "\n",
    "Wir haben soweit unsere Daten im richtigen Format, jetzt beschäftigen wir uns mit dem erstellen von `CNN` in PyTorch. Sowie es `linear` layers in PyTorch gibt, gibt es auch Convolutional Layers im `nn` Modul.\n",
    "\n",
    "`nn.Conv2d()` ist so eine Layer. Bevor wir Sie benutzen, besprechen wir kurz die wichitgsten Parameter.\n",
    "\n",
    "- `in_channels` Die Anzahl der Channels die das Bild vor Der Convolution hat \n",
    "- `out_channels` Wie viel Channels soll, das Bild nach der Convolution haben.\n",
    "- `kernel_size` Wie groß ist der Kenel, also die Höhe/Breite in Pixeln\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = conv1 = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = conv1(batch_x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sie sehen, der Batch hat sich in der Größe verändert. Zunächst haben wir immer noch `32` Bilder, allerdings wie spezifiziert, haben wir jetzt `3` Channel. Auch die Höhe und Breite unseres Bildes hat sich verändert. Insgesamt haben wir jeweils 2 Pixel pro Dimension verloren. Das liegt daran, wie Convolutions funktionieren.\n",
    "\n",
    "![](https://miro.medium.com/max/700/1*L4T6IXRalWoseBncjRr4wQ@2x.gif)\n",
    "<center><h7>Source: Arden Dertat, 2017 </h7></center>\n",
    "\n",
    "Hier sehen an einem Beispiel, warum bei einer Kernel Size von 3 unser Output Bild um zwei Pixel kleiner wird. Links ist das Inputfile und rechts der Output. Da wir den Kernel nicht über den Rand des Bildes schieben können, \"verlieren\" wir den äußern Rand des Bildes\n",
    "\n",
    "Um zu verhindern, dass diese Information verloren geht, können wir das Bild *padden*. Dadurch vergrößern wir das Bild, zum Beispiel mit Pixel, die den Wert Null haben.\n",
    "![](https://miro.medium.com/max/700/1*W2D564Gkad9lj3_6t9I2PA@2x.gif)\n",
    "<center><h7>Source: Arden Dertat, 2017 </h7></center>\n",
    "\n",
    "Durch das Padding, kann der Kernel einmal über das ganze Bild geschoben werden.\n",
    "Wir können die Breite des Paddings auch als Parameter in `Conv2d` mitgeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=3, padding =1)\n",
    "out = conv1(batch_x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durch das Padding schrumpft nun das Bild nicht in der Breite. Dadurch, dass wir jetzt `3` Channel haben, können wir dieses immer noch mit `plt.imshow` uns zeigen lassen.  Hierbei müssen wir den ein Bild aus dem Batch auswählen und mit dem Befehl `detach()` die Gradienten, die durch `autograd` gespeichert werden, entfernen.\n",
    "\n",
    "*Ein solches Bild, kann man nur als Beispiel benutzen um die Transformation zu verdeutlichen. Die tatsächlichen Farben und Intensitäten sind hier aber irrelevant, da Sie von dem Netzwerk arbiträr gewählt sind. Zum Beispiel, gibt die Reihenfolge der Channels an, welcher Channel für welche Farbe zuständig ist. Ein Convolution ist sich natürlich nicht bewusst, dass es so eine Ordnung in den Channels gibt.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(min_max(out.detach().numpy()[0].transpose((1, 2, 0))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sie können immer noch eine 5 erkennen, allerdings diesmal in Farbe. Wie oben schon beschrieben, eigenen sich die Farben nicht zum Interpretieren. Es soll lediglich die Diversifizierung des Inputs darstellen. Der weitere neue Layer die Sie heute benutzen werden ist `nn.MaxPool2d()`. \n",
    "\n",
    "Dieses Layer ist einen **Pooling** Layer.\n",
    "\n",
    "\n",
    "![](https://miro.medium.com/max/700/1*ReZNSf_Yr7Q1nqegGirsMQ@2x.png)\n",
    "<center><h7>Source: Arden Dertat, 2017 </h7></center>\n",
    "\n",
    "Pooling Layer führen zu eine Reduzierung der Bildgröße. Dadurch brauchen weniger Parameter (Weights), was dazu führt, das unsere Netzwerke schneller trainieren. Wenn Sie ein Bild (größer als 28 x 28 Pixel)anschauen, dann sehen nicht jeden einzelnen Pixel, sondern Pixel in einer gewissen Proximität werden schmelzen zusammen. Pooling funktioniert ähnlich. Hier werden mehrer Pixel mit Hifle des domminantesten Wertes zusammen gefasst.\n",
    "Weniger Parameter, bedeuete auch eine geringere Chance zu overfitten. \n",
    "\n",
    "Die meist benutzte Pooling Layer ist die Max Pooling Layer. Hierbei wird der größte Wert im Kernel, als neuer Wert für den Output gewählt. Es gibt natürlich eine Vielzahl von anderen [Pooling](https://pytorch.org/docs/stable/nn.html#pooling-layers) Layers.\n",
    "\n",
    "Neben der Kernel Size, die Größe der des Quadrates das gepoolt werden soll, geben wir diesmal auch den `stride` an. Der Stride gibt an, um wieviele Pixel wir den Kernel verschieben. \n",
    "\n",
    "![](https://www.oreilly.com/library/view/machine-learning-for/9781786469878/assets/09ad7edc-334f-4c54-944b-af21139b0587.png)\n",
    "<center><h7>Source: Rodolfo Bonnin - Machine Learning for Developers </h7></center>\n",
    "\n",
    "\n",
    "[Hier](https://ezyang.github.io/convolution-visualizer/index.html) ist eine Website in der Sie den Effekt von verschiedenen Parametern auf die Convolution visualisiert bekommen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pool1 = nn.MaxPool2d(kernel_size = 2, stride = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sie können jetzt den Output der 2DConv (`out`) als Input für die Pooling Layer benutzen.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out2 = pool1(____)\n",
    "out2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Lösung:</b></summary>\n",
    "    \n",
    "```python \n",
    "out2 = pool1(out)\n",
    "```\n",
    "</details>\n",
    "\n",
    "Da sich nichts an der Anzahl der Channels geändert hat, können wir dieses Bild immernoch visualsieren.\n",
    "Es ist zu erkennen, dass sich das Bild verkleinert hat, dennoch könnnen wir noch eine 5 erkennen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(min_max(out2.detach().numpy()[0].transpose((1, 2, 0))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit `nn.Sequential` können Sie auch wieder mehrere Layers hintereinander schalten. Wichitg, wir brauchen auch wieder eine nicht-lineare Aktivierungsfunktion, diese wird normalerweise nach dem Convolution eingefügt.\n",
    "\n",
    "Füllen Sie den Code aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = nn.Sequential(nn.Conv2d(_,3,3,1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(2,2),\n",
    "                   nn.Conv2d(__,6,3,1),\n",
    "                   nn.______,\n",
    "                   nn.MaxPool2d(2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Lösung:</b></summary>\n",
    "    \n",
    "```python \n",
    "cnn = nn.Sequential(nn.Conv2d(1,3,3,1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(2,2),\n",
    "                   nn.Conv2d(3,6,3,1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(2,2))\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt können wir den Batch `batch_x` einmal durch das Netzwerk führen. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dieser Output eignet sich aber noch nicht, um Vorhersagen zu treffen. Um diese zu treffen, müssen wir die Bilder wieder in ein traditionelles Neuronales Netzwerk führen. Doch diese akzeptieren nur Input in Form eines Vektors. Deshalb konvertieren wir jedes Bild zurück in einen Vektor. \n",
    "\n",
    "Also unser Tensor hat die `shape` `[32, 6, 5, 5]` und soll zu einem Tensor `[32, 6 x 5 x 5]` = `[32, 150]`.\n",
    "\n",
    "Dafür können wir die \"Layer\" `nn.Flatten(starting_dim)` benutzen. Hierbei müssen wir nur den Parameter `starting_dim` festlegen. Dieser bestimmt, ab welche Dimension wir beginnen, die Dimensionen zusammenzuführen. Da wir für jedes Bild einen eigenen Vektor wollen, benutzen wir eine `starting_dim = 1`. Mit cnn.add_module(), können wir noch extra Layers zu unserem Modul hinzufügen.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add_module(\"flatten\",nn.Flatten(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn(batch_x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Größe des Batches ist nun `(32,150)`. `32` ist immer noch die Anzahl der Bilder im Batch (Dimension 0), aber unsere zweite Dimension hat jetzt die Größe `150`. Das heißt, jedem Bild im Batch ist ein Vektor zugeordnet. Jetzt können wir auch noch eine traditionelle Linear Layer einbauen. Vor der `nn.Linear` Layer benutzen fügen wir aber noch zusätzlich eine BatchNorm und eine Dropout Layer ein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add_module(\"bn\", nn.BatchNorm1d(____))\n",
    "cnn.add_module(\"dp\", nn._________(0.2))\n",
    "cnn.add_module(\"fc\", nn.Linear(____,___))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Lösung:</b></summary>\n",
    "    \n",
    "```python \n",
    "cnn.add_module(\"bn\", nn.BatchNorm1d(150))\n",
    "cnn.add_module(\"dp\", nn.Dropout(0.2))\n",
    "cnn.add_module(\"fc\", nn.Linear(150,10))\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun können Sie die Loss Funktion und Optimizer bestimmen. Durch das Benutzen von PyTorchs `loaders` und der `nn` library, können Sie denselben `for-loop` von letzter Woche ohne Änderung kopieren. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_funktion = nn.CrossEntropyLoss()\n",
    "updaten =  torch.optim.Adam(_____________, lr =0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Lösung:</b></summary>\n",
    "    \n",
    "```python \n",
    "loss_funktion = nn.CrossEntropyLoss()\n",
    "updaten =  torch.optim.Adam(cnn.parameters(), lr =0.001)\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "for i in range(EPOCHS):\n",
    "    loss_list = [] # diese Liste speichter den Loss jedes Minibatches\n",
    "    cnn.train() \n",
    "    for minibatch in train_loader: # for-loop geht durch alle minibatches\n",
    "        images, labels = minibatch # minibatch wird in Bilder und Labels geteilt\n",
    "\n",
    "        updaten.zero_grad()\n",
    "        output = cnn(images) # Forward Propagation\n",
    "        loss   = loss_funktion(output, labels)\n",
    "        loss.backward()\n",
    "        loss_list.append(loss.item())\n",
    "        updaten.step()\n",
    "    cnn.eval()    \n",
    "    output = cnn(train_x)\n",
    "    train_acc=((output.max(dim=1)[1]==train_y).sum()/float(output.shape[0])).item()\n",
    "    print(\n",
    "        \"Training Loss: %.2f Training Accuracy: %.2f\"\n",
    "        % (np.mean(loss_list), train_acc)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zu letzt evaluieren wir das Netzwerk auf dem Testdatensatz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.genfromtxt('../data/mnist/mnist_test.csv', delimiter=',', skip_header =False)\n",
    "test_x = torch.tensor(min_max(test_data[:,1:]), dtype=torch.float32)\n",
    "test_y = torch.tensor(test_data[:,0], dtype=torch.long)\n",
    "test_x = test_x.reshape(test_x.shape[0],1,28,28)\n",
    "print(test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = cnn(test_x)\n",
    "acc=((output.max(dim=1)[1]==test_y).sum()/float(output.shape[0])).item()\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Übungsaufgabe\n",
    "\n",
    "Wieder verwenden wir für die Übungsaufgabe die Toxicity Daten. Diesmal aber "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "def min_max(x):\n",
    "    return (x - np.min(x)) / (np.max(x) - np.min(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1796, 4096]) torch.Size([1796])\n"
     ]
    }
   ],
   "source": [
    "mol_img_data = torch.tensor(np.genfromtxt('../data/toxicity/molasimg.csv', delimiter=',', skip_header =False),dtype=torch.float32)\n",
    "train, test=train_test_split(mol_img_data ,test_size= 0.2 , train_size= 0.8, random_state=1234)\n",
    "\n",
    "train_x = train[:,:-1]\n",
    "train_y = train[:,-1]\n",
    "test_x = test[:,:-1]\n",
    "test_y = test[:,-1]\n",
    "\n",
    "\n",
    "print(train_x.shape, train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd5c996dfd0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_x[10,:].view(64,64), cmap= \"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x.view(train_x.shape[0],1,64,64)\n",
    "test_x = test_x.view(test_x.shape[0],1,64,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 64, 64]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "torch_train = data.TensorDataset(train_x, train_y)\n",
    "train_loader = data.DataLoader(torch_train, batch_size=32)\n",
    "\n",
    "batch_x, batch_y =next(iter(train_loader))\n",
    "print(batch_x.shape, batch_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = nn.Sequential(nn.Conv2d(1,3,3,1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(2,2),\n",
    "                   nn.Conv2d(3,6,3,1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(2,2),\n",
    "                   nn.Conv2d(6,12,3,1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 12, 6, 6])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn(batch_x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add_module(\"flatten\",nn.Flatten(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 432])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn(batch_x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add_module(\"bn\", nn.BatchNorm1d(432))\n",
    "cnn.add_module(\"dp\", nn.Dropout(0.2))\n",
    "cnn.add_module(\"fc\", nn.Linear(432,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn(batch_x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.70 Training Accuracy: 0.60 Test Accuracy: 0.58\n",
      "Training Loss: 0.66 Training Accuracy: 0.64 Test Accuracy: 0.62\n",
      "Training Loss: 0.64 Training Accuracy: 0.66 Test Accuracy: 0.63\n",
      "Training Loss: 0.63 Training Accuracy: 0.66 Test Accuracy: 0.64\n",
      "Training Loss: 0.62 Training Accuracy: 0.68 Test Accuracy: 0.66\n",
      "Training Loss: 0.60 Training Accuracy: 0.69 Test Accuracy: 0.66\n",
      "Training Loss: 0.60 Training Accuracy: 0.69 Test Accuracy: 0.67\n",
      "Training Loss: 0.59 Training Accuracy: 0.69 Test Accuracy: 0.67\n",
      "Training Loss: 0.58 Training Accuracy: 0.70 Test Accuracy: 0.68\n",
      "Training Loss: 0.58 Training Accuracy: 0.69 Test Accuracy: 0.65\n",
      "Training Loss: 0.57 Training Accuracy: 0.68 Test Accuracy: 0.65\n",
      "Training Loss: 0.56 Training Accuracy: 0.61 Test Accuracy: 0.58\n",
      "Training Loss: 0.57 Training Accuracy: 0.68 Test Accuracy: 0.65\n",
      "Training Loss: 0.56 Training Accuracy: 0.69 Test Accuracy: 0.66\n",
      "Training Loss: 0.57 Training Accuracy: 0.70 Test Accuracy: 0.66\n",
      "Training Loss: 0.56 Training Accuracy: 0.74 Test Accuracy: 0.69\n",
      "Training Loss: 0.56 Training Accuracy: 0.74 Test Accuracy: 0.70\n",
      "Training Loss: 0.55 Training Accuracy: 0.74 Test Accuracy: 0.69\n",
      "Training Loss: 0.55 Training Accuracy: 0.73 Test Accuracy: 0.69\n",
      "Training Loss: 0.54 Training Accuracy: 0.75 Test Accuracy: 0.71\n",
      "Training Loss: 0.55 Training Accuracy: 0.67 Test Accuracy: 0.64\n",
      "Training Loss: 0.54 Training Accuracy: 0.75 Test Accuracy: 0.70\n",
      "Training Loss: 0.54 Training Accuracy: 0.75 Test Accuracy: 0.70\n",
      "Training Loss: 0.53 Training Accuracy: 0.75 Test Accuracy: 0.69\n",
      "Training Loss: 0.53 Training Accuracy: 0.76 Test Accuracy: 0.70\n",
      "Training Loss: 0.52 Training Accuracy: 0.76 Test Accuracy: 0.70\n",
      "Training Loss: 0.52 Training Accuracy: 0.76 Test Accuracy: 0.69\n",
      "Training Loss: 0.52 Training Accuracy: 0.76 Test Accuracy: 0.69\n",
      "Training Loss: 0.51 Training Accuracy: 0.76 Test Accuracy: 0.69\n",
      "Training Loss: 0.51 Training Accuracy: 0.77 Test Accuracy: 0.68\n"
     ]
    }
   ],
   "source": [
    "loss_funktion = nn.BCEWithLogitsLoss()\n",
    "updaten =  torch.optim.Adam(cnn.parameters(), lr =0.0003)\n",
    "EPOCHS = 30\n",
    "for i in range(EPOCHS):\n",
    "    loss_list = [] # diese Liste speichter den Loss jedes Minibatches\n",
    "    cnn.train() \n",
    "    for minibatch in train_loader: # for-loop geht durch alle minibatches\n",
    "        images, labels = minibatch # minibatch wird in Bilder und Labels geteilt\n",
    "\n",
    "        updaten.zero_grad()\n",
    "        output = cnn(images) # Forward Propagation\n",
    "        loss   = loss_funktion(output.squeeze(), labels)\n",
    "        loss.backward()\n",
    "        loss_list.append(loss.item())\n",
    "        updaten.step()\n",
    "    cnn.eval()    \n",
    "    \n",
    "    output = cnn(train_x)\n",
    "    train_acc = torch.sum((output>0).squeeze().int() == train_y)/train_y.shape[0]\n",
    "    output = cnn(test_x)\n",
    "    test_acc = torch.sum((output>0).squeeze().int() == test_y)/test_y.shape[0]\n",
    "    \n",
    "    print(\n",
    "        \"Training Loss: %.2f Training Accuracy: %.2f Test Accuracy: %.2f\"\n",
    "        % (np.mean(loss_list), train_acc, test_acc )\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
