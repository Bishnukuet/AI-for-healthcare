{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "train_img = train_data[:,:-1].view(60000,1,28,28)\n",
    "plt.imshow(ex.numpy()[0], cmap = \"Greys\")\n",
    "\n",
    "torch_train = data.TensorDataset(train_img, train_data[:,-1])\n",
    "train_loader = data.DataLoader(torch_train, batch_size=32)\n",
    "example_x, example_y =  next(iter(train_loader))\n",
    "\n",
    "conv1 = nn.Conv2d(1, 3, 3,padding = 1)\n",
    "\n",
    "\n",
    "conv2 = nn.Conv2d(3, 6, 3,stride =2,padding = 1)\n",
    "\n",
    "\n",
    "a = nn.Sequential(nn.Conv2d(1, 3, 3,padding = 1), nn.Conv2d(3, 6, 3,padding = 1))\n",
    "\n",
    "tada =a(example_x)\n",
    "tada.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils import data\n",
    "\n",
    "def min_max(x):\n",
    "    return (x - np.min(x)) / (np.max(x) - np.min(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zunächst laden wir wieder die Trainingsdaten ein und konvertieren Sie zu einem Tensor. Insgesamt sind es 60,000 Bilder und 784 Pixel + eine Spalte für die Labels der Bilder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 784]) torch.Size([60000])\n"
     ]
    }
   ],
   "source": [
    "train_data = np.genfromtxt('../data/mnist/mnist_train.csv', delimiter=',', skip_header =False)\n",
    "train_data = min_max(x)\n",
    "\n",
    "\n",
    "train_data = torch.tensor(train_data, dtype= torch.long)\n",
    "train_x = train_data[:,:-1]\n",
    "train_y = train_data[:,-1]\n",
    "print(train_x.shape, train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bisher haben wir Bilder immer als 1D Input in unsere Neuronales Netzwerk eingeführt. Wir wollen diesmal aber die 2D Struktur benutzen. Dafür müssen wir aus einem Vektor der Länge `784` eine Matrix mit den Maßen `28 x 28` machen.\n",
    "\n",
    "Hierfür können wir die Funktion `vektor.view(28,28)` benutzen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 28])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[0,:].view(28,28).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir können uns dieses Bild anschauen, können aber nicht viel erkennen. Man könnte eventuell erahnen, um welche Zahl es sich handeln soll. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  5.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   3.,  18.,  18.,  18., 126., 136., 175.,  26., 166., 255., 247.,\n",
       "         127.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  30.,  36.,  94.,\n",
       "         154., 170., 253., 253., 253., 253., 253., 225., 172., 253., 242., 195.,\n",
       "          64.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  49., 238., 253., 253.,\n",
       "         253., 253., 253., 253., 253., 253., 251.,  93.,  82.,  82.,  56.,  39.,\n",
       "           0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  18., 219., 253., 253.,\n",
       "         253., 253., 253., 198., 182., 247., 241.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  80., 156., 107.,\n",
       "         253., 253., 205.,  11.,   0.,  43., 154.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  14.,   1.,\n",
       "         154., 253.,  90.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         139., 253., 190.,   2.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          11., 190., 253.,  70.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,  35., 241., 225., 160., 108.,   1.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,  81., 240., 253., 253., 119.,  25.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,  45., 186., 253., 253., 150.,  27.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,  16.,  93., 252., 253., 187.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0., 249., 253., 249.,  64.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,  46., 130., 183., 253., 253., 207.,   2.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,  39., 148., 229., 253., 253., 253., 250., 182.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  24.,\n",
       "         114., 221., 253., 253., 253., 253., 201.,  78.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  23.,  66., 213.,\n",
       "         253., 253., 253., 253., 198.,  81.,   2.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  18., 171., 219., 253., 253.,\n",
       "         253., 253., 195.,  80.,   9.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,  55., 172., 226., 253., 253., 253., 253.,\n",
       "         244., 133.,  11.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0., 136., 253., 253., 253., 212., 135., 132.,\n",
       "          16.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[0,:].view(28,28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doch mithilfe von `matplotlib` können wir Pixel Arrays auch darstellen. `cmap = \"greys\"` gibt hierbei an, dass wir unser Farbspektrum nur Schwarz-Weiß haben wollen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f222d0c6150>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_x[0,:].view(28,28), cmap= \"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir haben bis jetzt nur ein Bild das richtige Format gebracht, um alle Bilder auf einmal in dem richtigen Format zu bringen, können wir aber auch `.view()` benutzen. Das Tensor oben hatte den Format `(Höhe,Breite)`. Damit wir alle Bilder konvertieren können, müssen wir denn `tensor` um eine weitere Dimension erweitern.  Wir wollen einen `tensor` mit den Dimensionen `(Anzahl Bilder, Höhe, Breite)`.\n",
    "\n",
    "Allerdings würde hier PyTorch einen Strich durch die Rechnung machen. PyTorch kann sowohl mit Schwarz-Weiß Bildern, als auch mit farbigen Bildern arbeiten. In PyTorch werden farbige Bilder über drei Matrizen dargestellt. Eine für Rot, eine für Grün und eine für Blau. Diese werden auch als Channel bezeichnet. Also ein farbiges Bild hat 3 Channel, ein s/w Bild hat aber nur einen.\n",
    "Ein farbiges Bild würde in PyTorch die Dimensionen `(3, Höhe, Breite)` haben. Deswegen geht PyTorch davon aus, dass alle Bilder aus drei Dimensionen besteht. Für eine Schwarz/Weiß Bild brauchen wir deshalb auch eine weitere Dimension. Die dritte Dimension hat aber nur die Größe eins, da wir nur einen Channel haben.\n",
    "![](https://miro.medium.com/max/700/1*icINeO4H7UKe3NlU1fXqlA.jpeg)\n",
    "\n",
    "<center><h7>Source: Mathanraj Sharma, 2019 </h7></center>\n",
    "\n",
    "Deshalb stellen wir ein s/w Bild wie folgt dar: `(1, Höhe, Breite)`. Daraus folgt, dass alle Bilder vom MNIST Datensatz diesem Format entsprechen müssen: `(Anzahl Bilder, 1, Höhe, Breite)`\n",
    "\n",
    "\n",
    "\n",
    "Konvertieren Sie `train_x` in dieses Format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 1, 28, 28])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x = train_x.view(_____,1,____,____)\n",
    "train_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Lösung:</b></summary>\n",
    "    \n",
    "```python \n",
    "train_x = train_x.view(60000,1,28,28)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sie haben jetzt alle Bilder in das Format `(1,28,28)` konvertiert.\n",
    "Sie können jetzt immer noch die Bilder mit `plt.imshow` anzeigen lassen.\n",
    "\n",
    "Beachten Sie, wie jetzt der Tensor indiziert ist. `[0,0,:,:]`. Wir wählen das erste Bild aus, und auch den ersten und einzigen Channel. Wir wählen natürlich die gesamte Höhe und Breite aus, um das Bild komplett darzustellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f222a9eef90>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_x[0,0,:,:], cmap= \"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie letzte Woche können Sie sich auch diesmal einen `DataLoader` benutzen. Dafür müssen wir erst ein TensorDataset erstellen. Mit `next(iter())` können wir uns den ersten Batch des Dataloaders ausgeben lassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_____' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-3a2240084528>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_____\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m____\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m______\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name '_____' is not defined"
     ]
    }
   ],
   "source": [
    "torch_train = data.TensorDataset(_____, ____)\n",
    "train_loader = data.DataLoader(______, batch_size=32)\n",
    "\n",
    "batch_x, batch_y =next(iter(train_loader))\n",
    "print(batch_x.shape, batch_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Lösung:</b></summary>\n",
    "    \n",
    "```python \n",
    "torch_train = data.TensorDataset(train_x,train_y)\n",
    "train_loader = data.DataLoader(torch_train, batch_size=32)\n",
    "\n",
    "batch_x, batch_y =next(iter(train_loader))\n",
    "print(batch_x.shape, batch_y.shape)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[5, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0]]],\n",
       "\n",
       "\n",
       "        [[[0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0]]],\n",
       "\n",
       "\n",
       "        [[[4, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[7, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0]]],\n",
       "\n",
       "\n",
       "        [[[3, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0]]],\n",
       "\n",
       "\n",
       "        [[[8, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0]]]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_train = data.TensorDataset(train_x,train_y)\n",
    "train_loader = data.DataLoader(torch_train, batch_size=32)\n",
    "\n",
    "batch_x, batch_y =next(iter(train_loader))\n",
    "batch_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie Sie sehen können, hat der `batch_x` die Dimensionen `[32, 1, 28, 28]`. Also `32` Bilder, die Größe unseres Batches, `1`Channel, `28` Pixel in der Höhe und `28` in der Breite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNNs in PyTorch erstellen.\n",
    "\n",
    "Wir haben soweit unsere Daten im richtigen Format, jetzt beschäftigen wir uns mit dem erstellen von `CNN` in PyTorch. Sowie es `linear` layers in PyTorch gibt, gibt es auch Convolutional Layers im `nn` Modul.\n",
    "\n",
    "`nn.Conv2d()` ist so eine Layer. Bevor wir Sie benutzen, besprechen wir kurz die wichitgsten Parameter.\n",
    "\n",
    "- `in_channels` Die Anzahl der Channels die das Bild vor Der Convolution hat \n",
    "- `out_channels` Wie viel Channels soll, das Bild nach der Convolution haben.\n",
    "- `kernel_size` Wie groß ist der Kenel, also die Höhe/Breite in Pixeln\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = conv1 = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 26, 26])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = conv1(batch_x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sie sehen, der Batch hat sich in der Größe verändert. Zunächst haben wir immer noch `32` Bilder, allerdings wie spezifiziert, haben wir jetzt `3` Channel. Auch die Höhe und Breite unseres Bildes hat sich verändert, insgesamt haben wir jeweils 2 Pixel pro Dimension verloren. Das liegt daran, wie Convolutions funktionieren.\n",
    "\n",
    "![](https://miro.medium.com/max/700/1*L4T6IXRalWoseBncjRr4wQ@2x.gif)\n",
    "<center><h7>Source: Arden Dertat, 2017 </h7></center>\n",
    "\n",
    "Hier sehen an einem Beispiel warum bei einer Kernel Size von 3 unser Output Bild um zwei Pixel kleiner wird. Links ist das Inputfile und rechts der Output. Da wir den Kernel nicht über den Rand des Bildes schieben können, \"verlieren\" wir den äußern Rand des Bildes\n",
    "\n",
    "Um zu verhindern, dass diese Information verloren geht, können wir das Bild *padden*. Dadurch vergrößern wir das Bild, zum Beispiel mit Pixel, die den Wert Null haben.\n",
    "![](https://miro.medium.com/max/700/1*W2D564Gkad9lj3_6t9I2PA@2x.gif)\n",
    "<center><h7>Source: Arden Dertat, 2017 </h7></center>\n",
    "\n",
    "Durch das Padding, kann der Kernel einmal über das ganze Bild geschoben werden.\n",
    "Wir können die Breite des Paddings auch als Parameter in `Conv2d` mitgeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 28, 28])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1 = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=3, padding =1)\n",
    "out = conv1(batch_x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durch das Padding schrumpft nun das Bild nicht in der Breite. Dadurch, dass wir jetzt `3` Channel haben, können wir dieses immer noch mit `plt.imshow` uns zeigen lassen.  Hierbei müssen wir den ein Bild aus dem Batch auswählen und mit dem Befehl `detach()` die Gradienten, die durch `autograd` gespeichert werden, entfernen.\n",
    "\n",
    "*Ein solches Bild, kann man nur als Beispiel benutzen um die Transformation zu verdeutlichen. Die tatsächlichen Farben und Intensitäten sind hier aber irrelevant, da Sie von dem Netzwerk arbiträr gewählt sind. Zum Beispiel, gibt die Reihenfolge der Channels an, welcher Channel für welche Farbe zuständig ist. Ein Convolution ist sich natürlich nicht bewusst, dass es so eine Ordnung in den Channels gibt.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f2225d79450>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(out.detach().numpy()[0].transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sie können immer noch eine 5 erkennen, allerdings diesmal in Farbe. Wie oben schon beschrieben, eigenen sich die Farben nicht zum Interpretieren. Es soll lediglich die Diversifizierung des Inputs darstellen. Der weitere neue Layer die Sie heute benutzen werden ist `nn.MaxPool2d()`. \n",
    "\n",
    "Dieses Layer ist einen **Pooling** Layer.\n",
    "\n",
    "\n",
    "![](https://miro.medium.com/max/700/1*ReZNSf_Yr7Q1nqegGirsMQ@2x.png)\n",
    "<center><h7>Source: Arden Dertat, 2017 </h7></center>\n",
    "\n",
    "Pooling Layer führen zu eine Reduzierung der Bildgröße. Dadurch brauchen weniger Parameter (Weights), was dazu führt, das unsere Netzwerke schneller trainieren. Wenn Sie ein Bild (größer als 28 x 28 Pixel)anschauen, dann sehen nicht jeden einzelnen Pixel, sondern Pixel in einer gewissen Proximität werden schmelzen zusammen. Pooling funktioniert ähnlich. Hier werden mehrer Pixel mit Hifle des domminantesten Wertes zusammen gefasst.\n",
    "Weniger Parameter, bedeuete auch eine geringere Chance zu overfitten. \n",
    "\n",
    "Die meist benutzte Pooling Layer ist die Max Pooling Layer. Hierbei wird der größte Wert im Kernel, als neuer Wert für den Output gewählt. Es gibt natürlich eine Vielzahl von anderen [Pooling](https://pytorch.org/docs/stable/nn.html#pooling-layers) Layers.\n",
    "\n",
    "Neben der Kernel Size, die Größe der des Quadrates das gepoolt werden soll, geben wir diesmal auch den `stride` an. Der Stride gibt an, um wieviele Pixel wir den Kernel verschieben. \n",
    "\n",
    "![](https://www.oreilly.com/library/view/machine-learning-for/9781786469878/assets/09ad7edc-334f-4c54-944b-af21139b0587.png)\n",
    "<center><h7>Source: Rodolfo Bonnin - Machine Learning for Developers </h7></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 14, 14])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool1 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sie können jetzt den Output der 2DConv (`out`) als Input für die Pooling Layer benutzen.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 14, 14])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out2 = pool1(____)\n",
    "out2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Lösung:</b></summary>\n",
    "    \n",
    "```python \n",
    "out2 = pool1(out)\n",
    "```\n",
    "</details>\n",
    "\n",
    "Da sich nichts an der Anzahl der Channels geändert hat, können wir dieses Bild immernoch visualsieren.\n",
    "Es ist zu erkennen, dass sich das Bild verkleinert hat, dennoch könnnen wir noch eine 5 erkennen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f2229903310>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(out2.detach().numpy()[0].transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit `nn.Sequential` können Sie auch wieder mehrere Layers hintereinander schalten. Wichitg, wir brauchen auch wieder eine nicht-lineare Aktivierungsfunktion, diese wird normalerweise nach dem Convolution eingefügt.\n",
    "\n",
    "Füllen Sie den Code aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for %: 'torch.Size' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_990868/63633859.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m cnn = nn.Sequential(nn.Conv2d(_,3,3,1),\n\u001b[0m\u001b[1;32m      2\u001b[0m                    \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                    \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaxPool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                    \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                    \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m______\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/intro_ki/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias, padding_mode, device, dtype)\u001b[0m\n\u001b[1;32m    430\u001b[0m         super(Conv2d, self).__init__(\n\u001b[1;32m    431\u001b[0m             \u001b[0min_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilation_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m             False, _pair(0), groups, bias, padding_mode, **factory_kwargs)\n\u001b[0m\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/intro_ki/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, bias, padding_mode, device, dtype)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mfactory_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dtype'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0min_channels\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mgroups\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'in_channels must be divisible by groups'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mout_channels\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mgroups\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for %: 'torch.Size' and 'int'"
     ]
    }
   ],
   "source": [
    "cnn = nn.Sequential(nn.Conv2d(_,3,3,1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(2,2),\n",
    "                   nn.Conv2d(__,6,3,1),\n",
    "                   nn.______,\n",
    "                   nn.MaxPool2d(2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Lösung:</b></summary>\n",
    "    \n",
    "```python \n",
    "cnn = nn.Sequential(nn.Conv2d(1,3,3,1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(2,2),\n",
    "                   nn.Conv2d(3,6,3,1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(2,2))\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt können wir den Batch `batch_x` einmal durch das Netzwerk führen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 6, 5, 5])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_out = cnn(batch_x)\n",
    "cnn_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dieser Output eignet sich aber noch nicht um Vorhersagen zumachen. Um diese zu machen müssen wir die Bilder wieder ein traditionales Neuronales Netzwerk benutzen. Doch diese akzeptieren nur Input in Form eines Vektors. Deshlab konvertieren wir jedes Bild zurück in einen Vektor. \n",
    "\n",
    "Also unser Tensor hat die `shape` `[32, 6, 5, 5]` und soll zu einem Tensor `[32, 6 x 5 x 5]` = `[32, 150]`.\n",
    "\n",
    "Dafür können wir die \"Layer\" `nn.Flatten(starting_dim)` benutzen. Hierbei müssen wir nur den Parameter `starting_dim` festlegen. Dieser bestimmt ab welche Dimenion wir beginnen die Dimensionen zusamme zu führen. Da wir die für jedes Bild einen eigenen Vektor wollen, benutzen eine `starting_dim = 1`. Mit cnn.add_module(), können wir noch extra Layers zu unserem Modul hinzufügen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(1, 3, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (1): ReLU()\n",
       "  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (3): Conv2d(3, 6, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (4): ReLU()\n",
       "  (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.add_module(\"flatten\",nn.Flatten(1))\n",
    "cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 150])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn(batch_x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add_module(\"fc\", nn.Linear(150,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_funktion = nn.CrossEntropyLoss()\n",
    "updaten =  torch.optim.SGD(cnn.parameters(), lr =0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Long but found Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_990868/2196020792.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mupdaten\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Forward Propagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mloss\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mloss_funktion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/intro_ki/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/intro_ki/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/intro_ki/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/intro_ki/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/intro_ki/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    438\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    439\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 440\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Long but found Float"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "for i in range(EPOCHS):\n",
    "    loss_list = [] # diese Liste speichter den Loss jedes Minibatches\n",
    "                   # damit können wir am Ende den Durschnittslost innerhalb des Epochs berechnen\n",
    "    for minibatch in train_loader: # for-loop geht durch alle minibatches\n",
    "        images, labels = minibatch # minibatch wird in Bilder und Labels geteilt\n",
    "        \n",
    "        updaten.zero_grad()\n",
    "        output = cnn(images) # Forward Propagation\n",
    "    \n",
    "        loss   = loss_funktion(output, labels)\n",
    "        loss.backward()\n",
    "        loss_list.append(loss.item())\n",
    "        updaten.step()\n",
    "        \n",
    "    output = netzwerk(train_images)\n",
    "    acc=((output.max(dim=1)[1]==train_labels).sum()/float(output.shape[0])).item()\n",
    "    print(\n",
    "        \"Training Loss: %.2f Training Accuracy: %.2f\"\n",
    "        % (np.mean(loss_list), acc)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
