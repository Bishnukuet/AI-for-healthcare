{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "train_img = train_data[:,:-1].view(60000,1,28,28)\n",
    "plt.imshow(ex.numpy()[0], cmap = \"Greys\")\n",
    "\n",
    "torch_train = data.TensorDataset(train_img, train_data[:,-1])\n",
    "train_loader = data.DataLoader(torch_train, batch_size=32)\n",
    "example_x, example_y =  next(iter(train_loader))\n",
    "\n",
    "conv1 = nn.Conv2d(1, 3, 3,padding = 1)\n",
    "\n",
    "\n",
    "conv2 = nn.Conv2d(3, 6, 3,stride =2,padding = 1)\n",
    "\n",
    "\n",
    "a = nn.Sequential(nn.Conv2d(1, 3, 3,padding = 1), nn.Conv2d(3, 6, 3,padding = 1))\n",
    "\n",
    "tada =a(example_x)\n",
    "tada.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zunächst laden wir wieder die Trainingsdaten ein und konverteiren Sie zu einem Tensor. Insgesamt sindes 60,000 Bilder und 784 Pixel + eine Splate für die Labels der Bilder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 784]) torch.Size([60000])\n"
     ]
    }
   ],
   "source": [
    "train_data = np.genfromtxt('../data/mnist/mnist_train.csv', delimiter=',', skip_header =False)\n",
    "train_data = torch.tensor(train_data, dtype= torch.float)\n",
    "train_x = train_data[:,:-1]\n",
    "train_y = train_data[:,-1]\n",
    "print(train_x.shape, train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bisher haben wir Bilder immer als 1D Input in unsere Neuronales Netzwerk eingeführt. Wir wollen diesmal aber die 2D Struktur benutzen. Dafür müssen wir aus einem Vektor der Länge `784` eine Matrix mit den Maßen `28 x 28` machen.\n",
    "\n",
    "Hierfür können wir die Funktion `vektor.view(28,28)` benutzen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 28])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[0,:].view(28,28).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir können uns dieses Bild anschauen, können aber nicht viel erkennen. Man könnte eventuell erahnen, um welche Zahl es sich handeln soll. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  5.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   3.,  18.,  18.,  18., 126., 136., 175.,  26., 166., 255., 247.,\n",
       "         127.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  30.,  36.,  94.,\n",
       "         154., 170., 253., 253., 253., 253., 253., 225., 172., 253., 242., 195.,\n",
       "          64.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  49., 238., 253., 253.,\n",
       "         253., 253., 253., 253., 253., 253., 251.,  93.,  82.,  82.,  56.,  39.,\n",
       "           0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  18., 219., 253., 253.,\n",
       "         253., 253., 253., 198., 182., 247., 241.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  80., 156., 107.,\n",
       "         253., 253., 205.,  11.,   0.,  43., 154.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  14.,   1.,\n",
       "         154., 253.,  90.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         139., 253., 190.,   2.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          11., 190., 253.,  70.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,  35., 241., 225., 160., 108.,   1.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,  81., 240., 253., 253., 119.,  25.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,  45., 186., 253., 253., 150.,  27.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,  16.,  93., 252., 253., 187.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0., 249., 253., 249.,  64.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,  46., 130., 183., 253., 253., 207.,   2.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,  39., 148., 229., 253., 253., 253., 250., 182.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  24.,\n",
       "         114., 221., 253., 253., 253., 253., 201.,  78.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  23.,  66., 213.,\n",
       "         253., 253., 253., 253., 198.,  81.,   2.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  18., 171., 219., 253., 253.,\n",
       "         253., 253., 195.,  80.,   9.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,  55., 172., 226., 253., 253., 253., 253.,\n",
       "         244., 133.,  11.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0., 136., 253., 253., 253., 212., 135., 132.,\n",
       "          16.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[0,:].view(28,28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doch mithilfe von `matplotlib` können wir Pixel Arrays auch darstellen. `cmap = \"Greys\"` gibt hierbei an, dass wir unser Farbspektrum nur Schwarz-Weiß haben wollen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ffa5176aa00>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAOU0lEQVR4nO3dfYxUZZbH8d8RQVSIQWlY4pDt2YmaNSbbgyVZgxCWcQnyDxCczZA4YSPZnviSDIaYNb2J40tiiFlmxGgm6dlGmM2s4yggmJhdlJAYEkVLRQXxPe3QA0ITlRGizAJn/+jLpsWqp5qqW3ULzveTdKrqnvv0PZb941bVvbcec3cBOPedV3QDAFqDsANBEHYgCMIOBEHYgSDOb+XGJk6c6J2dna3cJBBKf3+/Dh06ZJVqDYXdzOZJWi1plKT/cPeVqfU7Ozu1Y8eOqvVRo0Y10g4QXqlUqlqr+2W8mY2S9LikmyRdLWmJmV1d7+8D0FyNvGefLukjd//E3f8i6feSFuTTFoC8NRL2yyXtHfZ4IFv2LWbWbWZlMysPDg42sDkAjWgk7JU+BPjOubfu3uvuJXcvdXR0NLA5AI1oJOwDkqYOe/w9SfsaawdAszQS9tckXWFm3zezMZJ+ImlzPm0ByFvdh97c/biZ3SnpfzR06G2Nu++uNY7Da0AxGjrO7u7PS3o+p14ANBGnywJBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSAIOxBEQ7O44ux38uTJZP3YsWNN2/a6deuS9aNHjybr7777brL+yCOPVK319PQkxz722GPJ+oUXXpisr1q1Klm/7bbbkvVmaCjsZtYv6StJJyQdd/dSHk0ByF8ee/Z/cPdDOfweAE3Ee3YgiEbD7pK2mNnrZtZdaQUz6zazspmVBwcHG9wcgHo1GvYZ7j5N0k2S7jCzWaev4O697l5y91JHR0eDmwNQr4bC7u77stuDkjZKmp5HUwDyV3fYzexiMxt/6r6kuZJ25dUYgHw18mn8ZEkbzezU7/kvd//vXLoK5vDhw8n6iRMnkvW33nqram3Lli3JsV9++WWy3tvbm6wXqbOzM1lfsWJF1VpfX19y7CWXXJKsz5w5M1mfM2dOsl6EusPu7p9I+rscewHQRBx6A4Ig7EAQhB0IgrADQRB2IAgucW2BgYGBZL2rqytZ/+KLL3Ls5uxx3nnpfVGtw2epy1CXLVuWHDtp0qRkfdy4ccl6O54typ4dCIKwA0EQdiAIwg4EQdiBIAg7EARhB4LgOHsLXHbZZcn65MmTk/V2Ps4+d+7cZD31375hw4bk2AsuuCBZnz17drKOb2PPDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBcJy9BWpN77t27dpk/ZlnnknWr7/++qq1xYsXJ8fWcsMNNyTrmzZtStbHjBlTtfbZZ58lx65evTpZx5lhzw4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQZi7t2xjpVLJy+Vyy7Z3rjh27FiynjqW3dPTkxz78MMPJ+vbtm1L1mfNmpWso7VKpZLK5bJVqtXcs5vZGjM7aGa7hi271MxeMLMPs9sJeTYMIH8jeRm/VtK805bdI2mru18haWv2GEAbqxl2d39J0uenLV4gaV12f52khfm2BSBv9X5AN9nd90tSdlt1Yiwz6zazspmVBwcH69wcgEY1/dN4d+9195K7l9pxsjsginrDfsDMpkhSdnswv5YANEO9Yd8saWl2f6mk9HWOAApX83p2M3tS0mxJE81sQNIvJK2U9AczWybpj5J+3Mwmo6v1/ekpEyY0dlT00UcfTdZnzpyZrJtVPOSLAtQMu7svqVL6Uc69AGgiTpcFgiDsQBCEHQiCsANBEHYgCL5K+hy3fPnyZP3VV19N1jdu3Jis7969O1m/5pprknW0Dnt2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiC4+znuNTXTEtSb29vsr5169ZkfcGCBcn6woULq9ZmzJiRHLto0aJknctnzwx7diAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IgimbkVTrevd5806f8/PbDh8+XPe216xZk6wvXrw4WR83blzd2z5bNTRlM4BzA2EHgiDsQBCEHQiCsANBEHYgCMIOBMH17EiaPn16sl7re+PvuuuuqrWnn346OfbWW29N1j/++ONk/e67765aGz9+fHLsuajmnt3M1pjZQTPbNWzZfWb2JzPbmf3Mb26bABo1kpfxayVVOk3qV+7elf08n29bAPJWM+zu/pKkz1vQC4AmauQDujvN7O3sZf6EaiuZWbeZlc2sPDg42MDmADSi3rD/WtIPJHVJ2i9pVbUV3b3X3UvuXuro6KhzcwAaVVfY3f2Au59w95OSfiMp/ZEtgMLVFXYzmzLs4SJJu6qtC6A91Lye3cyelDRb0kRJByT9InvcJckl9Uv6mbvvr7UxrmeP55tvvqlae+WVV5Jjb7zxxmS91t/uzTffXLX21FNPJceerVLXs9c8qcbdl1RY3NdwVwBaitNlgSAIOxAEYQeCIOxAEIQdCIJLXNFUY8eOrVqbPXt2cuyoUaOS9ePHjyfrzz77bNXa+++/nxx71VVXJetnI/bsQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAEx9nRkH379iXrGzZsqFp7+eWXk2NrHUev5brrrqtau/LKKxv63Wcj9uxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EATH2YOrNSXX448/nqw/8cQTyfrAwMAZ9zRSta537+zsrFozq/hty+c09uxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EATH2c8BR44cqVp77rnnkmMfeOCBZP2DDz6oq6c8zJkzJ1lfuXJlsn7ttdfm2c5Zr+ae3cymmtk2M9tjZrvN7OfZ8kvN7AUz+zC7ndD8dgHUayQv449LWuHufyvp7yXdYWZXS7pH0lZ3v0LS1uwxgDZVM+zuvt/d38jufyVpj6TLJS2QtC5bbZ2khU3qEUAOzugDOjPrlPRDSTskTXb3/dLQPwiSJlUZ021mZTMr1zoPG0DzjDjsZjZO0npJy939zyMd5+697l5y91JHR0c9PQLIwYjCbmajNRT037n7qa8LPWBmU7L6FEkHm9MigDzUPPRmQ9cC9kna4+6/HFbaLGmppJXZ7aamdBjA0aNHk/W9e/cm67fcckvV2ptvvllXT3mZO3du1dr999+fHJv6Kmgp5mWqjRjJcfYZkn4q6R0z25kt69FQyP9gZssk/VHSj5vSIYBc1Ay7u2+XVO2f0B/l2w6AZuF0WSAIwg4EQdiBIAg7EARhB4LgEtccfP3118n68uXLk/Xt27cn6++9996ZtpSb+fPnJ+v33ntvst7V1VW1Nnr06HpaQp3YswNBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEBxnz/T39yfrDz30UNXaiy++mBz76aef1tNSLi666KJk/cEHH0zWb7/99mR9zJgxZ9wTisGeHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeC4Dh7Zv369cl6X19f07Y9bdq0ZH3JkiXJ+vnnV//f2N3dnRw7duzYZB3nDvbsQBCEHQiCsANBEHYgCMIOBEHYgSAIOxCEuXt6BbOpkn4r6a8knZTU6+6rzew+Sf8iaTBbtcfdn0/9rlKp5OVyueGmAVRWKpVULpcrzro8kpNqjkta4e5vmNl4Sa+b2QtZ7Vfu/u95NQqgeUYyP/t+Sfuz+1+Z2R5Jlze7MQD5OqP37GbWKemHknZki+40s7fNbI2ZTagyptvMymZWHhwcrLQKgBYYcdjNbJyk9ZKWu/ufJf1a0g8kdWloz7+q0jh373X3kruXOjo6Gu8YQF1GFHYzG62hoP/O3TdIkrsfcPcT7n5S0m8kTW9emwAaVTPsZmaS+iTtcfdfDls+ZdhqiyTtyr89AHkZyafxMyT9VNI7ZrYzW9YjaYmZdUlySf2SftaE/gDkZCSfxm+XVOm4XfKYOoD2whl0QBCEHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIGp+lXSuGzMblPTpsEUTJR1qWQNnpl17a9e+JHqrV569/bW7V/z+t5aG/TsbNyu7e6mwBhLatbd27Uuit3q1qjdexgNBEHYgiKLD3lvw9lPatbd27Uuit3q1pLdC37MDaJ2i9+wAWoSwA0EUEnYzm2dm75vZR2Z2TxE9VGNm/Wb2jpntNLNC55fO5tA7aGa7hi271MxeMLMPs9uKc+wV1Nt9Zvan7LnbaWbzC+ptqpltM7M9ZrbbzH6eLS/0uUv01ZLnreXv2c1slKQPJP2jpAFJr0la4u7vtrSRKsysX1LJ3Qs/AcPMZkk6Ium37n5NtuxhSZ+7+8rsH8oJ7v6vbdLbfZKOFD2NdzZb0ZTh04xLWijpn1Xgc5fo65/UguetiD37dEkfufsn7v4XSb+XtKCAPtqeu78k6fPTFi+QtC67v05DfywtV6W3tuDu+939jez+V5JOTTNe6HOX6Ksligj75ZL2Dns8oPaa790lbTGz182su+hmKpjs7vuloT8eSZMK7ud0NafxbqXTphlvm+eununPG1VE2CtNJdVOx/9muPs0STdJuiN7uYqRGdE03q1SYZrxtlDv9OeNKiLsA5KmDnv8PUn7CuijInffl90elLRR7TcV9YFTM+hmtwcL7uf/tdM03pWmGVcbPHdFTn9eRNhfk3SFmX3fzMZI+omkzQX08R1mdnH2wYnM7GJJc9V+U1FvlrQ0u79U0qYCe/mWdpnGu9o04yr4uSt8+nN3b/mPpPka+kT+Y0n/VkQPVfr6G0lvZT+7i+5N0pMaeln3vxp6RbRM0mWStkr6MLu9tI16+09J70h6W0PBmlJQbzdo6K3h25J2Zj/zi37uEn215HnjdFkgCM6gA4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEg/g+thk117EBIPQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_x[0,:].view(28,28), cmap= \"Greys\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir haben bis jetzt nur ein Bild das richtige Format gebracht, um alle Bilder auf einmal in dem richtigen Format zu bringen, können wir aber auch `.view()` benutzen. Das Tensor oben hatte den Format `(Höhe,Breite)`. Damit wir alle Bilder konvertieren können, müssen wir denn `tensor` um eine weitere Dimension erweitern.  Wir wollen einen `tensor` mit den Dimensionen `(Anzahl Bilder, Höhe, Breite)`.\n",
    "\n",
    "Allerdings würde hier PyTorch einen Strich durch die Rechnung machen. PyTorch kann sowohl mit Schwarz-Weiß Bildern, als auch mit farbigen Bildern arbeiten. In PyTorch werden farbige Bilder über drei Matrizen dargestellt. Eine für Rot, eine für Grün und eine für Blau. Diese werden auch als Channel bezeichnet. Also ein farbiges Bild hat 3 Channel, ein s/w Bild hat aber nur einen.\n",
    "Ein farbiges Bild würde in PyTorch die Dimensionen `(3, Höhe, Breite)` haben. Deswegen geht PyTorch davon aus, dass alle Bilder aus drei Dimensionen besteht. Für eine Schwarz/Weiß Bild brauchen wir deshalb auch eine weitere Dimension. Die dritte Dimension hat aber nur die Größe eins, da wir nur einen Channel haben.\n",
    "\n",
    "# BILD DAS CHANNEL ERKLÄRT\n",
    "\n",
    "Deshalb stellen wir ein s/w Bild wie folgt dar: `(1, Höhe, Breite)`. Daraus folgt, dass alle Bilder vom MNIST Datensatz diesem Format entsprechen müssen: `(Anzahl Bilder, 1, Höhe, Breite)`\n",
    "\n",
    "\n",
    "\n",
    "Konvertieren Sie `train_x` in dieses Format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x.view(_____,1,____,____)\n",
    "train_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Lösung:</b></summary>\n",
    "    \n",
    "```python \n",
    "train_x = train_x.view(60000,1,28,28)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sie haben jetzt alle Bilder in das Format `(1,28,28)` konvertiert.\n",
    "Sie können jetzt immer noch die Bilder mit `plt.imshow` anzeigen lassen.\n",
    "\n",
    "Beachten Sie, wie jetzt der Tensor indiziert ist. `[0,0,:,:]`. Wir wählen das erste Bild aus, und auch den ersten und einzigen Channel. Wir wählen natürlich die gesamte Höhe und Breite aus, um das Bild komplett darzustellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ffa5177f2b0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_x[0,0,:,:], cmap= \"Greys\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie letzte Woche können Sie sich auch diesmal einen `DataLoader` benutzen. Dafür müssen wir erst ein TensorDataset erstellen. Mit `next(iter())` können wir uns den ersten Batch des Dataloaders ausgeben lassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_____' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-3a2240084528>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_____\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m____\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m______\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name '_____' is not defined"
     ]
    }
   ],
   "source": [
    "torch_train = data.TensorDataset(_____, ____)\n",
    "train_loader = data.DataLoader(______, batch_size=32)\n",
    "\n",
    "batch_x, batch_y =next(iter(train_loader))\n",
    "print(batch_x.shape, batch_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Lösung:</b></summary>\n",
    "    \n",
    "```python \n",
    "torch_train = data.TensorDataset(train_x,train_y)\n",
    "train_loader = data.DataLoader(torch_train, batch_size=32)\n",
    "\n",
    "batch_x, batch_y =next(iter(train_loader))\n",
    "print(batch_x.shape, batch_y.shape)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie Sie sehen können, hat der `batch_x` die Dimensionen `[32, 1, 28, 28]`. Also `32` Bilder, die Größe unseres Batches, `1`Channel, `28` Pixel in der Höhe und `28` in der Breite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNNs in PyTorch erstellen.\n",
    "\n",
    "Wir haben soweit unsere Daten im richtigen Format, jetzt beschäftigen wir uns mit dem erstellen von `CNN` in PyTorch. Sowie es `linear` layers in PyTorch gibt, gibt es auch Convolutional Layers im `nn` Modul.\n",
    "\n",
    "`nn.Conv2d()` ist so eine Layer. Bevor wir Sie benutzen, besprechen wir kurz die wichitgsten Parameter.\n",
    "\n",
    "`in_channels`\n",
    "`out_channels`\n",
    "`kernel_size`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = conv1 = nn.Conv2d(1, 3, 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
