{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Erstes Neuronales Netzwerk\n",
    "\n",
    "Bevor wir das Netzwerk trainieren, müssen wir als Erstes unsere Libaries und Funktionen laden.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt # neben Numpy laden wir auch eine Funktion der Libary matplotlib -> sie ermöglicht es Graphen in Python zu erstellen\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aktiverungsfunktionen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x)) # np.exp() = e^()\n",
    "\n",
    "def softmax(x, axis):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=axis, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding\n",
    "\n",
    "Wir wollen die Labels nicht als einfache Zahl, sondern als Vektor aus Nullen und Einsen haben. Siehe unten:<br>\n",
    "<center>\n",
    "0  =  [1 0 0 0 0 0 0 0 0 0]<br>\n",
    "1  =  [0 1 0 0 0 0 0 0 0 0]<br>\n",
    "2  =  [0 0 1 0 0 0 0 0 0 0]<br>\n",
    "3  =  [0 0 0 1 0 0 0 0 0 0]<br>\n",
    "4  =  [0 0 0 0 1 0 0 0 0 0]<br>\n",
    "5  =  [0 0 0 0 0 1 0 0 0 0]<br>\n",
    "usw. <br>\n",
    "9  =  [0 0 0 0 0 0 0 0 0 1]<br>\n",
    "</center>\n",
    "\n",
    "Die Funktion `one-hot` macht genau das."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(x):\n",
    "    \"\"\"Die Labels der Bilder müssen noch in Vektoren von Länge 10 codiert werden\"\"\"\n",
    "    dod = len(set(x)) # Checkt wie viele verschieden Ziffern es im Datennsatz gibt\n",
    "    target = np.zeros([x.shape[0], dod]) # Eine Matrix aus Nullen wird erstellt\n",
    "    for i in range(x.shape[0]): # Der for-loop setzt eine 1 in die Matrix abhängig davon welches Label das Bild hat\n",
    "        target[i, x[i]] = 1\n",
    "\n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skaliert die originalen Bilder, die Werte zwischen 0 und 255 haben können, zu Bildern mit Werten zwischen 0 und 1.\n",
    "# Neuronale Netzwerke können besser trainiert werden\n",
    "def min_max(x):\n",
    "    return (x - np.min(x)) / (np.max(x) - np.min(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daten einlesen\n",
    "\n",
    "Wir können mit Numpy ohne Probleme die benötigten Daten einlesen. Die Trainingsdaten `mnist_train.csv` enhalten Bilder und deren Labels. Die Bilder sind schon von einer Matrix in einen Vektor transformiert worden.<br>\n",
    "<center>\n",
    "<img src=\"https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/73_blog_image_1.png\" style=\"width: 800px;\">\n",
    "</center>\n",
    "\n",
    "Der Datensatz kann auch extern angeschaut werden. Dafür klicken Sie einfach in der linke Spalte (je nach Jupyter Version) auf den Ordner `Data` und dannach auf die Datei `mnist_train.csv`. Die erste Spalte des jeweiligen Datensatzes enthält die Labels. In jeder Reihe ist ein Bild, zudem ist jeder Spalte (bis auf der ersten) ein Pixeln zugeordnet.\n",
    "\n",
    "Mit `np.genfromtxt()` lassen sich aus .txt-Datein Arrays in Python erstellen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.genfromtxt('Data/mnist_train.csv', delimiter=',', skip_header =False) #genfromtxt liest .txt Datein, mit delimiter =\",\" können auch .csv (comma seperated values) Datein einglesen werden  \n",
    "test_data=np.genfromtxt('Data/mnist_test.csv', delimiter=',', skip_header =False) # hier lesen wir die Test Daten ein\n",
    "\n",
    "# Nach dem wir die Daten einglesen haben, trennen wir die Daten noch in Bilder und Labels\n",
    "# Des Weiteren konvertieren wir Labels von Float zu Integer mit .astype(int)\n",
    "train_labels=train_data[:,0].astype(int) \n",
    "train_images = train_data[:,1:]\n",
    "\n",
    "test_labels=test_data[:,0].astype(int)\n",
    "test_images = test_data[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir haben jetzt den Trainingsdatensatz eingelesen. Sie können mit `train_images.shape` sehen, dass die Variable `train_images` eine 60000 x 784 Matrix ist, also 60000 Reihen und 784 Spalten hat. Jede Reihe ist ein Bild und jede Spalte ist ein Pixel. Die orginalen Bilder waren 28 x 28 Pixel groß. Im nächsten Schritt benutzen wir die *Funktion* `one_hot()` um die Labels richtig zu codieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets=one_hot(train_labels)\n",
    "test_targets = one_hot(test_labels)\n",
    "\n",
    "train_targets[:5,:] # die Labels der ersten fünf Bilder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als letzten Schritt müssen wir noch den `min_max` scaler benutzen, um die Pixelwerte zwichen Nulls und Eins zu skalieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = min_max(train_images)\n",
    "test_images = min_max(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In der folgenden Zelle können Sie ein Beispielbild sehen. Die Funktion `.reshape([28,28])` transformiert das Bild in seine ursprüngliche Version zurück. So kann man das Bild auch wiklich erkennen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(train_images[0].reshape([28, 28]), cmap=\"gray\")\n",
    "print(\"Correct Label: %s\" % train_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Das Model\n",
    "\n",
    "\n",
    "**Weights Initalisieren** <br>\n",
    "Als Erstes müssen die Weight und Bias Matrizen/Vektoren mit den richtigen Dimensionen erstellt werden. Dafür wird eine Funktion geschrieben. In ihr muss man die Größe der jeweiligen Layer (`input_size`, `hidden_size`, `output_size`) festlegen. Mit diesen können Sie die Weightmatrizen erstellen. Während die `b`- Vektoren mit Nullen gefüllt werden können, müssen die Weightmatrizen mit zufälligen kleinen Zahl initialisiert werden.\n",
    "\n",
    "Die Funktion `np.random.randn(number_of_rows, number_of_columns)` setzt zufällige Werte zwischen minus Eins und plus Eins in die Matrix ein. Zusätzlich wird der Term`* np.sqrt(2/layer_size)` hinzugefügt. Dieser verkleinert die Werte noch weiter und soll damit ein besseres Trainieren des Netzwerkes garantieren. \n",
    "\n",
    "Im folgenden Code sollen Sie die richtigen Matrixgrößen (Anzahl der Reihen/Spalten) eintragen. Die Inputmatrix `X` hat 60000 Reihen und 784 Spalten und bei einer Matrixmultiplikation müssen die Anzahl der Spalten der ersten Matrix mit der Anzahl der Reihen der zweiten Matrix übereinstimmen.\n",
    "Denkt Sie auch daran, dass wir das Transpose der Weightmatrix benutzen werden.\n",
    "\n",
    "Den Bias `b` wird mit Nullen initialisiert. `np.zeros(500)` würde eine Vektor der Länge 500 mit Nullen füllen.\n",
    "Auch brauchen wir nicht die tatsächlien Zahlen, sondern nur die Namen der Inputvariablen nennen,  also nicht 784 sondern `input_size`.\n",
    "Die  tatsächlichen Werte können dann bei der eigentlichen Benutzung der Funktion definiert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion zur intialisierung der Weights\n",
    "def init_weights(input_size, hidden_size, output_size):\n",
    "    # Zwei leere Listen mit Länge zwei\n",
    "    b = [0] * 2\n",
    "    W = [0] * 2\n",
    "\n",
    "    # hier werden die Weights W mit kleinen, zufälligen Gewichten initialisiert\n",
    "    \n",
    "    W[0] = np.random.randn(        ,         ) * np.sqrt(2 / input_size)  # HIER MÜSSEN DIE RICHTIGEN GRÖßEN ANGEGBEN WERDEN\n",
    "    W[1] = np.random.randn(        ,         ) * np.sqrt(2 / hidden_size) # HIER MÜSSEN DIE RICHTIGEN GRÖßEN ANGEGBEN WERDEN\n",
    "\n",
    "    # der bias kann 0 sein\n",
    "    b[0] = np.zeros(       ) # HIER MÜSSEN DIE RICHTIGEN GRÖßEN ANGEGBEN WERDEN\n",
    "    b[1] = np.zeros(       ) # HIER MÜSSEN DIE RICHTIGEN GRÖßEN ANGEGBEN WERDEN\n",
    "\n",
    "\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Weights können jetzt initialisiert werden: \n",
    "\n",
    "Die Inputgröße ist vorgegeben, da 784 Pixel in jedem Bild sind. Auch die Outputgröße ist vorgegeben, da das Netzweerk zwischen 10 verschiedene Ziffern unterscheiden muss.\n",
    "Der einzige Wert, den Sie selber bestimmen können, ist die Größe der Hidden-Layer. Es gibt keine einfache Regel für die Größe, aber wenn sie zu klein ist, kann das die Genauigkeit des Neuronalen Netzwerkes verringern. Wenn sie zu groß ist, trainiert das Netzwerk zu langsam und wird eventuell instabil. Auch das kann die Genauigkeit beeinflussen. Probieren Sie am Anfang Werte wie 100 oder 200 aus. \n",
    "\n",
    "Grundsätzliche werden Zahlen wie die Größe der Hidden-Layer oder die Lernrate, die Sie frei wählen können, als **Hyperparameter** bezeichnet. \n",
    "\n",
    "Mit dieser Funktion können jetzt die Weight initialisiert werden. Die Funktion gibt Ihnen zwei Listen `W` und `b` aus. Die beiden Listen enhalten jeweils zwei Matrizen/Vektoren, einmal für die erste und einmal für die zweite Transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W, b = init_weights(input_size=784, hidden_size=200,output_size= 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenn Sie die Weights richtig initialisiert haben, sollte `W[0].shape` `(200,784)` sein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Forward Pass** <br>\n",
    "\n",
    "Nachdem die Weights erstellt worden sind, können Sie den Forwardpass durch das Netzwerk durchführen. Die Bilder werden durch das Netzwerk geschickt, welches am Ende diese Bilder klassifiziert.\n",
    "Die Funktion `forward_pass()` macht genau das. Ihr Input ist:\n",
    "* `W`: die Liste der Weight Matrizen\n",
    "* `b`: die Liste der Bias Vektoren\n",
    "* `X`: die Inputmatrix der Bilder \n",
    "\n",
    "\n",
    "Als Erstes wird $z_1 = xW_1^T+b_1$ berechnet. <br>\n",
    "Dannach $a_1= sigmoid(z_1)$ <br>\n",
    "<br>\n",
    "Somit sind die Activations für die Hiddenlayer berechnet. Um die Klassifzierung machen zu können, müssen die Werte noch ein zweites mal transformiert werden:<br>\n",
    "$\\hat{y} = softmax(a_1W_2^T+b_2)$\n",
    "\n",
    "Die Funktion gibt am Ende drei Variablen aus:\n",
    "* `z_1`: die Werte der ersten linearen Transformation\n",
    "* `a_1`: die Werte nach der ersten Aktivierungsfunktion\n",
    "* `y_hat`: die Werte der Outputlayer \n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "*Denkt daran, dass die Weights der ersten Layer nicht in `W[1]`, sondern in `W[0]` gespeichert sind, da die Indizierung bei Null und nicht bei Eins anfängt.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(W, b, X):\n",
    "    \n",
    "    z_1 = # CODE UM Z_1 AUZURECHNEN\n",
    "    a_1 = # CODE UM A_1 AUSZURECHNEN\n",
    "    z_2 = # CODE UM Z_2 AUSZURECHNEN\n",
    "    y_hat = #CODE UM Y_HAT AUSZURECHNEN\n",
    "    return z_1, a_1, y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss Funktion**\n",
    "\n",
    "Nach dem Forward Pass wird der Loss berechnet. Dieser misst, wie gut oder schlecht das Model die Zahlen klassifizieren kann.\n",
    "\n",
    "Hierfür werden nur die Werte der Output Layer (`y_hat`) und die tatsächlichen Werte (`y`) gebraucht. Die wahren Werte wurden schon vorher mit der Funktion `one_hot` berechnet und `train_targets` genannt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(y_hat, y):\n",
    "    return -np.sum(np.log(y_hat) * y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sie können jetzt die ersten drei Funktion zusammen benutzen, um Ihre erste Klassifizierung durchzuführen.\n",
    "Denkt daran, dass die `train_images` der Input für das Netzwerk sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234) # Ein Seed wird gesetzt, damit die Ergebnisse der zufälligen Initialisierung für alle Teilnehmer:innen gleich sind\n",
    "W, b = # CODE, UM DIE WEIGHTS ZU INITIALISIEREN\n",
    "\n",
    "# Benutzen Sie die train_images als X (Input)\n",
    "z_1, a_1, y_hat = # CODE FÜR DEN FORWARD PASS\n",
    "\n",
    "# Hier berechnen Sie den Loss\n",
    "calc_loss(           ,           )/y_hat.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Loss ist sehr schwierig zu interpretieren. \n",
    "Deswegen können wir zusätzlich die Genaugikeit (accuracy) als Maß benutzen. Die Genauigeit ist der Prozentsatz an richtig klassifizierten Bildern\n",
    "$ \\frac{n_{richtig}}{n}$.\n",
    "\n",
    "Die Genauigkeit ist ein viel einfacheres Maß um zu beurteilen, wie gut das Model funktioniert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(true_labels,predicted):\n",
    "    return (np.sum(true_labels == np.argmax(predicted, axis=1)) / predicted.shape[0]) # argmax gibt den Index des Maximalwertes wieder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(train_labels,y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Momentan hat das Netzwerk eine Genauigkeit von 9,8% Prozent. 10% Genauigkeit wären zu erwarten, wenn das Netzwerk zufällig entscheiden würde.\n",
    "Um besser als 9,7% zu werden, müssen Sie das Netzwerk trainieren und die Weights verändern. Dazu benutzen Sie die Back Propagation.\n",
    "\n",
    "**Back Propagation**\n",
    "\n",
    "Der Fehler in der Klassifizierung wird nun zurück durch das Netztwerk geschickt und mit Hilfe der Gradienten werden die Weights angepasst.\n",
    "\n",
    "* $dZ_2 = \\hat{y} - y$ \n",
    "* $dW_2 = \\frac{1}{n} \\cdot dZ_2^Ta_1$\n",
    "* $db_2 = \\frac{1}{n} \\cdot \\sum_{i=1}^n dZ_2$\n",
    "* $dZ_1 = dZ_2W_2 \\cdot a_1 \\cdot (1-a_1)$\n",
    "* $dW_1 = \\frac{1}{n} \\cdot dZ_1^TX$\n",
    "* $db_1 = \\frac{1}{n} \\cdot\\sum_{i=1}^n dZ_1$\n",
    "\n",
    "Wie gesagt: Wie die Ableitungen mathematische Zustande kommen, sprengt den Rahmen dieses Seminar. In der tatsächlichen Anwendung gibt es Libaries, welche die Gradienten automatisch berechenen können. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(x, z_1, a_1, y_hat, y):\n",
    "    n = x.shape[0] # n ist die Anzahl der Bilder\n",
    "    \n",
    "    # Gradients für die Weights der zweiten Layer\n",
    "    dz_2 =  # CODE UM dZ_2 AUZURECHNEN\n",
    "    dW_2 =  # CODE UM dW_2 AUZURECHNEN\n",
    "    db_2 = np.sum(dz_2, axis=0) / n\n",
    "    \n",
    "    # Gradients für die Weights der ersten Layer\n",
    "    dz_1 = np.multiply(np.matmul(dz_2, W[1]), np.multiply(a_1, 1 - a_1))\n",
    "    dW_1 = # CODE UM dW_1 AUZURECHNEN\n",
    "    db_1 = # CODE UM db_1 AUZURECHNEN\n",
    "\n",
    "    return [dW_1, dW_2], [db_1, db_2] # Hier werden wieder zwei Listen als Output gegeben, in jeder der Listen befinden sich jeweils die Gradienten für W_1,W_2 und b_1 und b_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Weights Updaten**\n",
    "\n",
    "Im letzten Schritt werden die Weights angepasst. Dafür werden die Weights ein kleinwenig entgegen des Gradienten verschoben. \n",
    "Wie weit genau, hängt von der Lernrate `lr` ab. Je größer diese ist, umso größer werden die Schritte.\n",
    "Ist die Lernrate zu klein dauert das Trainieren eventuell zu lange, oder das Netzwerk bleibt in einem lokalen Minimum hängen. Ist die Lernrate zu groß, springt der Loss des  Netzwerkes zu viel und dementsprechend kann kein optimale Performance garantiert werden.\n",
    "Die Lernrate gehört, wie die Größe der Hidden Layer, zu den Hyperparametern.\n",
    "<center>\n",
    "<img src=\"https://sebastianraschka.com/images/blog/2015/singlelayer_neural_networks_files/perceptron_learning_rate.png\" style=\"width: 800px;\">\n",
    "<h8><center>Source: Sebastian Raschka, https://sebastianraschka.com</center></h8>\n",
    "</center>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(W, b, grad_W, grad_b, lr=0.0001):\n",
    "    W[0] = # CODE FÜR W[0]\n",
    "    W[1] = # CODE FÜR W[1]\n",
    "    b[0] = # CODE FÜR b[0]\n",
    "    b[1] = # CODE FÜR b[1]\n",
    "\n",
    "    return W, b # die Funktion gibt die neuen Weights und Biases aus (2 Listen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "Jetzt können Sie alles zusammenfügen. Sie initalisieren erst die Weights, dann werden die Inputs durch das Netzwerk geschickt und der Loss wird berechnet.  Anschließend wird der Loss wieder zurück durch das Netzwerk geführt und die Gradienten berechnet. Die Gradienten werden benutzt die  Weights zu updaten. Für die Lernrate gebrauchen Sie 0,1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.WEIGHTS INITALISIEREN\n",
    "np.random.seed(1234) \n",
    "W, b = init_weights(input_size=784, hidden_size=200,output_size= 10) \n",
    "\n",
    "# 2.FORWARD PROPAGATION\n",
    "z_1, a_1, y_hat = forward_pass(W,b,train_images)\n",
    "\n",
    "# 3.LOSS BERECHNEN\n",
    "print(\"Loss beim ersten Durchlauf: \",calc_loss(y_hat,train_targets)/y_hat.shape[0], \"Genauigkeit beim ersten Durchlauf\", accuracy(train_labels, y_hat) )\n",
    "\n",
    "# 4. BACKPROPAGATION\n",
    "grad_W, grad_b = # CODE FÜR DIE BACKPROPAGATION\n",
    "# 5. WEIGHTS UPDATEN\n",
    "W, b = # WEIGHT UPDATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Loss und die Genauigkeit sind noch gleich. Erst wenn Sie erneut den Input durch das Netzwerk schicken, können Sie den Effekt der veränderten Weights sehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_1, a_1, y_hat = forward_pass(W,b,train_images)\n",
    "\n",
    "# jetzt berechnen Sie den Loss\n",
    "print(\"Loss beim zweiten Durchlauf: \",calc_loss(y_hat,train_targets)/y_hat.shape[0], \"Genauigkeit beim zweiten Durchlauf\", accuracy(train_labels, y_hat) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tatsächlich verringert sich der Loss, aber die Genauigkeit hat sich auch verringert. Das kann während des Trainings passieren, ist aber nicht weiter schlimm.  Der Effekt des Trainings wird oft erst nach mehreren Epochs sichtbar. Sie können den Schritt des Trainierens einfach wiederholen.\n",
    "Um dies effizienter zumachen, können Sie einfach einen `for-loop` benutzen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.WEIGHTS INITALISIEREN\n",
    "np.random.seed(1234)\n",
    "W, b = init_weights(784, 200, 10)\n",
    "\n",
    "EPOCHS= 50 # wie oft der Datensatz durch das Netzwerk geführt wird\n",
    "for i in range(EPOCHS):\n",
    "    \n",
    "    # 2. FORWARD PROPAGATON\n",
    "    z_1, a_1, y_hat = forward_pass(W, b,train_images)\n",
    "    \n",
    "    # 3. LOSS BERECHNEN\n",
    "    loss = calc_loss(y_hat, train_targets) / y_hat.shape[0]\n",
    "    acc = accuracy(train_labels, y_hat)\n",
    "    \n",
    "    print(\n",
    "        \"Training Loss: %.2f Training Accuracy: %.2f\"\n",
    "        % (loss, acc)\n",
    "    )\n",
    "    \n",
    "    # 4. BACK PROPAGATION\n",
    "    grad_W, grad_b = back_prop(train_images, z_1, a_1, y_hat, train_targets)\n",
    "    \n",
    "    # 5. WEIGHTS UPDATEN\n",
    "    W, b = update(W, b, grad_W, grad_b, lr = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sie sehen schon eine Verbesserung und kommen auf eine Genaugigkeit von 69%. Allerdings dauert das Trainieren sehr lange. Eine größere Lernrate sollte das Training schneller machen. Probieren Sie eine Lernrate von 0,3 aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "W, b = init_weights(784, 200, 10)\n",
    "loss= []\n",
    "EPOCHS= 50 # wie oft der Datensatz durch das Netzwerk geführt wird\n",
    "for i in range(EPOCHS):\n",
    "    z_1, a_1, y_hat = forward_pass(W, b,train_images)\n",
    "    loss= calc_loss(y_hat, train_targets) / y_hat.shape[0]\n",
    "    acc = accuracy(train_labels, y_hat)\n",
    "    print(\n",
    "        \"Training Loss: %.2f Training Accuracy: %.2f\"\n",
    "        % (loss, acc)\n",
    "    )\n",
    "    \n",
    "    # wir machen die backpropagation\n",
    "    grad_W, grad_b = back_prop(train_images, z_1, a_1, y_hat, train_targets)\n",
    "    # und mit den Gradienten updaten wir die Weights\n",
    "    W, b = update(W, b, grad_W, grad_b, lr = )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit einer Lernrate von 0.3 erreichen Sie nach 50 Epochs eine Genauigkeit von 81%. Sie können das Netzwerk noch weiter verbessern indem Sie  noch  länger trainieren. Was müssten Sie am Code ändern, damit für 25 weitere Epochs trainiert wird **ohne, dass das Netzwerk ganz neu trainiert werden muss**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "W, b = init_weights(784, 200, 10)\n",
    "loss= []\n",
    "EPOCHS= 25 # wie oft der Datensatz durch das Netzwerk geführt wird\n",
    "for i in range(EPOCHS):\n",
    "    z_1, a_1, y_hat = forward_pass(W, b,train_images)\n",
    "    loss= calc_loss(y_hat, train_targets) / y_hat.shape[0]\n",
    "    acc = accuracy(train_labels, y_hat)\n",
    "    print(\n",
    "        \"Training Loss: %.2f Training Accuracy: %.2f\"\n",
    "        % (loss, acc)\n",
    "    )\n",
    "    \n",
    "    # wir machen die backpropagation\n",
    "    grad_W, grad_b = back_prop(train_images, z_1, a_1, y_hat, train_targets)\n",
    "    # und mit den Gradienten updaten wir die Weights\n",
    "    W, b = update(W, b, grad_W, grad_b, lr = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durch weiteres Training haben Sie das Netzwerk um 3% Genauigkeit verbessern können. Grundsätzlich haben Sie noch die Möglichkeit mit der Größe der Hidden Layer etwas zu verändern. \n",
    "\n",
    "Bevor wir das machen, können Sie erst einmal schauen, wie gut unser Modell funktioniert für Bilder, die es noch nicht gesehen hat. Die Rede ist von dem Testdatendatz. Dazu speisen sie den Testdatensatz in das trainierte Netzwerk ein:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, test_y_hat = forward_pass(W, b, test_images) # durch das _,_,test_y_hat werden z_1 und a_1 nicht mit ausgegeben, da wir diese nicht brauchen\n",
    "accuracy(test_labels, test_y_hat)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Genauigkeit auf dem Testdatensatz beträgt auch 85%. Man kann sagen, dass 85% der Bilder richtig erkannt wurden.\n",
    "Es ist unüblich, dass Netzwerke besser für den Testdatensatz funktioniert. Dies ist ein Hinweis darauf, dass Sie das Modell nicht lange genug trainiert haben.\n",
    "\n",
    "Als nächstes können Sie sich anschauen, bei welchen Bilder das Netzwerk am meisten Probleme hat.\n",
    "Der Code in der nächsten Zelle, sortiert die fasch identifizierten Bilder nach ihre Wahrscheinlichkeit (Dieser Code ist nicht unbedingt leicht zu verstehen, ist aber für das Verständnis von Neuronalen Netzwerken auch nicht essentiell):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "falsche_klassifizierung = np.where(test_labels != np.argmax(test_y_hat, axis=1))[0]# welche Bilder wurde falsch klassifiziert\n",
    "len(falsche_klassifizierung) # soviele Bilder wurden falsch klassifiziert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hier werden de Wahrscheinlichkeiten gesammelt die das Model dem Bild zugeorndet hat in der richtige Kategorie zu sein\n",
    "probs = [] \n",
    "for image in falsche_klassifizierung:\n",
    "    probs.append(test_y_hat[image,test_labels[image]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wir sortieren die Bilder Anhand der Wahrscheinlichkeiten, je kleiner die Wahrscheinlichkeit desto sicherer war das Model das das Bild nicht in der richtige Katehorie ist\n",
    "falsche_klassifizierung=falsche_klassifizierung[np.argsort(probs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so sehen 10 Bilder aus die falsch klassifiziert werden\n",
    "for i in range(10):\n",
    "    plt.imshow(test_images[falsche_klassifizierung[i]].reshape([28, 28]), cmap=\"gray\")\n",
    "    plt.show()\n",
    "    print(\n",
    "        \"Predicted Label: %s, Correct Label %s\"\n",
    "        % (\n",
    "            np.argmax(test_y_hat, axis=1)[falsche_klassifizierung[i]],\n",
    "            test_labels[falsche_klassifizierung[i]],\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bei manchen Bilder können Sie klar sehen, warum diese falsch kategorisiert wurden. Bei anderen jedoch ist es für ein menschliches Auge ganz einfach die richtige Ziffer zuerkennen, trotzdem hat das Netzwerk damit Probleme.\n",
    "\n",
    "Das war es für den ersten Tag. Wenn Sie wollen, könnem Sie noch mit den Hyperparameter spielen um Ihr Netzwerk zu verbessern."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
