{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science - Toxikologische Vorhersagen\n",
    "\n",
    "\n",
    "---\n",
    "### Lernziele\n",
    "\n",
    "- Sie können eine SVM trainieren.\n",
    "- Sie verstehen den Grund für das Skalieren von Variablen\n",
    "- Sie können ein Random Forest Model trainieren.\n",
    "- Sie verstehen Y-Scrambeling und den erkennen die Notwendigkeit für Train/Test Splits.\n",
    "- Sie können Daten in Train und Testset teilen.\n",
    "---\n",
    "\n",
    "Heute werden Sie sich mit den ein paar Grundlagen von Data Science und Machine Learning beschäftigen. Diese werden später auch für das Trainieren von Neuralen Netzwerken relevant sein. \n",
    "Als Beispiel werden Modelle erstellt, die die toxikologische Bedenklichkeit von Molekülen vorhersagt.\n",
    "In dem konkreten Beispiel geht es um die Messung des **mitochondrialen Membranpotentials** (MMP). Dieses wird als ein Indikator für die allgemeine Zellgesundheit benutzt \\[1\\].\n",
    "\n",
    "\n",
    "<img src=\"https://www.researchgate.net/publication/326685180/figure/fig4/AS:654070805172233@1532954040230/assay-of-a549-cells-mitochondrial-membrane-potential-with-Jc-1-staining-method-Notes.png\" width=400 height=200 />\n",
    "\n",
    "<center>Beispiel eines MMP Assays.<br> <i> Source: Liao et al.[2] licensed under CC-BY-NC</i></center>\n",
    "\n",
    "Die Daten wurden aus dem Datensatz der Tox21-Challenge entnommen. In dieser Challenge ging es darum, die toxikologischen Eigenschaften von Molekülen vorherzusagen. Hierzu wurden die Messungen auf insgesamt zwölf Assays bereitgestellt. Für jetzt fokussieren wir uns nur auf einen Assay und benutzen auch nicht den kompletten Datensatz, sondern nur ca. 2000 Moleküle. \n",
    "\n",
    "Zuvor werden wir anhand eines Beispiels den Effekt von Variablen Scaling betrachten.\n",
    "\n",
    "---\n",
    "**Referenzen:**\n",
    "<br>\n",
    "<br>\n",
    "\\[1\\] Sakamuru, S., Attene-Ramos, M. S., & Xia, M. (2016). Mitochondrial membrane potential assay. In High-throughput screening assays in toxicology (pp. 17-22). Humana Press, New York, NY. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5375165/\n",
    "<br>\n",
    "<br>\n",
    "\\[2\\] Liao, C., Xu, D., Liu, X., Fang, Y., Yi, J., Li, X., & Guo, B. (2018). Iridium (III) complex-loaded liposomes as a drug delivery system for lung cancer through mitochondrial dysfunction. International journal of nanomedicine, 13, 4417."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from rdkit.Chem import AllChem as Chem\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from rdkit.Chem.Lipinski import * \n",
    "from rdkit.Chem.rdMolDescriptors import CalcExactMolWt, CalcTPSA\n",
    "from rdkit.Chem.Crippen import MolLogP, MolMR\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import seaborn as  sns\n",
    "#import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "#import svgutils\n",
    "%matplotlib inline\n",
    "%run ../utils/utils.py #lädt vorgeschriebene Funktionen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling\n",
    "\n",
    "Das Scalen/Skalieren von Variablen kann essenziell für den Erfolg von Machine Learning Modellen sein. Das skalieren von Variablen bedeutet, dass wir die Größenordnung der Werte von einer Variable verändern. Genauer gesagt wollen wir, dass alle Variablen dieselbe Skala benutzen. \n",
    "\n",
    "Zum Beispiel der Wert eines Hauses in € wird oft zwischen 50.000 und mehreren Millionen liegen. Die Fläche des Hauses aber liegt wahrscheinlich nur zwischen 10 und mehreren hundert Quadratmetern. Die Werte, die der Preis annehmen kann, sind viel größer als die der Fläche. \n",
    "\n",
    "Für manche Algorithmen, die auf Distanzen oder Gradienten setzen, kann das ein Problem sein, da die Skala der Variablen für diesen Variablen einen direkten Einfluss auf die Wichtigkeit der Variablen hat.\n",
    "Variablen mit größeren Skalen kommt mehr Wichtigkeit zuteil, obwohl die Skala arbiträr ist. Man könnt den Preis eines Hauses auch in 10,000 € angeben (50,000 wird zu 50). Die Skala ändert sich, nicht aber die eigentliche Variable.\n",
    "\n",
    "Um diesen Effekt zu verhindern, skalieren wir Variablen auf einheitliche Werte.\n",
    "Zum Beispiel `MinMax Scaling` skaliert alle Werte zwischen `0` und `1`. \n",
    "\n",
    "Im folgenden Beispiel werden Sie sehen, welchen Einfluss das Skaling auf eine Support Vector Machine haben kann."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zunächst laden Sie die Daten mit Python ein. \n",
    "\n",
    "___\n",
    "**Dateitypen:**\n",
    "<br> <br>\n",
    "Es gibt viele verschiedene Formate in denen Daten gespeichert werden. Das wahrscheinlich am meisten benutzte ist das `comma seperated value` Format. Dieses ist an dem Kürzel `.csv` am Ende einer Datei zu erkennen.  <div style=\"float: right;\"><img  src=\"https://images.freeimages.com/images/large-previews/7f6/tab-key-1243535.jpg\" width=150 height=100 />freeimages.com: T. Al Nakib</div> Wie der Name erahnen lässt werden die einzelnen Werte durch ein Komma getrennt. Werte die in die selbe Reihe gehören werden in die selbe Reihe geschrieben und durch das Komma separiert. Andere Dateiformate, die oft benutzt werden, sind  Textdateien `.txt`. Hier können Sie nicht direkt aus der Endung schließen wie die Struktur aufgebaut ist.\n",
    "Oft wird aber das System \"Werte in einer Zeile gehören in eine Zeile\" beibehalten. Nur das Zeichen, das einzelne Werte trennt, kann sich unterscheiden. Ein oft benutzter `seperator` ist der Tab. Der Tab-`seperator` wird auch gerne im `.smi` Format benutzt. Dieses Format werden Sie öfter sehen, wenn Sie mit SMILES arbeiten. Falls Sie sich mal nicht sicher sind welcher `seperator` benutzt wird können Sie Datei in einem einfachen Texteditor öffnen. In Windows zum Beispiel \"Notepad\". Hier sollten Sie dann erkennen wie Werte voneinander getrennt werden.\n",
    "___\n",
    "\n",
    "In diesem Fall sind die Daten in einem `.tab` Format gespeichert. Also die Werte werden mit einem Tab separiert sein. Sie können trotzdem die Funktion `pd.read_csv()` benutzen, auch wenn die Datei keine `.csv`-Datei ist. Dann müssen Sie aber zusätzlich den `seperator` festlegen `sep= \"\\t\"`. Das Symbol `\"\\t\"` wird dann als Tab erkannt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ab\")\n",
    "print(\"a\\tb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie Sie vielleicht bereits an einigen der geladen Libraries gesehen haben, werden wir heute ein paar Ihnen noch unbekannte Funktionen benutzen. \n",
    "\n",
    "- `pandas` enthält nützliche Funktionen zum Verarbeiten großer Datenmengen und kann so ziemlich alles, was auch Excel kann (manches auch besser).  `pandas` erstellt sogenannte `DataFrames`. DataFrames ähneln 2D `arrays` in `numpy` und speichern Daten in Zeilen und Spalten. Der Unterschied ist, dass wir verschiedene Datentypen in einem `DataFrame` speichern können. Zum Beispiel `floats` und `strings` in zwei verschiedenen Spalten. Wir können auch Spalten und Zeilen Namen zuordnen, um eine bessere Übersicht über unsere Daten zu haben.\n",
    "\n",
    "- `sklearn` kennen Sie schon vom ROC-AUC (Woche 04). `sklearn` bringt viele Funktionen mit sich, die wichtig sind, für das Aufbereiten von Daten. Darüber hinaus gibt es verschiedene Machine Learning Algorithmen in `sklearn` u.a. auch der Random Forest.\n",
    "\n",
    "In der folgenden Zelle werden die Daten für ein einfaches Beispiel eingelesen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_beispiel = pd.read_csv(\"../data/toxicity/toy_example.tab\", sep = \"\\t\")\n",
    "\n",
    "print(\"Type:\", type(toy_beispiel))\n",
    "print(\"Shape:\",toy_beispiel.shape)\n",
    "toy_beispiel.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die mit `pandas` eingelesenen Daten werden zuerst in einem `DataFrame` gespeichert. Dabei handelt es sich um eine Tabelle mit Zeilen und Spalten. Mit `.head()` können Sie sich die ersten 5 Zeilen einen `DataFrame` anzeigen lassen. Die Daten enthalten drei Spalten: `x1` `x2` und `y`. Insgesamt enthält die Datei 150 Einträge. Also ingesamt 150 Messpunkt. \n",
    "\n",
    "`y` gibt die fiktive Zugehörigkeit der einzelnen Datenpunkte zu einer von zwei Klassen an.\n",
    "\n",
    "Um eine bessere Übersicht zubekommen, erstellen wir einen Graphen für die Daten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(toy_beispiel.x1, toy_beispiel.x2,\"o\")\n",
    "plt.plot( toy_beispiel.x1[toy_beispiel.y==1],  toy_beispiel.x2[toy_beispiel.y==1],\"o\")\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie Sie sehen können, liegt die eine Klasse innerhalb der anderen. Wir wollen eine SVM trainieren, um die beiden Klassen voneinander zu unterscheiden.\n",
    "\n",
    "Ihnen sollte auch auffallen, dass die Skalen der beiden Input Variablen `x1` und `x2` deutlich verschieden ist. \n",
    "\n",
    "`x1` Werte liegen ungefähr zwischen `-1` und `1`. <br>`x2` Werte befinden sich zwischen ca. `8` und `12`.\n",
    "\n",
    "Wir werden zunächst die Daten nicht skalieren, und direkt eine SVM auf diese Daten trainieren. Dafür bietet das Modul `sklearn.svm` einige Funktionalitäten. Genau bei der linearen Regression mit `sklearn` wird erst das Model erstellt und dann wird s trainiert.\n",
    "Die Funktion `SVC` (Support Vector Classification) kann für das Klassifizieren benutzt werden. Als Erstes erstellen wir eine Variable `model`. Diese enthält den Support Vector Classifier (`SVC`). Um diesen zu trainieren, nutzen wir die Funktion `.fit(x,y)` um den Klassifier an unseren Daten anzupassen. \n",
    "\n",
    "Bis jetzt haben wir den SVC nur trainiert, um seiner Vorhersagen zu erhalten, müssen wir noch einmal die Funktion `model.predict(x)` benutzen.\n",
    "\n",
    "\n",
    "**Wichtig:** Vielleicht haben Sie es schon oben gesehen, in `pandas` können wir Variablen (also Spalten) direkt aus dem `DataFrame` auswählen. Also `toy_beispiel.y` wählt die Spalte `y` aus dem DataFrame `toy_beispiel` aus. \n",
    "Wollen Sie Werte mit der klassischen Indizierung, wie bei `arrays` auswählen, müssen Sie erste ein `.iloc[]` an den Namen des DataFrames hängen. Die Indizierung funktioniert dann genau wie bei `numpy`.\n",
    "\n",
    "`toy_example.iloc[:,:2]` wählt die beiden ersten Spalten, also `x1` und `x2` aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SVC()\n",
    "model.fit(toy_beispiel.iloc[:,:2], toy_beispiel.y)\n",
    "y_pred = model.predict(toy_beispiel.iloc[:,:2])\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um die Güte der Vorhersagen zu beurteilen, können Sie wieder die Genauigkeit/Accuracy benutzen. Sie können dieselbe Funktion von letzter Woche dafür benutzen. Berechnen Sie Accuracy für die Vorhersagen dieses Modells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    return np.sum(y_true==y_pred)/len(y_true)\n",
    "\n",
    "accuracy(_____, ____)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>Lösung:</b></summary>\n",
    "\n",
    "```python\n",
    "accuracy(toy_beispiel.y, y_pred)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Accuracy ist um die `0.7`. Nicht schlecht aber das ist noch Luft nach oben. Mit einer der vorgeschriebenen Funktionen in `utils.py` können Sie auch die Decision Boundaries sichtbar machen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_svc(toy_beispiel, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sie sehen, dass die Decision Boundary sehr langgezogen ist. Das liegt daran, dass die Skala von `x2` größer ist. Das heißt der Abstand von Decision Boundary zu Datenpunkten zweier Klassen (den es zu maximieren gilt) kann man leichter für `x2` als für `x1` maximieren. Deswegen sieht man eine gute Decision Boundary für die Werte von `x2` nicht aber für `x1`. \n",
    "\n",
    "Um das zu ändern, können wir probieren die Daten zu skalieren.\n",
    "Hierfür benutzen wir den sogenannten `MinMax` Scaler bei dem alle Werte zwischen 0 und 1 skaliert werden. Wir wenden diesen Scaler auf beide Input Variablen an (`x1` `x2`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max(x):\n",
    "    return (x - np.min(x)) / (np.max(x) - np.min(x))\n",
    "\n",
    "toy_beispiel.x1 = min_max(toy_beispiel.x1)\n",
    "toy_beispiel.x2 = min_max(toy_beispiel.x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(toy_beispiel.x1, toy_beispiel.x2,\"o\")\n",
    "plt.plot( toy_beispiel.x1[toy_beispiel.y==1],  toy_beispiel.x2[toy_beispiel.y==1],\"o\")\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Plot sieht insgesamt genau gleich aus, aber die Skalen, x- und y-Achse, haben Sich verändert. \n",
    "Das heißt, das relative Verhältnis der Werte hat sich nicht geändernt.\n",
    "Können Sie jetzt ein `model_2` mit `SVC` erstellen, welches mit den skalierten Werten trainiert wird? Berechnen Sie auch die Accuracy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = ____\n",
    "model_2.fit(_____, ______)\n",
    "y_pred = model_2.predict(toy_beispiel._____)\n",
    "accuracy(______,_____)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>Lösung:</b></summary>\n",
    "\n",
    "```python\n",
    "model_2 = SVC()\n",
    "model_2.fit(toy_beispiel.iloc[:,:2], toy_beispiel.y)\n",
    "y_pred = model_2.predict(toy_beispiel.iloc[:,:2])\n",
    "accuracy(toy_beispiel.y, y_pred)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nur durch das Skalieren der Inputvariablen erhalten Sie eine Verbesserung von 0,3 in der Genauigkeit.\n",
    "Sie können auch anhand der Decision Boundary eine deutliche Verbesserung erkennen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_svc(toy_beispiel, model_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auch wenn die Daten nur für dieses Beispiel generiert wurden, zeigen sie dennoch warum das Skalieren von Inputvariablen so wichtig ist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Test Split\n",
    "\n",
    "Von einem theoretischen Beispiel, kommen wir jetzt zum einer praktischen Augfagbe. Die toxikologische Vorhersage.\n",
    "Wie am Anfang erwähnt haben wir die Assay Messung für verschiedene Moleküle bereit gestellt. Auch diese laden wir mit `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/toxicity/sr-mmp.tab\", sep = \"\\t\")\n",
    "\n",
    "print(\"Type:\", type(data))\n",
    "print(\"Shape:\",data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Daten enthalten drei Spalten: `Compound` `SMILES` und `activity`. Insgesamt enthält die Datei 2246 Moleküle.\n",
    "\n",
    "-  `Compound` enthält die Bezeichnung(ID) mit der die Werte im originalen Datensatz unterschieden werden können\n",
    "- `SMILES` enthält die SMILES strings\n",
    "- `activity` die Information, ob ein Molekül aktiv (`1`) oder inaktiv (`0`) getestet worden ist\n",
    "\n",
    "---\n",
    "Falls Sie wissen wollen wie viele aktive und damit toxische Moleküle im Datensatz enthalten sind, können sie einfach die Summe der Spalte `activity`berechnen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(data.activity) # data.activity wählt die in 'data' enthalten Spalte 'activity' aus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie Sie sehen konnten, können Sie `DataFrame` Spalten auch ohne explizite Indizes auswählen. Dafür folgt auf den Namen des `DataFrames` ein `.` mit dem jeweiligen Namen der Spalte.\n",
    "\n",
    "Für die Analyse ist die `Compounds` Spalte nicht wichtig. Darum wird sie aus `data` entfernt. Danach benennen wir noch die Spalten neu, damit alle kleingeschrieben werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.iloc[:,1:] #alle Spalten bis auf die erste (index 0) werden ausgewählt\n",
    "data.columns = [\"smiles\", \"activity\"]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sie haben zwar Daten, aber immer noch keinen Input für ihre Model. Smiles sind `str` Variablen, aber Modelle brauchen numerische Variablen als Input. Das heißt, Sie müssen erst noch sogenannte Features erstellen. Dafür können Sie die Deskriptoren von letzter Woche benutzen. In der folgenden Zellen werden die SMILES zu `mol` konvertiert und mit denen die Deskriptoren berechnet. Diesmal berechnen wir noch zusätzlich die Molar Refractivity (Maß für die Gesamtpolarisierbarkeit), die Anzahl der drehbaren Bindung und die TPSA (topologische polare Oberfläche). Anschließend kombinieren wir alle Variablen zu einem `DataFrame`.\n",
    "\n",
    "Passen Sie die `for loops` an:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Wir konvertieren all Smiles zu mols\n",
    "mols = np.array([Chem.MolFromSmiles(x) for x in ________ ]) # Welche Smiles müssen ausgewählt werden\n",
    "\n",
    "# 1) N hydrogen bond donors\n",
    "num_hb_donors = [NumHDonors(x) for x in _______ ] #durch welche Variable loopen wir durch\n",
    "\n",
    "# 2) Hydrogen bond acceptors\n",
    "num_hb_acceptors = [NumHAcceptors(x) for _____ in ______] #durch welche Variable loopen wir durch\n",
    "\n",
    "# 3) Number of rotable bonds \n",
    "num_rotablebonds = [NumRotatableBonds(x) for x in mols]\n",
    "\n",
    "# 4) Molecular Mass: CalcExactMolWt()\n",
    "mw = [ _________(___ ) for x in mols]  # Berechnen Sie die das Geicht mit CalcExactMolWt()\n",
    "\n",
    "# 5) log P: MolLogP()\n",
    "logP = [________ ___ __ __ ______] #Berechnen Sie den logP mit MolLogP()\n",
    "\n",
    "# 6) Molar refractivity \n",
    "mr = [MolMR(x) for x in mols]\n",
    "\n",
    "# 7) Polar Surface\n",
    "tpsa = [CalcTPSA(x) for x in mols]\n",
    "\n",
    "aux_data=pd.DataFrame({\n",
    "    \"hb_donors\": num_hb_donors,\n",
    "    \"hb_acceptors\": num_hb_acceptors,\n",
    "    \"rotable_bonds\": num_rotablebonds,\n",
    "    \"mw\": mw,\n",
    "    \"logP\": logP,\n",
    "    \"mr\":mr,\n",
    "    \"tpsa\":tpsa\n",
    "    })\n",
    "\n",
    "aux_data[\"activity\"] = data.activity # Wir fügen noch die activity hinzu\n",
    "aux_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>Lösung:</b></summary>\n",
    "\n",
    "```python\n",
    "# Wir konvertieren all Smiles zu mols\n",
    "mols = np.array([Chem.MolFromSmiles(x) for x in data.smiles])\n",
    "\n",
    "# 1) N hydrogen bond donors\n",
    "num_hb_donors = [NumHDonors(x) for x in mols]\n",
    "\n",
    "# 2) Hydrogen bond acceptors\n",
    "num_hb_acceptors = [NumHAcceptors(x) for x in mols]\n",
    "\n",
    "# 3) Number of rotable bonds \n",
    "num_rotablebonds = [NumRotatableBonds(x) for x in mols]\n",
    "\n",
    "# 4) Molecular Mass\n",
    "mw = [CalcExactMolWt(x) for x in mols]\n",
    "\n",
    "# 5) log P\n",
    "logP = [MolLogP(x) for x in mols]\n",
    "\n",
    "# 6) Molar refractivity \n",
    "mr = [MolMR(x) for x in mols]\n",
    "\n",
    "# 7) Polar Surface\n",
    "tpsa = [CalcTPSA(x) for x in mols]\n",
    "\n",
    "aux_data=pd.DataFrame({\n",
    "    \"hb_donors\": num_hb_donors,\n",
    "    \"hb_acceptors\": num_hb_acceptors,\n",
    "    \"rotable_bonds\": num_rotablebonds,\n",
    "    \"mw\": mw,\n",
    "    \"logP\": logP,\n",
    "    \"mr\":mr,\n",
    "    \"tpsa\":tpsa\n",
    "    })\n",
    "\n",
    "aux_data[\"activity\"] = data.activity # Wir fügen noch die activity hinzu\n",
    "aux_data\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der `DataFrame` `aux_data` hat für jeden Deskriptor eine eigene Spalte, also insgesamt sieben. Die erste Reihe enthält die Deskriptoren für das erste Molekül, die zweite für das zweite usw.\n",
    "Diesen Datensatz können Sie jetzt verwenden, um ein Modell zu erstellen. \n",
    "Allerdings müssen noch ein paar extra Schritte unternommen werden, bevor Sie wirklich ein funktionierendes Modell haben und Vorhersagen treffen können.\n",
    "\n",
    "Zunächst haben wir zwei Arten von Variablen. Variablen wie die Anzahl der drehbaren Bindungen werden als `discrete` beschrieben, da sie nur ganze Zahlen enthalten. Es gibt keine 3,5 dreh\n",
    "bare Bindungen in einem Molekül. Im Gegensatz dazu sind Variablen wie logP oder TPSA `continous`, also Variablen die Werte über den ganzen Zahlenstrahl annehmen können. Zusätzlich haben wir Variablen mit unterschiedlichen Skalen. Werte für das Gewicht sind viel größer als z.B. Werte für den logP oder die Anzahl der HB Akzeptoren. \n",
    "\n",
    "Da wir aber als nächste ein Random Forest Modell benutzen brauchen, müssen die Variablen nicht skaliert werden.\n",
    "Denn Random Forests benutzen weder Distanzen noch Gradienten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zunächst teilen wir den Datensatz in 'x' und 'y' also input und output\n",
    "x = aux_data.iloc[:,:7].values #'[:,:7]' Alle Spalten bis auf die 7 Spalte\n",
    "y = aux_data.iloc[:,7].values #'[:,7]' Nur Spalte 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die neue `x` Variable ist nun ein Array (anstatt eines `DataFrames`). Sie können jetzt ein Random Forest Modell trainieren. Ähnlich wie bei dem `SVC`, müssen Sie erst eine Variable mit dem Modell erstellen und dann mit `.fit()` das Model auf die Daten trainieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators = 1000, random_state = 42) \n",
    "#n_estimators wählt aus wie viele Trees benutzt werden sollen\n",
    "rf.fit(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um zu sehen, wie gut das Model funktioniert, müssen Sie erst noch die Vorhersagen aus dem Model für unsere Daten extrahieren. Anders als sonst benutzen wir hierfür die Funktion <br>`.predict_proba(x)[:,1]`. Wir benutzen den trainierten `rf⁣`, um `y_hat` vorherzusagen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat=rf.predict_proba(x)[:,1]\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die vorhergesagte Wahrscheinlichkeit vom ersten Molekül ist `0.014`. Wie bei der logistischen Regression heißt das, dass das Molekül laut unserem Model zu 1.4 %  auf dem MMP Assay aktiv sein wird, also wahrscheinlich nicht toxisch ist. Je höher die Wahrscheinlichkeit, desto wahrscheinlicher (laut dem Modell) ist das Molekül aktiv auf dem Assay. \n",
    "Oft wird als „Cut-Off“ Wert 0.5 gewählt. Wir würden also ab einem Wert von 0.5 davon ausgehen, dass das Modell diese Moleküle als toxisch ansieht. \n",
    "\n",
    "Um besser einschätzen zu können wie gut das Modell funktioniert, vergleichen Sie die vorhergesagten Werte `pred_y` mit den tatsächlichen Werten `y`.\n",
    "Dafür können Sie zum Beispiel die *Genauigkeit* nehmen. Die Genauigkeit ist der Prozentsatz der richtig klassifizierten Moleküle. Dafür werden alle Wahrscheinlichkeiten größer gleich 0.5 aufgerundet und Wahrscheinlichkeiten kleiner als 0.5 abgerundet. Als letzten Schritt müssen Sie nur den Prozentsatz der Moleküle berechnen, bei dem die gerundeten Werte `pred_y` gleich der Werte in `y` sind.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.round(y_hat)\n",
    "accuracy(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anschließend können wir den AUC mit der Funktion. `roc_auc_score()` berechnen. Der AUC misst wie gut unser Netzwerk im vorhersagen ist. Er vergleicht die Wahrscheinlichkeiten vom RF mit den tatsächlichen Werten für die Moleküle. Ein Wert von 1 bedeutet eine perfekte Vorhersage. Ein Wert von 0.5 bedeutet, dass das Netzwerk nicht besser als der Zufall ist.\n",
    "\n",
    "Anders als für die `accuracy` benutzen wir hier die Wahrscheinlichkeiten `y_hat` anstatt die gerundeten Werte `y_pred`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "roc_auc_score(y, ____)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>Lösung:</b></summary>\n",
    "\n",
    "```python\n",
    "roc_auc_score(y,y_hat)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sehr gut. Das Model ist fast perfekt im Vorhersagen, ob ein Molekül aktiv ist. Es trifft zu 99 % der Fälle die richtige Entscheidung. In der folgenden Zelle werden die falsch klassifizierten Moleküle ausgewählt und mit ihrer vorhergesagten Wahrscheinlichkeit angezeigt. Es ist nicht unbedingt notwendig, dass Sie diesen Code verstehen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "falsch_klassifizierte=np.where(y_pred!=y)[0]\n",
    "Draw.MolsToGridImage(mols[falsch_klassifizierte],\n",
    "                     legends=[\"Vorhergesagte Wahrscheinlichkeit:\\n\"+str(np.round(x,3)) for x in y_hat[falsch_klassifizierte]],\n",
    "                     useSVG=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Y-Scrambling\n",
    "\n",
    "Was auffällt ist, dass die Wahrscheinlichkeiten für diese Moleküle meistens relativ nah an 0.5 sind. Das heißt, für diese Moleküle war sich das Modell nicht sehr sicher. Insgesamt wurden aber nur 19 Moleküle falsch klassifiziert, wir sollten uns also nicht all zu viele Sorgen machen.\n",
    "\n",
    "Das Problem:\n",
    "\n",
    "*Hat das Model wirklich gelernt, was wichtig ist für den MMP Assay. Oder hat das RF Modell unsere Daten einfach nur auswendig gelernt?*\n",
    "\n",
    "Wir können das mit einem einfachen Test ausprobieren. Wir trainieren den RF noch einmal, bevor dies aber machen vertauschen wir die `activity` Variable. Das heißt, die echten Messungen werden gemischt und neu auf die Moleküle verteilt. Dieser Prozess wird auch **y-scrambling** genannt. \n",
    "\n",
    "Angenommen unsere echten Daten sehen wie folgt aus:\n",
    "\n",
    "smiles|Deskriptor 1| Deskriptor 2|activity\n",
    "------|------------|-------------|--------\n",
    "SMILES 1|$x_{1,1}$ |$x_{1,2}$|$y_1$\n",
    "SMILES 2|$x_{2,1}$ |$x_{2,2}$|$y_2$\n",
    "SMILES 3|$x_{3,1}$ |$x_{3,2}$|$y_3$\n",
    "SMILES 4|$x_{4,1}$ |$x_{4,2}$|$y_4$\n",
    "\n",
    "Für jeden SMILES wurden 2 Deskriptoren berechnet. Wir haben auch die Aktivität($y_1$-$y_4$) für jedes Molekül aufgezeichnet. Die Aktivität $y_1$ ist die gemessene Aktivität von SMILES 1 usw.\n",
    "\n",
    "Nach dem *Y-scrambling* sehen unsere Daten so aus:\n",
    "\n",
    "smiles|Deskriptor 1| Deskriptor 2|activity\n",
    "------|------------|-------------|--------\n",
    "SMILES 1|$x_{1,1}$ |$x_{1,2}$|$y_2$\n",
    "SMILES 2|$x_{2,1}$ |$x_{2,2}$|$y_3$\n",
    "SMILES 3|$x_{3,1}$ |$x_{3,2}$|$y_4$\n",
    "SMILES 4|$x_{4,1}$ |$x_{4,2}$|$y_1$\n",
    "\n",
    "Die $y$ Werte wurden zufällig anderen Molekülen zugeordnet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Y-Scrambling führt dazu, dass die tatsächlichen Muster, die zwischen den `x` Variablen (also unseren Deskriptoren wie logP,...) und der Output Variable `y` bestehen, verloren gehen.  Das liegt daran, dass jetzt die Beziehung nur noch zufällig ist. Wenn unser Random Forest tatsächlich Muster lernt, anstatt die Daten auswendig zu lernen, dann sollte das Model auf diesem Datensatz schlechter funktionieren.\n",
    "\n",
    "Um dies auszuprobieren brauchen Sie die library `random`. Mit der Funktion `random.shuffle()` werden die Werte von `y` zufällig neu sortiert. Sie können dann nochmal das Random Forest Modell trainieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(15) #ein Seed stell sicher das Sie alle die selbe zufälligen Daten erhalten\n",
    "y_random=np.array(y) # erst speichern wir y in einem array\n",
    "random.shuffle(y_random) # dann shuffeln wir y-random, wir müssen diese Variable nicht extra speichern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir haben gerade die Aktivitätsinformation gemischt. Wir können jetzt ein Random Forest Model trainieren. Diesmal benutzen wir aber nicht `y`, sondern `y_random`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Das Model wird neu trainiert\n",
    "rf = RandomForestClassifier(n_estimators = 1000, random_state = 42)\n",
    "rf.fit(x, ______) # Welcher y Variable brauchen Sie\n",
    "y_hat=rf.predict_proba(x)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>Lösung:</b></summary>\n",
    "\n",
    "```python\n",
    "rf = RandomForestClassifier(n_estimators = 1000, random_state = 42)\n",
    "rf.fit(x, y_random) # Welcher y Variable brauchen Sie\n",
    "y_hat=rf.predict_proba(x)[:,1]\n",
    "```\n",
    "</details>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt berechnen Sie erneut die Accuracy (achten Sie darauf, dass wir wieder `y_random` und nicht `y` für `y_true` benutzen):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.round(y_hat)\n",
    "accuracy(y_random, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tatsächlich verschlechtert sich die Genauigkeit nach dem Y-Scrambling. Aber trotzdem ist die Genauigkeit über 90 %. Das Random Forest Modell ist immer noch relativ gut im Vorhersagen der toxikologischen Bedenklichkeit, obwohl die Daten, die Sie benutzt haben, gar keinen Sinn mehr ergeben. Das Modell konnte gar nicht lernen, weil es gar nichts zu lernen gab. Also hat das RF Modell seine Genauigkeit nur durch das Auswendiglernen erreicht. \n",
    "\n",
    "Darum benutzt man **immer** einen Validierungsdatensatz. Dieses Validierungsset wird nicht im Training benutzt, und somit sieht das Model diese Moleküle bei der Bewertung zum ersten Mal. Auswendiglernen kann immer noch passieren, wird dem Model aber nicht bei dem Validierungsset helfen.  \n",
    "\n",
    "---\n",
    "Oft werden Datensätze nicht nur in Train/Validierungset geteilt, sondern in Train/Validierung/Testset.\n",
    "Die Modelle werden dann anhand des Validierungsset optimiert und nur das optimierte Model wird dann auf dem Testset getestet.\n",
    "Technisch gesehen müsste das hier verwendetet Validierungsset Testset heißen, da wir keine Modelloptimierung betreiben. Damit aber für die folgenden Wochen die Terminologie einheitlich bleibt, nennen wir es Validierungset.\n",
    "\n",
    "---\n",
    "Auch hier gibt es wieder Funktionen, die Ihnen die Arbeit abnehmen. `train_test_split` teilt die Daten in ein Test(hier Validierungsset) und ein Trainingset. Wir benutzen 80 % des Datensatzes für das Training und die restlichen für das Validierungsset. Die Moleküle werden dann zufällig zwischen den beiden Sets aufgeteilt. Danach werden die Daten wieder in `x` und `y` getrennt, diesmal aber für `train` und `val` separat.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val=train_test_split(aux_data,test_size= 0.2, train_size= 0.8, random_state=1234)\n",
    "\n",
    "train_x = train.iloc[:,:7]\n",
    "train_y = train.iloc[:,7]\n",
    "val_x = val.iloc[:,:7]\n",
    "val_y =  val.iloc[:,7]\n",
    "f\"Train Shape: {train.shape}, Val Shape: {val.shape}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Trainingssatz enthält nur noch 1796 Moleküle und das Validationset 450. \n",
    "\n",
    "Wir trainieren zunächst den Random Forest nur mit den Trainingsdaten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators = 1000, random_state = 42)\n",
    "rf.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Vorhersagen machen wir aber nur für die Moleküle im Validerungsset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat=rf.predict_proba(val_x)[:,1]\n",
    "y_pred = np.round(y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir können hetzt auch die Genauigkeit berechnen. Welche Variable brauchen wir jetzt für `y_true`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(___, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>Lösung:</b></summary>\n",
    "\n",
    "```python\n",
    "accuracy(y_val, y_pred)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Genauigkeit hat sich um einiges verschlechtert, ist aber immer noch gut. Allerdings können wir uns diesmal sicher sein, dass die Performance nicht durch Auswendiglernen zustande kam, da das Model vorher nie diese Moleküle gesehen hat. Wenn wir jetzt den Y-Scrambling Test anwenden, sollte die Performance drastisch schlechter werden. \n",
    "Wir ersetzen die `aux_data.activity` mit den vorhin erstellten `y_random`. Das sind die gemischten `y` Werte und wiederholen die Analyse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_random' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-994689704e2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maux_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_random\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'y_random' is not defined"
     ]
    }
   ],
   "source": [
    "aux_data.activity = y_random "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Danach teilen wir wieder die Daten in Trainings- und Validierungsset ein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_random, val_random=train_test_split(aux_data,test_size= 0.2, train_size= 0.8, random_state=1234)\n",
    "train_x_random = train_random.iloc[:,:7]\n",
    "train_y_random = train_random.iloc[:,7]\n",
    "val_x_random = val_random.iloc[:,:7]\n",
    "val_y_random =  val_random.iloc[:,7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir wiederholen wieder das trainieren mit den randomisierten Daten. Danach lassen wir das Model vorhersagen für das Validierungsset machen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model\n",
    "rf = RandomForestClassifier(n_estimators = 1000, random_state = 42)\n",
    "rf.fit(train_x_random, train_y_random)\n",
    "y_hat=rf.predict_proba(val_x_random)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als letztes berechnen wir noch die Genauigkeit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.round(y_hat)\n",
    "accuracy(val_y_random, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit dem Benutzen eines Validierungsset fällt die Genauigkeit des Modells mit den Y-scrambled Daten drastisch auf ungefähr 50 %. Es ist nicht besser als ein Model, das einfach raten würde.\n",
    "Erst durch das Benutzen eines Validierungsset konnten wir zeigen, dass das Modell etwas über das Auswendiglernen hinaus gelernt hat.\n",
    "\n",
    "Vor allem ging es darum zu zeigen, wie wichtig es ist, ein Model nicht auf dem Trainingsset zu bewerten. <br><br>\n",
    "Das Y-Scrambeling wird nur selten in der Praxis benutzt. Aber die OECD fordert zum Beispiel, einen Y-Scrambling Test für das Validieren von Quantitive Structure-activity Relationship (QSAR) Models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance\n",
    "\n",
    "Als letzten Schritt schauen wir uns noch einmal die Feature Importance an. Die Feature Importance beurteilt, wie wichtig die einzelnen Variablen zur Entscheidung sind. Abhängig von welchem Machine Learning Algorithmus man benutzt kann man dies relativ leicht einsehen.<br> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(train_x, train_y)\n",
    "aux_data.activity = data.activity\n",
    "\n",
    "train, val=train_test_split(aux_data,test_size= 0.2, train_size= 0.8, random_state=1234)\n",
    "train_x = train.iloc[:,:7]\n",
    "train_y = train.iloc[:,7]\n",
    "val_x = val.iloc[:,:7]\n",
    "val_y =  val.iloc[:,7]\n",
    "\n",
    "\n",
    "# Train Model\n",
    "rf = RandomForestClassifier(n_estimators = 1000, random_state = 42)\n",
    "rf.fit(train_x, train_y)\n",
    "y_hat=rf.predict_proba(val_x)[:,1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In der nächsten Zelle trainieren wir zuerst noch einmal das Random Forest Modell (ohne y scrambling) und lassen uns dann in den letzten beiden Zeilen die Feature Importance anzeigen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_importances = pd.Series(rf.feature_importances_, index=aux_data.columns.values[:-1])\n",
    "feat_importances.nlargest(20).nsmallest(20).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es wird deutlich ersichtlich, dass der LogP der wichtigste Parameter für die Bestimmung der Toxizität ist, während die Anzahl an H-Brücken-Donoren und -Akzeptoren weniger relevant ist. <br><br> Da wir für Menschen verständliche Deskriptoren als Features gewählt haben, können wir uns auch gut anschauen welchen Einfluss ein einzelner Parameter auf die Aktivität hat (das hätten wir auch tatsächlich schon vor dem Trainieren machen können). <br><br> Zuerst lassen wir uns die Aktivitätsverteilung in Abhängigkeit vom LogP anzeigen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.kdeplot(aux_data.logP[aux_data.activity==1], color=\"red\")\n",
    "sns.kdeplot(aux_data.logP[aux_data.activity==0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier sehen wir einen deutlichen Trend. Bei höherem LogP ist das Molekül wahrscheinlicher aktiv. <br> <br>Probieren sie das einmal selbst mit den anderen Deskriptoren aus (`hb_donors`, `hb_acceptors`, `rotable_bonds`, `mw`, `mr`, `tpsa`). Welche Deskriptoren zeigen außer dem LogP noch unterschiedlich verteilte Aktivitäten?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Übungsaufgabe \n",
    "\n",
    "Sie haben ja bereits Fingerprints als Molekülrepresentationen kennengelernt. Da diese einfach zu berechnen sind und immer eine fest definierte Länge haben, sind sie gut als Features für Machine Learning geeignet. Allerdings sind Fingerprints weniger leicht für Menschen zu interpretieren.\n",
    "\n",
    "Ihre Aufgabe wird es sein, nochmals ein Random Forest Modell zu trainieren. Diesmal werden Sie die Fingerprints als Input nehmen.\n",
    "\n",
    "<br><br>Für Sie wurde die `get_fingerprints()` Funktion schon vorgeschrieben. Mit der lassen sich Fingerprints aus den SMILES berechnen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fps = get_fingerprints(data)\n",
    "fps[\"activity\"] = data.activity\n",
    "fps.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fps` enthält insgesamt 2049 Spalten. 2048 davon sind die jeweiligen Bits des Fingerprints. Die letzte Spalte enthält die `activity`.\n",
    "\n",
    "Teilen Sie zunächst den Datensatz in `train` und `test`. 80 % der Daten sollen in das Trainingsset und 20 % in das Validierungsset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val=train_test_split(_____,test_size= ___ , train_size= ______, random_state=1234)\n",
    "\n",
    "train_x = __________\n",
    "train_y = __________\n",
    "val_x = __________\n",
    "val_y = __________ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nachdem Sie die Daten geteilt haben, trainieren Sie einen Random Forest Classifier mithilfe des Trainingsdatensatzes.\n",
    "Mit dem trainierten Modell klassifizieren Sie dann die Moleküle im Test Datensatz `val`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators = 1000, random_state = 42)\n",
    "rf.fit(_____.values, _____.values)\n",
    "y_hat=rf.predict_proba(______)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Berrechnen Sie den ROC-AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(___,____)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir können uns auch wieder die Feature Importance anschauen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_importances = pd.Series(rf.feature_importances_, index=range(2048))\n",
    "feat_importances.nlargest(20).nsmallest(20).plot(kind='barh', title = \"Importance of Features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dieser Plot lässt sich leider nicht mehr so gut interpretieren, auch wenn deutlich wird, dass die obersten fünf Bits wichtig für die Aktivität sind. Leider kann man die Bits nicht so gut darstellen, aber es lassen sich die Fragmente, die jedem Bit zugeordnet sind mit RDKit zeigen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "most_important_bits = feat_importances.nlargest(20).index.values\n",
    "print(\"Die 20 wichtigsten bits:\", most_important_bits)\n",
    "mol_ll = []\n",
    "bi_ll = []\n",
    "\n",
    "\n",
    "for i in range(20):\n",
    "    bit = most_important_bits[i]\n",
    "    for x in data.smiles:\n",
    "        bi ={}\n",
    "        mol = Chem.MolFromSmiles(x)\n",
    "        fp = Chem.rdMolDescriptors.GetMorganFingerprintAsBitVect(mol, radius=2, bitInfo=bi)\n",
    "        if np.sum(np.array(list(bi))==bit)>0:\n",
    "            mol_ll.append(mol)\n",
    "            bi_ll.append(bi)\n",
    "            break\n",
    "        \n",
    "prints=[(mol_ll[i],most_important_bits[i], bi_ll[i]) for i in range(20)]\n",
    "\n",
    "Draw.DrawMorganBits(prints, useSVG=True, molsPerRow=3, legends= [str(most_important_bits[i]) for i in range(20)], subImgSize= [300,300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das wichtigste Bit ist für unsere Daten eine phenolische Hydroxygruppe (aromatische Atome sind gelb hervorgehoben, das zentrale Atom ist blau). Auch sonst sind viele aromatische Fragmente in den wichtigsten Bits vertreten."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
