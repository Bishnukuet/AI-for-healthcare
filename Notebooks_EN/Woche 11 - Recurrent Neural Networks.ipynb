{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c347b95a",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "Recurrent Neural Networks (RNNs) are another special form of neural networks. \n",
    "RNNs are mainly used for sequences that are arranged in a fixed order. In these cases, the order of the individual elements of the sequence is often crucial for the interpretation of the whole sequence.\n",
    "\n",
    "Languages as a classical example lend themselves immediately. This is because the ordering of a sentence influences the interpretation of the individual words. \n",
    "\n",
    "Example:\n",
    "\n",
    "> I am not a fan of this movie.\n",
    "\n",
    "\n",
    "The word \"*fan*\" has a positive connotation. But the \"*not*\" before the word, turns the interpretation around. That is, the word \"*fan*\" should be interpreted in the context of the whole sentence. \n",
    "However, RNNs can also be used in chemistry/pharmacy. For example, SMILES `strings` or protein sequences are suitable for RNNs. \n",
    "\n",
    " `()` have a strong influence on how individual parts of the smile can be interpreted.\n",
    "\n",
    "`CCCC`|`CC(C)C`\n",
    "------|--------\n",
    "<img src=\"Img/rnn/mol1.png\" width=\"200\"/> |<img src=\"Img/rnn/mol2.png\" width=\"200\"/> \n",
    "\n",
    "\n",
    "The general concept of an RNN is relatively simple:\n",
    "Word by word (or even character by character) a sentence (or Smiles) is passed through the network. \n",
    "The output layer is completely ignored at first, but after a word has passed through the network, the activations of the hidden layer ($h_1$) are stored.\n",
    "\n",
    "Using the example sentence \"*Hallo Welt*\" (Hello World) in the figure. $h_1$ here are the activations for the word \"Hallo\".\n",
    "\n",
    "In the context of RNNs, we also refer to the activations of the hidden layer as **Hidden State**. $h_1$ is the hidden state for the word \"*Hallo*\".\n",
    "\n",
    "Next, the second word is passed through the network. We want to calculate $h_2$, but to the activations of the word \"Welt\" we also add the activations $h_1$. So $h_2$ is a combination of the activations of \"Welt\", but also of \"Hallo\". The word \"Welt\" was interpreted together with the previous word.\n",
    "\n",
    "\n",
    "<img src=\"Img/rnn/rnn_1.svg.png\" width=\"200\"/> \n",
    "\n",
    "\n",
    "If we had a third word, $h_3$ would be calculated from the activations of the third word and $h_2$. And since $h_2$ contains the information of both the second and the first word, both words influence the interpretation of the third word.\n",
    "\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/724/1*1U8H9EZiDqfylJU7Im23Ag.gif\">\n",
    "*Source: Michael Phi - An illustrated Guide to Recurrent Neural Networks*\n",
    "\n",
    "In the GIF, you can see that the influence of the hidden state of \"*What*\" (black), the first word, decreases as we get closer to the end of the sentence. However, it still has an influence on the interpretation of the last word.\n",
    "\n",
    "The hidden state of the last part of the sentence (\"*?* \"), called $O5$ ($h_5$) in the example, is a combination of all previous hidden states and the activations of \"*?* \".\n",
    "\n",
    "<img src=\"https://ichi.pro/assets/images/max/724/1*yQzlE7JseW32VVU-xlOUvQ.png\">\n",
    "\n",
    "\n",
    "We can use this hidden state as input to another network that makes its prediction based on this last hidden state.\n",
    "\n",
    "Similar to how a CNN is used to convert an image into a vector, RNNs are used to convert sequences into vectors.\n",
    "\n",
    "\n",
    "# Data Preparation:\n",
    "\n",
    "Before we train our RNN, we need to get the data into the right format. Letters and words cannot simply be read by a neural network.\n",
    "As with the labels from the MNIST dataset (0-9), we can one-hot encode words or, in the case of Smiles, characters.\n",
    "\n",
    "Suppose we have two smiles:\n",
    "\n",
    "`smiles = [\"CCN=C=O\", \"NC(=O)CC(=O)O\"]`\n",
    "\n",
    "There are six different symbols in total:\n",
    "`C`, `N`, `=`, `O`, `(`, `)` \n",
    "\n",
    "We can represent a `C` as a vector of length 6, which has a `1` at the first position and otherwise only zeros. We can also represent an `N` as a vector, except that we shift the `1` by one position.\n",
    "\n",
    "We can do this for all symbols in the smiles:\n",
    "\n",
    "```python\n",
    "\"C\" = [1,0,0,0,0,0]\n",
    "\"N\" = [0,1,0,0,0,0]\n",
    "\"=\" = [0,0,1,0,0,0]\n",
    "\"O\" = [0,0,0,1,0,0]\n",
    "\"(\" = [0,0,0,0,1,0]\n",
    "\")\" = [0,0,0,0,0,1]\n",
    "```\n",
    "These symbols are also often called **tokens**.\n",
    "We can encode a Smiles `string` using these rules. Encoding a Smiles `string` using these rules results in a matrix:\n",
    "\n",
    "```python\n",
    "\"CCN=C=O\" -> np.array([[1,0,0,0,0,0],\n",
    "                      [1,0,0,0,0,0],\n",
    "                      [0,1,0,0,0,0],\n",
    "                      [0,0,1,0,0,0],\n",
    "                      [1,0,0,0,0,0],\n",
    "                      [0,0,1,0,0,0],\n",
    "                      [0,0,0,1,0,0]])\n",
    "```\n",
    "\n",
    "The `string` `\"CCN=C=O\"` becomes a matrix where each row is a token and each column indicates which symbols are assigned to that row.\n",
    "\n",
    "With the following code you can automate this conversion.\n",
    "Many functions are already customly written by us. But if you are still interested in exactly how these functions look like, you can find the code in the file `../utils/utils.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69935db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from rdkit.Chem import AllChem as Chem\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "%run ../utils/utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90a3c4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles = [\"CCN=C=O\",\"NC(=O)CC(=O)O\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9658c3",
   "metadata": {},
   "source": [
    "First, we need a kind of dictionary that stores all occurring symbols and assigns a number to them. This number also indicates at which position in the one-hot vector the `1` will appear. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3133058e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0, 'O': 1, '=': 2, 'N': 3, ')': 4, '(': 5}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary = create_dict(smiles)\n",
    "dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b355072c",
   "metadata": {},
   "source": [
    "The `=` is assigned to a `0` and the `N` is assigned to a `1` and so on....\n",
    "\n",
    "With the function `tokenize()` we can convert the smiles into a number string. We now represent the Smiles `string` through the numbers. \n",
    "The function just has to be told which smiles to encode and which `dictionary` to use for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b1ea9dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 3, 2, 0, 2, 1], [3, 0, 5, 2, 1, 4, 0, 0, 5, 2, 1, 4, 1]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_smiles = tokenize(smiles,dictionary)\n",
    "tokenized_smiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f40859",
   "metadata": {},
   "source": [
    "The Smiles are now represented as a simple sequence of numbers.\n",
    "However, they are still of different lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c4aca32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 13]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(x) for x in tokenized_smiles]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6d3bb5",
   "metadata": {},
   "source": [
    "The first Smiles consists of 7 symbols/tokens, the other of 13. This is a problem, because an RNN expects each sequence to be of equal length. Of course, this is not always possible, because larger molecules have more symbols than smaller ones. \n",
    "To solve the problem, we *pad* all sequences to the length of the longest smile.\n",
    "To allow for of strings *padding*  we need to add a new token to our dictionary: `\"<pad>\"`. This token will be added to each Smiles `string` until it has the same length as the longest Smiles. \n",
    "The `\"<pad>\"` is to tell the network that these symbols are no longer relevant to the actual Smiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a045f7fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_smiles_length = max([len(x) for x in tokenized_smiles])\n",
    "max_smiles_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1dcf6095",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0, 'O': 1, '=': 2, 'N': 3, ')': 4, '(': 5, '<pad>': 6}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary[\"<pad>\"] = len(dictionary)\n",
    "dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98b1e90",
   "metadata": {},
   "source": [
    "Now we have added the token `<pad>` to our dictionary. The last thing we have to do is to append this token to our first smile `tokenized_smiles[0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b44af33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 3, 2, 0, 2, 1, 6, 6, 6, 6, 6, 6]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_missing_tokens = max_smiles_length-len(tokenized_smiles[0])\n",
    "tokenized_smiles[0] += [dictionary[\"<pad>\"]] * num_missing_tokens \n",
    "tokenized_smiles[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b107b894",
   "metadata": {},
   "source": [
    "Now both Smiles have the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66bc61fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13, 13]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(x) for x in tokenized_smiles]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb806be8",
   "metadata": {},
   "source": [
    "Now that the smiles have the same length, we can convert the numbers to one-hot coded vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ceb8ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "vocabulary_length = len(dictionary)\n",
    "print(vocabulary_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecf595f",
   "metadata": {},
   "source": [
    "In total there are 7 symbols in our dictionary.\n",
    "With the function `token_to_onehot` the `tokenized_smiles` become matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "204d7a7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]]\n",
      "(2, 13, 7)\n"
     ]
    }
   ],
   "source": [
    "onehot_tokens = token_to_onehot(tokenized_smiles, vocabulary_length)\n",
    "print(onehot_tokens[0])\n",
    "      \n",
    "print(onehot_tokens.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fce5a92",
   "metadata": {},
   "source": [
    "`onehot_tokens` is a `np.array` with the dimensions `(2,13,7)` . The first dimension is the number of smiles (`2`). The second dimension is the length of the sequences (`13`). The third dimension is the number of different tokens (`7`).\n",
    "\n",
    "By itself, our data would now be ready for an RNN. But instead of taking these one-hot encoded vectors as input, we first apply an *Embedding Layer*. \n",
    "\n",
    "# Word Embeddings\n",
    "\n",
    "These one-hot encoded vectors are rarly directly used as input. Before being used as input for an RNN the one-hot encoded vectors pass trough an embedding layer. This replaces the one-hot encoded vectors with initially random numbers. To better understand what this means, let's first look at an embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4af505f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.19151945, 0.62210877, 0.43772774, 0.78535858],\n",
       "       [0.77997581, 0.27259261, 0.27646426, 0.80187218],\n",
       "       [0.95813935, 0.87593263, 0.35781727, 0.50099513],\n",
       "       [0.68346294, 0.71270203, 0.37025075, 0.56119619],\n",
       "       [0.50308317, 0.01376845, 0.77282662, 0.88264119],\n",
       "       [0.36488598, 0.61539618, 0.07538124, 0.36882401],\n",
       "       [0.9331401 , 0.65137814, 0.39720258, 0.78873014]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1234)\n",
    "embedding_layer = np.random.rand(7,4)\n",
    "embedding_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc46a7ce",
   "metadata": {},
   "source": [
    "An embedding layer consists of a single weight matrix. Initially it contains random numbers. The number of rows corresponds exactly to the number of different tokens in our dictionary. \n",
    "An embedding layer simply exchanges all vectors that have a `1` at the first position (`[1,0,0,0,0,0]`) with the first row from the `embedding_layer`. Which is `embedding_layer[0,:]= [0.19151945, 0.62210877, 0.43772774, 0.78535858]`.\n",
    "\n",
    "To achieve this, we simply need to multiply the one-hot encoded smiles by the embedding layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a2b69630",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.19151945 0.62210877 0.43772774 0.78535858]\n",
      " [0.19151945 0.62210877 0.43772774 0.78535858]\n",
      " [0.68346294 0.71270203 0.37025075 0.56119619]\n",
      " [0.95813935 0.87593263 0.35781727 0.50099513]\n",
      " [0.19151945 0.62210877 0.43772774 0.78535858]\n",
      " [0.95813935 0.87593263 0.35781727 0.50099513]\n",
      " [0.77997581 0.27259261 0.27646426 0.80187218]\n",
      " [0.9331401  0.65137814 0.39720258 0.78873014]\n",
      " [0.9331401  0.65137814 0.39720258 0.78873014]\n",
      " [0.9331401  0.65137814 0.39720258 0.78873014]\n",
      " [0.9331401  0.65137814 0.39720258 0.78873014]\n",
      " [0.9331401  0.65137814 0.39720258 0.78873014]\n",
      " [0.9331401  0.65137814 0.39720258 0.78873014]]\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = np.matmul(onehot_tokens,embedding_layer)\n",
    "print(token_embeddings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90eab5d2",
   "metadata": {},
   "source": [
    "You can see the embeddings of the first smile here above.\n",
    "Below you can see the first line of the one-hot coded smile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "59a7c88a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot_tokens[0,0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36ea152",
   "metadata": {},
   "source": [
    "If you now look at the first row (index `0`) in the weight matrix of the `embedding_layer`, you will notice that this vector has exactly the same values as the first row in the `token_embeddings` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd8dc700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.19151945, 0.62210877, 0.43772774, 0.78535858])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5663a9c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.19151945, 0.62210877, 0.43772774, 0.78535858])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings[0,0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf3c957",
   "metadata": {},
   "source": [
    "More simply explained: \n",
    "An embedding layer converts one-hot encoded vectors into vectors with random weights. \n",
    "\n",
    "*But why is this done?*\n",
    "\n",
    "One advantage is that texts or even Smiles in most cases consist of more than just 7 symbols or words. For example, if we were to encode all the words that appear in a document, these one-hot encded vectors would become very large. By \"embedding\" the vectors, we can first reduce the size of these input vectors.\n",
    "\n",
    "More importantly, the weights in the embedding layer can be learned. This means that these weights are updated during backpropagation.\n",
    "Thus, the embeddings adapt during training. This is convenient because you expect similar words to have similar embeddings after training. For example, the words truck and car are more similar in usage than car and beach. \n",
    "If car and truck have similar embeddings, i.e., are described by similar vectors, then they can be processed more easily in the context of a sentence.\n",
    "\n",
    "\n",
    "> A car drives on the road\n",
    "\n",
    "> A truck is driving on the road\n",
    "\n",
    "The two sentences describe two very similar situation and if the numerical representations are also similar, it is easier for the network to learn.\n",
    "\n",
    "\n",
    "In the case of smiles, it can be argued that the role of a nitrogen in a molecule is more like that of a carbon than a fluorine. This should also be reflected in the embeddings.\n",
    "\n",
    "\n",
    "# RNNs\n",
    "\n",
    "We have now converted the smiles to the correct format. We just need to convert the `np.array` into a tensor. Make sure that we also use the `.permute()` function. The function `.permute()` is used to swap dimensions of a tensor. This is necessary because for RNNs PyTorch expects the tensor to be arranged as follows:\n",
    "`[length of smile, number of smiles, embedding size]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a3ef6384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 2, 4])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings_tensor = torch.tensor(token_embeddings, dtype= torch.float).permute(1,0,2)\n",
    "token_embeddings_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92d9bd1",
   "metadata": {},
   "source": [
    "The tensor `token_embeddings_tensor` has the above dimensions. Each smile consists of `13` tokens, our batch consists of `2` smiles and each token is described by `4` values (from the embedding layer). \n",
    "\n",
    "Now we can define an RNN. As usual there is also a RNN class in the `torch.nn` module.\n",
    "As always we have to be careful when defining the dimensions of the RNN. The first dimension is the size of theinput vectors, that is the embedding size (`4`). The second dimension specifies the size of the hidden layer. This also defines how big the vectors of the hidden state should be.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1ea9ce53",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1234)\n",
    "rnn = nn.RNN(4,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50365c7d",
   "metadata": {},
   "source": [
    "You can now simply pass the `token_embeddings_tensor` through the `rnn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "657190f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_rnn = rnn(token_embeddings_tensor)\n",
    "len(output_rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622cfe87",
   "metadata": {},
   "source": [
    "The output of the RNN (`output_rnn`) is a list with length two.\n",
    "We first look at the first object of the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c1a1154e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-5.1854e-01,  3.2573e-01,  8.8806e-02,  3.2631e-01,  2.4860e-02,\n",
      "          -2.3541e-01,  1.3345e-01,  2.1532e-01,  4.3254e-01, -1.3007e-02],\n",
      "         [-6.0268e-01,  2.3149e-01,  1.1194e-01,  4.0218e-01, -5.5046e-02,\n",
      "          -2.2365e-01,  2.9007e-01,  2.9465e-01,  4.7601e-01,  1.2947e-01]],\n",
      "\n",
      "        [[-5.4682e-01,  3.2731e-01,  4.1438e-02,  4.7448e-01, -9.1607e-02,\n",
      "          -1.4660e-01, -9.0391e-02,  1.3491e-01,  5.3045e-01, -1.9623e-01],\n",
      "         [-5.0177e-01,  2.3414e-01,  2.3930e-02,  4.4177e-01, -6.7175e-02,\n",
      "          -1.7767e-01, -1.7725e-01,  1.1185e-01,  4.8777e-01, -1.7003e-01]],\n",
      "\n",
      "        [[-5.8859e-01,  2.7894e-01, -7.3078e-03,  6.2402e-01, -1.6086e-01,\n",
      "          -1.6881e-01,  6.8062e-02,  2.3859e-01,  6.2905e-01, -1.3412e-01],\n",
      "         [-4.5562e-01,  4.3098e-01, -1.1293e-01,  5.9627e-01, -1.3256e-01,\n",
      "          -1.3781e-01, -4.3761e-02,  2.0347e-01,  5.7765e-01, -2.3168e-01]],\n",
      "\n",
      "        [[-6.2021e-01,  1.5627e-01, -3.7084e-02,  6.8411e-01, -1.8842e-01,\n",
      "          -1.7499e-01,  9.2072e-02,  2.3215e-01,  5.8568e-01, -8.2049e-02],\n",
      "         [-6.6500e-01,  2.3953e-01, -2.4780e-02,  7.0330e-01, -1.7067e-01,\n",
      "          -1.4005e-01,  1.7969e-01,  2.4201e-01,  5.7770e-01, -8.9542e-02]],\n",
      "\n",
      "        [[-4.3771e-01,  2.6366e-01, -9.6043e-02,  5.9389e-01, -6.6826e-02,\n",
      "          -2.0501e-01, -2.0879e-01,  1.2868e-01,  4.9661e-01, -2.9018e-01],\n",
      "         [-5.5797e-01,  1.0467e-01,  2.1548e-03,  6.1629e-01, -1.2063e-01,\n",
      "          -1.1220e-01, -5.0053e-02,  2.8442e-01,  5.9606e-01, -1.9623e-01]],\n",
      "\n",
      "        [[-6.4638e-01,  2.7828e-01, -6.3520e-02,  7.3107e-01, -2.2987e-01,\n",
      "          -1.4588e-01,  1.7927e-01,  2.6040e-01,  5.9355e-01, -1.4132e-01],\n",
      "         [-5.3926e-01,  4.3162e-02,  3.7070e-02,  5.4057e-01, -2.7543e-01,\n",
      "          -1.4684e-01, -1.7876e-01,  3.7414e-01,  6.2199e-01, -1.4781e-01]],\n",
      "\n",
      "        [[-5.6906e-01,  1.0084e-01,  3.1145e-03,  6.2920e-01, -1.1253e-01,\n",
      "          -1.0630e-01, -5.0757e-02,  2.6570e-01,  5.8827e-01, -2.1079e-01],\n",
      "         [-4.3297e-01,  2.5408e-01, -1.3873e-01,  5.9163e-01, -1.7227e-01,\n",
      "          -1.8563e-01, -2.6532e-01,  1.2726e-01,  5.5274e-01, -2.9687e-01]],\n",
      "\n",
      "        [[-6.0602e-01,  1.1701e-01, -5.5710e-02,  6.9500e-01, -2.2127e-01,\n",
      "          -1.5478e-01,  6.6006e-02,  2.9652e-01,  6.1937e-01, -1.7290e-01],\n",
      "         [-4.8382e-01,  3.9314e-01, -1.0948e-01,  6.5434e-01, -1.0751e-01,\n",
      "          -1.4907e-01, -1.2962e-01,  1.5150e-01,  5.5517e-01, -3.3519e-01]],\n",
      "\n",
      "        [[-6.0005e-01,  5.8202e-02, -4.7244e-02,  6.9589e-01, -1.8518e-01,\n",
      "          -1.5159e-01,  1.2729e-02,  2.5036e-01,  5.8581e-01, -1.7585e-01],\n",
      "         [-4.9872e-01,  4.5965e-01, -1.4112e-01,  6.6642e-01, -1.1002e-01,\n",
      "          -1.2834e-01,  3.6761e-04,  1.9171e-01,  5.3159e-01, -2.9920e-01]],\n",
      "\n",
      "        [[-5.8644e-01,  7.6479e-02, -6.1255e-02,  7.0200e-01, -1.9287e-01,\n",
      "          -1.6252e-01,  2.3041e-02,  2.6815e-01,  5.9290e-01, -1.8300e-01],\n",
      "         [-6.7929e-01,  2.4838e-01, -2.2232e-02,  7.1829e-01, -1.5016e-01,\n",
      "          -1.5514e-01,  2.2310e-01,  2.4160e-01,  5.6246e-01, -1.2346e-01]],\n",
      "\n",
      "        [[-5.9419e-01,  7.5985e-02, -5.9912e-02,  7.0450e-01, -1.9616e-01,\n",
      "          -1.5404e-01,  2.0958e-02,  2.6103e-01,  5.8427e-01, -1.8560e-01],\n",
      "         [-5.6997e-01,  1.0495e-01,  1.1860e-02,  6.1999e-01, -1.2172e-01,\n",
      "          -1.1439e-01, -3.0045e-02,  2.8158e-01,  5.8887e-01, -2.0808e-01]],\n",
      "\n",
      "        [[-5.9299e-01,  7.5869e-02, -5.9066e-02,  7.0374e-01, -1.8970e-01,\n",
      "          -1.5968e-01,  2.5594e-02,  2.6184e-01,  5.8662e-01, -1.8607e-01],\n",
      "         [-5.4158e-01,  3.9146e-02,  4.3254e-02,  5.3811e-01, -2.7252e-01,\n",
      "          -1.5279e-01, -1.7263e-01,  3.7147e-01,  6.2249e-01, -1.5036e-01]],\n",
      "\n",
      "        [[-5.9404e-01,  7.6700e-02, -5.8509e-02,  7.0424e-01, -1.9298e-01,\n",
      "          -1.5735e-01,  2.5612e-02,  2.6220e-01,  5.8560e-01, -1.8665e-01],\n",
      "         [-5.3264e-01,  9.5162e-02, -6.3684e-02,  6.2591e-01, -2.5664e-01,\n",
      "          -8.0790e-02, -1.4553e-01,  2.8329e-01,  6.4714e-01, -2.1017e-01]]],\n",
      "       grad_fn=<StackBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(output_rnn[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e415d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output_rnn[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8af006",
   "metadata": {},
   "source": [
    "The output `output_rnn[0]` has the dimensions `[13, 2, 10]`. The only thing that has changed compared to the input is the last dimension. Instead of the dimension `4` it is now `10`. \n",
    "\n",
    "In fact, the first part of the RNN output contains the Hidden States of each token in the Smiles.\n",
    "\n",
    "Think back to the GIF:\n",
    "<img src=\"https://miro.medium.com/max/724/1*1U8H9EZiDqfylJU7Im23Ag.gif\">\n",
    "*Source: Michael Phi - An illustrated Guide to Recurrent Neural Networks.\n",
    "\n",
    "`output_rnn[0]` contains $O1$ to $O5$. But since our sequences have length 13, `output_rnn[0]` contains 13 hidden states.\n",
    "\n",
    "But what does `output_rnn[1]` contain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "76272748",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5940,  0.0767, -0.0585,  0.7042, -0.1930, -0.1573,  0.0256,\n",
       "           0.2622,  0.5856, -0.1866],\n",
       "         [-0.5326,  0.0952, -0.0637,  0.6259, -0.2566, -0.0808, -0.1455,\n",
       "           0.2833,  0.6471, -0.2102]]], grad_fn=<StackBackward>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_rnn[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5a91a55e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 10])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_rnn[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7908f6",
   "metadata": {},
   "source": [
    "`output_rnn[1]` contains ONLY the last hidden state. In the GIF this is $O5$, for us it would be $O13$. This hidden state describes (theoretically) the complete sequence and is therefore very important.\n",
    "\n",
    "The `output_rnn[0][-1] == output_rnn[1][0]` can also be checked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c639f24b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5940,  0.0767, -0.0585,  0.7042, -0.1930, -0.1573,  0.0256,  0.2622,\n",
      "          0.5856, -0.1866],\n",
      "        [-0.5326,  0.0952, -0.0637,  0.6259, -0.2566, -0.0808, -0.1455,  0.2833,\n",
      "          0.6471, -0.2102]], grad_fn=<SelectBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5940,  0.0767, -0.0585,  0.7042, -0.1930, -0.1573,  0.0256,  0.2622,\n",
       "          0.5856, -0.1866],\n",
       "        [-0.5326,  0.0952, -0.0637,  0.6259, -0.2566, -0.0808, -0.1455,  0.2833,\n",
       "          0.6471, -0.2102]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(output_rnn[0][-1])\n",
    "output_rnn[1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245d4c05",
   "metadata": {},
   "source": [
    "\n",
    "To understand in more detail what is happening, we will write an RNN ourself.\n",
    "\n",
    "\n",
    "Suppose we have a sentence `sentence = [\"Hello\", \"World\"]`. We have this stored as two words in a list. \n",
    "\n",
    "We also define two simple linear layers.  One maps the input from embedding size `4` to `10` dimensions. The other layer maps from `10` to `10` dimensons.\n",
    "\n",
    "Through the first network we send the first word `sentence[0]` and store the hidden state in `output_1`.\n",
    "\n",
    "\n",
    "```python\n",
    "sentence = [\"Hello\", \"World\"]\n",
    "\n",
    "lin_1 = nn.Linear(4,10) \n",
    "\n",
    "lin_2 = nn.Linear(10,10)\n",
    "\n",
    "output_1 =rnn(sentence[0])\n",
    "```\n",
    "\n",
    "Next, we also pass the second word `\"World\"` through the `lin_1`. But afterwards we also add the `lin_2(output_1)` to it. \n",
    "\n",
    "```python\n",
    "sentence = [\"Hello\", \"World\"]\n",
    "\n",
    "lin_1 = nn.Linear(4,10) \n",
    "\n",
    "lin_2 = nn.Linear(10,10)\n",
    "\n",
    "output_1 = lin_1(sentence[0])\n",
    "\n",
    "output_2 = lin_1(sentence[1]) + lin_2(output_1)\n",
    "```\n",
    "\n",
    "That is, the hidden state `output_2` is not determined by the word `\"World\"` alone, but the hidden state before it also has an influence. In fact, we also add a non-linear activation function. In RNNs, a Tanh function is used by default instead of a ReLU function.\n",
    "\n",
    "```python\n",
    "sentence = [\"Hello\", \"World\"]\n",
    "\n",
    "lin_1 = nn.Linear(4,10) \n",
    "\n",
    "lin_2 = nn.Linear(10,10)\n",
    "\n",
    "output_1 = lin_1(sentence[0])\n",
    "\n",
    "output_2 = torch.tanh(lin_1(sentence[1]) + lin_2(output_1))\n",
    "```\n",
    "\n",
    "If we had a third word in the sentence (`sentence[2]`), then this step would repeat. This time we add not `output_1`  but `output_2`:\n",
    "\n",
    "```python\n",
    "sentence = [\"Hello\", \"World\", \"Dude\"]\n",
    "\n",
    "lin_1 = nn.Linear(4,10) \n",
    "\n",
    "lin_2 = nn.Linear(10,10)\n",
    "\n",
    "output_1 = lin_1(sentence[0])\n",
    "\n",
    "output_2 = torch.tanh(lin_1(sentence[2]) + lin_2(output_1))\n",
    "\n",
    "output_3 = torch.tanh(lin_1(sentence[3]) + lin_2(output_2))\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "To check whether this is actually how the PyTorch RNN works, we can write our own implementation using the same weights as the PyTorch RNN.\n",
    "First we store the weights of the PyTorch `rnn`. We can now use these ourselves.\n",
    "Remember that `nn.Linear()` does nothing else than: `torch.mm(X,W.t())+b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c50167a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_1=list(rnn.parameters())[0]\n",
    "w_2=list(rnn.parameters())[1]\n",
    "b_1=list(rnn.parameters())[2]\n",
    "b_2=list(rnn.parameters())[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60372fd0",
   "metadata": {},
   "source": [
    "With these weights you can now calculate the hidden state for the first token in the Smiles sequence (`lin_1`). These are located in `token_embeddings_tensor[0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33012b6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "activations_jetzt = torch.mm(token_embeddings_tensor[0],____)+____\n",
    "activations_jetzt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34dfc7c3",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>Lösung:</b></summary>\n",
    "\n",
    "```python\n",
    "activations_jetzt = torch.mm(token_embeddings_tensor[0],w_1.t())+b_1\n",
    "activations_jetzt\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0cc165",
   "metadata": {},
   "source": [
    "Next we transform the hidden state of the previous token (`lin_2`). \n",
    "However, at the moment we are at the first word/token. So we don't have a hidden state of a previous token yet. This part was omitted in the previous text. In fact we start with a hidden state in which all values are zero. `h0 = torch.zeros(2,10)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df2dc15",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "h0 = torch.zeros(2,10)\n",
    "\n",
    "activations_vorher = torch.mm(___,____)+____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1752cd59",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>Lösung:</b></summary>\n",
    "\n",
    "```python\n",
    "h0 = torch.zeros(2,10)\n",
    "\n",
    "activations_vorher = torch.mm(h0,w_2.t())+b_2\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271e38a7",
   "metadata": {},
   "source": [
    "In the last step, the two activations are added and a `torch.tanh` activation function is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22abc20",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "torch.tanh(___________+_____________)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13ec875",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>Lösung:</b></summary>\n",
    "\n",
    "```python\n",
    "torch.tanh(activations_jetzt+activations_vorher)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30efabb2",
   "metadata": {},
   "source": [
    "This is the hidden state for the first token of the smile.\n",
    "We can also compare this with the hidden state of the `nn.RNN` and see that they are identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d03f84",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "output_rnn[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2321f929",
   "metadata": {},
   "source": [
    "We want to calculate the hidden states not only for the first token, but for all tokens in the smiles. Therefore we need a `for-loop`. \n",
    "\n",
    "First we initialize the first hidden state with zeros. And then we write a `for-loop`, which iterates through all 13 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "930a9487",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_____' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_102323/2527881365.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mh0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_smiles_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mactivations_jetzt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_____\u001b[0m  \u001b[0;31m# When calculating, always make sure to select the i element from the inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mactivations_vorher\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m________\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mh0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivations_jetzt\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mactivations_vorher\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# <-- The output is stored as h0,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name '_____' is not defined"
     ]
    }
   ],
   "source": [
    "h0 = torch.zeros(2,10)\n",
    "for i in range(max_smiles_length):\n",
    "    activations_jetzt = _____  # When calculating, always make sure to select the i element from the inputs\n",
    "    activations_vorher = ________\n",
    "    h0 = torch.tanh(activations_jetzt+activations_vorher) # <-- The output is stored as h0, \n",
    "h0                                                        #     to use it in the next iteration as the new h0\n",
    "                                                          #                   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2617ce",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>Lösung:</b></summary>\n",
    "\n",
    "```python\n",
    "h0 = torch.zeros(2,10)\n",
    "for i in range(max_smiles_length):\n",
    "    activations_jetzt = torch.mm(token_embeddings_tensor[i],w_1.t())+b_1\n",
    "    activations_vorher = torch.mm(h0,w_2.t())+b_2\n",
    "    h0 = torch.tanh(activations_jetzt+activations_vorher) \n",
    "h0                                                     \n",
    "```                                                          \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ab8db7",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b></b></summary>\n",
    "\n",
    "```python\n",
    "h0 = torch.zeros(2,10)\n",
    "for i in range(max_smiles_length):\n",
    "    activations_jetzt = torch.mm(token_embeddings_tensor[i],w_1.t())+b_1\n",
    "    activations_vorher = torch.mm(h0,w_2.t())+b_2\n",
    "    h0 = torch.tanh(activations_jetzt+activations_vorher) \n",
    "h0                                                     \n",
    "```                                                          \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f52917b",
   "metadata": {},
   "source": [
    "`h0` now contains the final hidden state. Again, we can check if our result is identical to PyTorchs `nn.RNN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c841036",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output_rnn[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2a849c",
   "metadata": {},
   "source": [
    "Of course, it is easier to use the PyTorch class rather than our own implementation. \n",
    "But programming it yourself should help you better understand what exactly happens in an RNN.\n",
    "\n",
    "Also, the code illustrates the biggest weakness of RNNs: the `for-loop`.\n",
    "We cannot pass a sentence/Smile through the network all at once. \n",
    "Each word/symbol must be passed through the network one at a time. This makes RNNs extremely slow.\n",
    "\n",
    "\n",
    "# PyTorch RNN\n",
    "\n",
    "PyTorch provides us not only with RNNs, but also `nn.Embedding` Layers. This is handy, for one thing, it makes backpropagation easier. In addition, we do not have create one-hot encoded vectors. PyTorch immediately takes as input the tokenized smiles `tokenized_smiles`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "03b8b34a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 3, 2, 0, 2, 1, 6, 6, 6, 6, 6, 6],\n",
       " [3, 0, 5, 2, 1, 4, 0, 0, 5, 2, 1, 4, 1]]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "47d0ae41",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = nn.Embedding(7,4, padding_idx = dictionary[\"<pad>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49be3f61",
   "metadata": {},
   "source": [
    "Here we defined a `torch` embedding layer was defined on top of it. It takes as input the number of different symbols/tokens in our dataset. In our case this would be `7`. The second parameter specifies the size of the embedding vectors. We will stick with the size `4`. The last thing we can tell PyTorch is which token, i.e. which number represents the padding. PyTorch will then set the embeddings for these tokens to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ba02f97c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 13, 4])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb(torch.tensor(tokenized_smiles)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f641ab29",
   "metadata": {},
   "source": [
    "The output of this embedding layer does not have the correct format yet. We still have to change the dimensions of the tensor with `Permute`. \n",
    "We can combine all these steps into an `nn.Sequential()` module. \n",
    "\n",
    "*There is no `permute` in the Pytorchs `nn` module, we wrote an adapated version to work in `nn.Sequential`. That's why we don't need an `nn.` in front of the `permute`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dca082",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Embedding(7,4, padding_idx = dictionary[\"<pad>\"]),\n",
    "                     Permute(1,0,2),\n",
    "                     nn.RNN(4,10))\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e198503",
   "metadata": {},
   "source": [
    "The `tokenized_smiles` can now be passed through the `model`. With `[1][0,:,:]` we can extract the final hidden states in the correct format. We can insert them directly into a linear layer. Since we need to index the output with `[1][0,:,:]`, we cannot use the linear layers directly in the same `nn.Sequential()` model. We need a second model that takes the `output_rnn` as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda59427",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_rnn = model(torch.tensor(tokenized_smiles))[1][0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8b57b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ll = nn.Sequential(nn.Linear(10,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980edb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ll(output_rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dcc187",
   "metadata": {},
   "source": [
    "There is still a problem with the `nn.RNN`. In the GIF you can clearly see that the first words in the sentence have less and less influence the longer the sentence gets. This can become a problem when sentences or Smiles become particularly long. Especially if subordinate clauses or, in the case of Smiles, additional branches are inserted into the `string`. It can happen that the beginning of the sentence or Smile is \"forgotten\" or lost by the network.\n",
    "\n",
    "For this reason, more complex RNN layers are usually used. This allows the networks to hold information over longer `strings`.\n",
    "\n",
    "A popular alternative is the Gated Recurrent Unit (GRU). Combining hidden states is much more complex than with \"vanilla RNNs\", but in PyTorch `nn.RNN` can easily be replaced by `nn.GRU`. Nothing needs to be changed on the rest of the network.\n",
    "\n",
    "RNN |GRU\n",
    "------|--------\n",
    "<img src=\"https://miro.medium.com/max/332/0*eRJCRsikdGGu8ffA.png\" width=\"200\"/> |<img src=\"https://miro.medium.com/max/700/1*RiOzdOVaaeKrUotY7-1a2A.png\" width=\"300\"/> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d90e7a",
   "metadata": {},
   "source": [
    "# Practise Exercise:\n",
    "\n",
    "In the exercise task, we will look at a new data set. The Blood-Brain Barrier Penetration (BBBP) dataset recorded for 2000 molecules whether they can diffuse through the blood-brain barrier.\n",
    "\n",
    "Most drugs and neurotransmitters cannot pass the blood-brain barrier. This is an important property for drugs that are intended to act in the central nervous system. Therefore, accurate prediction of these properties is of great interest.\n",
    "The original dataset was published in 2012. However, we use a slightly modified dataset. Here, all stereochemistry information has already been removed from the Smiles. In addition, the dataset contains only Smiles consisting of less than 75 tokens.\n",
    "> Martins, Ines Filipa, et al. \"A Bayesian approach to in silico blood-brain barrier penetration modeling.\" Journal of Chemical Information and Modeling 52.6 (2012): 1686-1697.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4c79a860",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from rdkit.Chem import AllChem as Chem\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from matplotlib import pyplot as plt\n",
    "%run ../utils/utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c6fa39",
   "metadata": {},
   "source": [
    "You can first read in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b5ff22b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CC(C)NCC(O)COc1cccc2ccccc12</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CC(C)(C)OC(=O)CCCc1ccc(N(CCCl)CCCl)cc1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CC1COc2c(N3CCN(C)CC3)c(F)cc3c(=O)c(C(=O)O)cn1c23</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CC(=O)NCCCOc1cccc(CN2CCCCC2)c1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cc1onc(-c2ccccc2Cl)c1C(=O)NC1C(=O)N2C1SC(C)(C)...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              smiles  target\n",
       "0                        CC(C)NCC(O)COc1cccc2ccccc12     1.0\n",
       "1             CC(C)(C)OC(=O)CCCc1ccc(N(CCCl)CCCl)cc1     1.0\n",
       "2   CC1COc2c(N3CCN(C)CC3)c(F)cc3c(=O)c(C(=O)O)cn1c23     1.0\n",
       "3                     CC(=O)NCCCOc1cccc(CN2CCCCC2)c1     1.0\n",
       "4  Cc1onc(-c2ccccc2Cl)c1C(=O)NC1C(=O)N2C1SC(C)(C)...     1.0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_bbbp = pd.read_csv(\"../data/bbbp/bbbp_clean.csv\")\n",
    "data_bbbp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90b2d3e",
   "metadata": {},
   "source": [
    "The `smiles` are given together with the `target`. A `1` indicates that these molecules can diffuse through the BBB. In the following cell we calculate the percentage of molecules that have this property in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "36a56f80",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_bbbp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_102323/4221171699.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_bbbp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mdata_bbbp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data_bbbp' is not defined"
     ]
    }
   ],
   "source": [
    "np.sum(data_bbbp.target)/data_bbbp.shape[0]*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83275f43",
   "metadata": {},
   "source": [
    "Because of the large imbalance, the ROC-AUC is the most suitable metric.\n",
    "But before we can turn our attention to the training, we must prepare the data.\n",
    "First create a `dictionary` that assigns numbers to all symbols in the `smiles`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "44644c6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dictionary = create_dict(data_bbbp.smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "505f2a75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'(': 0,\n",
       " 'O': 1,\n",
       " ')': 2,\n",
       " '1': 3,\n",
       " 'N': 4,\n",
       " 'C': 5,\n",
       " 'c': 6,\n",
       " '2': 7,\n",
       " 'Cl': 8,\n",
       " '=': 9,\n",
       " '3': 10,\n",
       " 'n': 11,\n",
       " 'F': 12,\n",
       " '-': 13,\n",
       " 'o': 14,\n",
       " 'S': 15,\n",
       " 'H': 16,\n",
       " '[': 17,\n",
       " ']': 18,\n",
       " '4': 19,\n",
       " 's': 20,\n",
       " '#': 21,\n",
       " 'Br': 22,\n",
       " 'I': 23,\n",
       " '+': 24,\n",
       " '5': 25,\n",
       " 'P': 26,\n",
       " '6': 27,\n",
       " 'B': 28}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3273b8",
   "metadata": {},
   "source": [
    "With this dictionary, you now convert the actual symbols of the Smiles to numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "54f0cb09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenized_smiles = tokenize(data_bbbp.smiles,dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077ef5e1",
   "metadata": {},
   "source": [
    "The problem is, as in the example, that the molecules and thus the `smiles` are of different lengths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d8785c0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([27, 36, 48, ..., 43, 56, 49])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length_ll = np.array([len(x) for x in tokenized_smiles])\n",
    "length_ll"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16e5343",
   "metadata": {},
   "source": [
    "Therefore you must first bring all `tokenized_smiles` to the same length. This is the length of the longest smile. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c561303d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length = max(length_ll)\n",
    "max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518f6f30",
   "metadata": {},
   "source": [
    "To all smiles that consist of less than 74 tokens, we add additional `<pad>` tokens until they are 74 tokens long. We assign the `<pad>` token the value `len(dictionary)` since this is the next unused number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2be6dd3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "print(len(dictionary))\n",
    "dictionary[\"<pad>\"]= len(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6e8b6e",
   "metadata": {},
   "source": [
    "The following code adds this padding token to all smiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a5392ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, tok_smi in enumerate(tokenized_smiles):\n",
    "    tokenized_smiles[i] = tok_smi+ [dictionary[\"<pad>\"]]*(max_length - length_ll[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5d0b8cf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " ...]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length_ll = [len(x) for x in tokenized_smiles]\n",
    "length_ll"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ac61b2",
   "metadata": {},
   "source": [
    "Now all `tokenized_smiles` are of the same length and in the correct format. But you have to divide the data into training and test dataset again. \n",
    "We merge the `tokenized_smiles` and targets from the `data_bbbp`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd115684",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_bbbp_tokenized = np.hstack([np.array(tokenized_smiles), data_bbbp.iloc[:,1:2]])\n",
    "data_bbbp_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e74a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data_bbbp_tokenized,test_size=0.2,train_size=0.8, random_state=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfa7e24",
   "metadata": {},
   "source": [
    "We separate the inputs and outputs from each other again. Important: The `targets` are in the last column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d91ce3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = torch.tensor(train[:,:-1], dtype=torch.long )\n",
    "train_y = torch.tensor(train[:,-1], dtype=torch.float)\n",
    "test_x = torch.tensor(test[:,:-1], dtype=torch.long)\n",
    "test_y = torch.tensor(test[:,-1], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05aa661a",
   "metadata": {},
   "source": [
    "Create the training loader so we can train with minibatches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96278c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(train_x, train_y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32)\n",
    "\n",
    "test_dataset = TensorDataset(test_x, test_y)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f89ddc",
   "metadata": {},
   "source": [
    "Then define the model which requires an embedding layer, a permute layer and an RNN. Here we use the GRU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc00107",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1111)\n",
    "model =nn.Sequential(nn.Embedding(len(dictionary),32, padding_idx = dictionary[\"<pad>\"]),\n",
    "                     Permute(1,0,2),\n",
    "                     nn.GRU(32,64))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0d48e2",
   "metadata": {},
   "source": [
    "You also need a linear layer that makes predictions based on the output of the GRU. \n",
    "For this we create a second model called `pred_ll`.\n",
    "\n",
    "Why do we need a second model?\n",
    "\n",
    "This is because all RNNs in PyTorch have more than one output. One for all hidden states and one for only the final hidden states. In this case the `nn.Sequential` network does not know which output should be passed from the RNN to the linear layer.\n",
    "\n",
    "Therefore we need a second model `pred_ll`. Here we use batchnorm and dropout. Make sure that the dimensions of `BatchNorm1d` and `Linear` correspond to the output dimension of the `GRU`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1f1a7b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1111)\n",
    "pred_ll = nn.Sequential(nn.BatchNorm1d(64),nn.Dropout(0.2),nn.Linear(64,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1673b9e2",
   "metadata": {},
   "source": [
    "Addtionally, define a loss function and an optimizer. Remember that we have a binary classification.\n",
    "Since we have two networks that we want to update together, we can combine the parameters of the two into one list and make them available to the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1b75df",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_funktion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(list(model.parameters()) + list(pred_ll.parameters()), lr =0.001) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6259062",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(40):\n",
    "    pred_ll.train()\n",
    "    for input_, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        rnn_output = model(input_)[1][0]\n",
    "        output = pred_ll(rnn_output).flatten()\n",
    "        \n",
    "        loss = loss_funktion(output, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    pred_ll.eval()\n",
    "    \n",
    "    rnn_output = model(train_x)[1][0]    \n",
    "    output = pred_ll(rnn_output).flatten()\n",
    "    loss_train = loss_funktion(output, train_y)\n",
    "    auc_train = roc_auc_score(train_y.numpy(),torch.sigmoid(output).detach().clone().numpy())\n",
    "    \n",
    "    rnn_output = model(test_x)[1][0]    \n",
    "    output = pred_ll(rnn_output).flatten()\n",
    "    loss_test = loss_funktion(output, test_y)\n",
    "    auc_test = roc_auc_score(test_y.numpy(),torch.sigmoid(output).detach().clone().numpy())\n",
    "    \n",
    "    print(\"Training Loss: %.3f Training AUC: %.3f | Test Loss: %.3f Test AUC: %.3f\"\n",
    "        % (loss_train.item(), auc_train,loss_test.item(), auc_test ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0535564",
   "metadata": {},
   "source": [
    "You can see that you can make accurate predictions with an RNN. However, in reality, ECFP and classical neural networks often work better. Especially on small datasets, since they are not as complex. \n",
    "\n",
    "Last, we look at the learned embeddings. For this, we store the weight matrix of the embeddings layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "80bf9b9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_102323/2682336203.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0membedding_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0membedding_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "embedding_weights = list(model[0].parameters())[0].detach().clone().numpy()\n",
    "embedding_weights.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7378cdb7",
   "metadata": {},
   "source": [
    "One way to analyze the embeddings is to compare the similarity of different tokens via the `cosine_similarity`. Tokens with similar function should have similar embeddings.\n",
    "\n",
    "As an example we calculate the similarity of the embeddings of a nitrogen in an aromatic ring (`n`).\n",
    "We have to find which number belongs to `n` in the dictionary, and hence also the index of the row in the corresponding embedding matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e61b5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_n = dictionary[\"n\"]\n",
    "dictionary[\"n\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da85fab4",
   "metadata": {},
   "source": [
    "We calculate the similarity of this embedding to all other embeddings. Afterwards a bar chart is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322387ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_N = cosine_similarity(embedding_weights[idx_n:idx_n+1,:],embedding_weights)[0]\n",
    "labels = [x for x in dictionary]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736a0fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_values=pd.DataFrame({\"symbol\": labels, \"similarity\":similarity_N}).sort_values(\"similarity\", ascending =False)\n",
    "sorted_values.plot.bar(\"symbol\", \"similarity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccc400e",
   "metadata": {},
   "source": [
    "The problem with such a small data set is that the embeddings are extremely dependent on the data set. Nevertheless, general trends can be identified. `n` is more similar to the aromatic atoms `o` or `c` than to the atoms outside an aromatic ring `C`,`N` and `O`. However, the exact embeddings can vary extremely from training to training.\n",
    "\n",
    "You can also compare other symbols by looking here which value is assigned to a certain token:\n",
    "\n",
    "`idx_n = dictionary[\"o\"]`.\n",
    "\n",
    "Choose another symbol."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
