{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"https://venturebeat.com/wp-content/uploads/2019/06/pytorch-e1576624094357.jpg?w=1200&strip=all\" style=\"width: 800px;\">\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### In this lesson you'll learn:\n",
    "* how to program a simple neural net using PyTorch.\n",
    "* how to implement more advanced layers in your neural net (Dropout, Batchnorm).\n",
    "* about more advanced optimisation (Momentum, adam).\n",
    "---\n",
    "\n",
    "Last week you programmed a simple neural network yourself. As mentioned earlier, it is not necessary to program every net yourself. Certain software packages take care of many of the inconveniences of creating and training nets \"by hand\".\n",
    "\n",
    "Essentially, there are two libaries that can be used: PyTorch and TensorFlow. TensorFlow is developed by Google and is the more popular choice, especially in industry. PyTorch, on the other hand, is mostly used in the scientific world. Basically, PyTorch is considered the easier framework to learn and is a bit more user-friendly overall. \n",
    "\n",
    "While there used to be major differences, today the two libraries are becoming more and more similar in functionality.\n",
    "\n",
    "Finally, there is Keras and PyTorch Lightning. Both aim to make neural network creation even easier. Keras uses TensorFlow in the background, but makes it easier to train networks, especially for beginners. The same is true for PyTorch Lightning and PyTorch.\n",
    "\n",
    "In cheminformatics, however, PyTorch is a good choice, since special libraries such as for Graph Neural Networks exist or existed only for PyTorch.\n",
    "\n",
    "\n",
    "An essential part of PyTorch is **autograd**. Autograd is a library that, as the name suggests, can compute and collect the gradients automatically. So you don't have to calculate the gradients yourself.\n",
    "Also, there are many functions, like activation functions or linear transformations, that are already implemented in PyTorch. \n",
    "\n",
    "*TensorFlow has these functionalities as well, of course.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors\n",
    "\n",
    "While you have been working with `numpy` arrays so far, today we will use `tensors`, more precisely PyTorch `tensors`. \n",
    "\n",
    "**What is the difference?**\n",
    "\n",
    "First of all, none. Arrays and tensors are similar in many ways. Both store numbers/values in a structured form. So in NumPy you can store matrices in a 2D array, but you can also store the same matrix in a 2D tensor.\n",
    "Also, tensors can be converted to arrays and arrays can be converted to tensors.\n",
    "\n",
    "The difference between the two \"storage options\" is that PyTorch tensors were developed by PyTorch. And NumPy arrays were developed by the developers of NumPy. \n",
    "Many features that NumPy offers are also available from PyTorch for their tensors (but may be called differently). \n",
    "PyTorch developed its tensors to perform mathematical operations faster. In addition, tensors can also be \"loaded\" onto the graphics card, which increases the speed of operations many times over."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculations with `tensors` are almost identical to calculations with `np.arrays`. But the functions can have different names. For example `torch.mm()` is the function for matrix multiplication and `.t()` is the transpose of a matrix. Similar to the function `np.array()` to create arrays, `torch.tensor()` creates tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # loads PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([[1,2,3],\n",
    "                [4,5,6]])\n",
    "\n",
    "W = torch.tensor([[8,9,10],\n",
    "                 [11,12,313]])\n",
    "\n",
    "b = torch.tensor([1,2])\n",
    "\n",
    "torch.mm(X,W.t())+b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the linear transformation, which you also already know.<br>\n",
    "However, PyTorch simplifies this step. \n",
    "In PyTorch there is a module called `nn`, this contains many functions that are helpful in creating neural networks.\n",
    "\n",
    "We can load the module `nn` with `from torch import nn`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The submodule `nn` provides among others the function `nn.Linear`. It performs the linear transformation $xW^T +b $.\n",
    "As input the function takes:\n",
    "\n",
    "\n",
    "* `in_features` the number of features the input has before the transformation, or the size of the input layers. Yesterday the images had 784 pixels, so 784 features.\n",
    "* `out_features` the number of features the input should have after the transformation. So `out_features` defines the size of the hidden layer. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_1 = nn.Linear(in_features = 784, out_features=300, bias=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isn`t the input for the layers missing?\n",
    "\n",
    "That's right, until now you haven't performed a linear transformation either, but only created a variable `layer_1`. This can then perform the linear transformation for us.\n",
    "\n",
    "---\n",
    "*Technically `nn.Linear` is not a function, but a `class`. Classes are special Python objects. How exactly classes work is not relevant for this course. What is important to understand is that*\n",
    "\n",
    "```py\n",
    "layer_1 = nn.Linear(in_features = 784, out_features=300, bias=True)\n",
    "```\n",
    "*creates an object `layer_1` which belongs to the class `nn.Linear`. Each class in Python can have its own functions. For example, most `nn` classes have a `forward` function that executes a particular forward pass.*\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A practical feature of `nn` layers is that the weights of these layers are automatically initialized by PyTorch. This already saves you a little work.\n",
    "The weights $W$ of these layers can also be viewed.\n",
    "\n",
    "For this you use `list(layer_1.parameters())[0]`.\n",
    "If you want to know the exact size of the weight matrix, you can use `.shape` like NumPy: `list(layer_1.parameters())[0].shape`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(layer_1.parameters())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(layer_1.parameters())[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the weight matrix has the same size as the matrix of the last week. You can also see that the matrix actually contains weights.\n",
    "All you need now is an input (images) that you want to change with this linear transformation. \n",
    "This is done by loading the training dataset from last week with `numpy`.\n",
    "\n",
    "Additionally, the images need to be transformed into a tensor. For this you use `torch.tensor()`.\n",
    "\n",
    "\n",
    "Of course you have to scale the data again. For this you use the min-max scaler.\n",
    "In PyTorch, you have to pay more attention to the data types. \n",
    "Therefore we define the datatype `dtype`. The datatype for our images is `float32`. You know `float` from yesterday, the `32` defines how exact this number can be.\n",
    "`long` may not mean anything to you, but it simply denotes `integers`.\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "<details>\n",
    "<summary><strong>Only for the particularly interested:</strong></summary>\n",
    "\n",
    "In the last notebook we discussed why we initialize the weight matrix in this particular way.\n",
    "In fact, in TensorFlow, the weight matrices are created so that matrix multiplication can be performed without `transpose`.\n",
    "\n",
    "    \n",
    "Here is the description of the code for the forward pass in TensorFlow.\n",
    "    \n",
    "\n",
    "  `Dense` implements the operation:\n",
    "  `output = activation(dot(input, kernel) + bias)`\n",
    "  where `activation` is the element-wise activation function\n",
    "  passed as the `activation` argument, `kernel` is a weights matrix\n",
    "  created by the layer, and `bias` is a bias vector created by the layer\n",
    "  (only applicable if `use_bias` is `True`). These are all attributes of\n",
    "  `Dense`.\n",
    "\n",
    "PyTorch's description can be found [here](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html).\n",
    "    \n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def min_max(x):\n",
    "    return (x - np.min(x)) / (np.max(x) - np.min(x))\n",
    "\n",
    "\n",
    "train_data = np.genfromtxt('../data/mnist/mnist_train.csv', delimiter=',', skip_header =False) #genfromtxt reads .txt files if we chose delimiter =\",\" the function can read also .csv files  (comma seperated values)\n",
    "\n",
    "train_images = min_max(train_data[:,1:])\n",
    "train_images = torch.tensor(train_images, dtype = torch.float32)\n",
    "train_labels=torch.tensor(train_data[:,0].astype(int), dtype = torch.long) \n",
    "\n",
    "\n",
    "test_data = np.genfromtxt('../data/mnist/mnist_test.csv', delimiter=',', skip_header =False) #genfromtxt reads .txt files if we chose delimiter =\",\" the function can read also .csv files  (comma seperated values)\n",
    "\n",
    "test_images = min_max(test_data[:,1:])\n",
    "test_images = torch.tensor(test_images, dtype = torch.float32)\n",
    "test_labels=torch.tensor(test_data[:,0].astype(int), dtype = torch.long) \n",
    "\n",
    "train_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set contains 60000 images with 784 pixels each.\n",
    "You can now use these as input for the linear transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_1=layer_1(train_images)\n",
    "print(z_1)\n",
    "z_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `layer_1` returns the output (`z_1`). This has the shape `[60000,300]`. So still 60000 images, but this time each has only 300 features (size of the hidden layer). Just like it was defined when `layer_1` was created.\n",
    "\n",
    "\n",
    "What you should notice is the `grad_fn=<AddmmBackward>` at the end of the `z_1` tensor. You can see that *autograd* has captured the gradients for this transformation. You can also see that we have performed a matrix multiplication `mm` and an addition `Add`. The last performed transformation of the tensor is always shown.\n",
    "\n",
    "What you are missing now is the activation function. PyTorch can help here as well. \n",
    "The `nn` library of PyTorch has a submodule `functional`. Here are many additional mathematical functions included, among others the `relu` and `sigmoid` functions. Since the `relu` function gives even better results in practice than the `sigmoid` function, we will use it now.\n",
    "\n",
    "`functional` can be imported as follows: `from torch.nn import functional as F`. We rename `functional` to `F`, sort of a standard when working with PyTorch.\n",
    "\n",
    "`F.relu()` can now be used to apply the ReLU function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "a_1 = F.relu(z_1)\n",
    "print(a_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenn Sie `a_1` mit `z_1` vergleichen, können Sie sehen, dass alle Werte, die vorher negativ waren, zu Null wurden und alle Werte, die positiv waren, unverändert sind.\n",
    "Auch kann man in der `grad_fn` sehen, dass eine ReLU angewendet wurde. Auch dies wurde von *autograd* aufgezeichnet. \n",
    "\n",
    "Der erste Teil des Forward Pass ist damit erledigt. \n",
    "Für den zweiten Schritt können wir einfach eine weitere Layer erstellen, die `a_1` als Eingabe erhält."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_2 = nn.Linear(300,10) # 10 ist die Anzahl der out_features, da wir 10 Ziffern haben.\n",
    "z_2=layer_2(a_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wieder brauchen Sie eine Aktivierungsfunktion. Aber dieses Mal die `softmax`-Funktion, um die Wahrscheinlichkeiten zu erhalten.\n",
    "Die Funktion `nn.functional` hat auch eine `softmax()`-Funktion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = F.softmax(z_2,dim=1)# der dim Parameter legt fest ob über Spalten oder Reihen die Softmax funktion angewandt wird.\n",
    "y_hat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch können Sie auch verschiedene Schichten kombinieren. Mit `nn.Sequential` können Sie die lineare Transformation und die Aktivierungsfunktion direkt nacheinander schalten. Die Eingabe wird automatisch durch jede der Layers geleitet.\n",
    "Das macht deinen Code übersichtlicher und einfacher zu schreiben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netzwerk = nn.Sequential(nn.Linear(784,300), \n",
    "                         nn.ReLU(), \n",
    "                         nn.Linear(300,10))\n",
    "netzwerk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie Sie sehen können, wurde ein Netzwerk mit einer Hidden Layer erstellt. Was Ihnen auffallen sollte, ist, dass anstelle von `F.relu` `nn.ReLU` verwendet wurde. Wenn eine `relu`-Funktion innerhalb von `Sequential()` verwendet werden soll, müssen Sie immer `nn.ReLU` verwenden. \n",
    "\n",
    "Das `netzwerk` kann nun die Bilder klassifizieren:\n",
    "Mit `network(input)` kann man dInput, z.B. unsere Bilder, durch das Netzwerk leiten.\n",
    "Anhand der Tensorgröße der Ausgabe kann man sehen, dass am Ende tatsächlich 60000 Bilder mit je 10 Merkmalen (die 10 Ziffern) als Output ausgegeben werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = netzwerk(train_images)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eine weitere Änderung ist, dass Sie nicht mehr die letzte Aktivierungsfunktion verwenden. PyTorch wählt die Aktivierungsfunktion automatisch aus. Die Entscheidung, welche Aktivierungsfunktion in der letzten Schicht zu verwenden ist, hängt von der Wahl der Lossfunktion ab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Funktion\n",
    "\n",
    "\n",
    "`nn` kann auch bei der Lossfunktion helfen. Die gängigsten Lossfunktionen sind bereits in PyTorch enthalten.\n",
    "Sie können einfach eine neue Variable erstellen und ihr die Funktion `nn.CrossEntropyLoss()` zuweisen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_funktion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion `loss_function` kann nun den Loss berechnen, indem sie automatisch die Softmax-Funktion anwendet.\n",
    "Dazu muss man nur `y_hat` und die `train_labels` in die Funktion eingeben. Hier sieht man ein weiterer Vorteil von Pytorch: man  muss die Labels nicht noch `one-hot` kodieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_funktion(output, train_labels)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back Propagation\n",
    "\n",
    "Der letzte Schritt besteht darin, eine Backpropagation durchzuführen. Dank *autograd* ist dies einfach mit dem Befehl `loss.backward()` möglich. Er berechnet die Gradienten für alle Weightmatrizen.\n",
    "\n",
    "Danach müssen Sie nur noch die Weightmatrizen aktualisieren. Wie Sie sich vorstellen können, kümmert sich PyTorch auch um diese Aufgabe.\n",
    "PyTorch bietet sogar eine Vielzahl von  unterschiedlichen Algorithmen, welche die Weight auf unterschiedliche Weise aktualisieren.\n",
    "\n",
    "Für das Aktualisieren der Gewichte wird ein neues Modul von PyTorch benötigt.\n",
    "Dazu lädt man `from torch import optim`. `optim` enthält Optimierer, Funktionen, die das Netz für uns optimieren - also die Weights aktualisieren.\n",
    "\n",
    "Ähnlich wie bei der Lossfunktion kann man einfach eine Variable erstellen und ihr eine Updatefunktion zuweisen. \n",
    "Sie können nun die Funktion `optim.SGD()` zur Aktualisierung der Weights verwenden. SGD = Stochastic Gradient Descent.  In der Funktion selbst legen Sie fest, welche Parameter (Weights) verändert werden sollen. Außerdem legen Sie hier die Lernrate fest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward() #sammelt die Gradienten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "weights_updaten=optim.SGD(netzwerk.parameters(), lr=0.01) # Sie legen fest welche Parameter und mit welche Lernrate diese verändert werden sollen\n",
    "\n",
    "weights_updaten.step()  # mit- step() updaten Sie die Weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt haben Sie alles, was Sie brauchen, um ein Netz zu trainieren.\n",
    "\n",
    "Sie können wieder einen `for-loop` verwenden, um das Training zu automatisieren.\n",
    "\n",
    "Sie werden feststellen, dass wir zusätzlich noch `updata.zero_grad()` verwenden. Diese Funktion wird verwendet, um die Gradienten des vorherigen Updates zu löschen. Wenn dies nicht geschieht, würde der Optimierer alle Gradienten aus allen Epochen konstant summieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Definieren von Netzwerk, LossFunktion und Update Algorithmus\n",
    "netzwerk = nn.Sequential(nn.Linear(784,300), \n",
    "                         nn.ReLU(), \n",
    "                         nn.Linear(300,10))\n",
    "\n",
    "loss_funktion = nn.CrossEntropyLoss()\n",
    "updaten = optim.SGD(netzwerk.parameters(), lr=0.3)\n",
    "EPOCHS = 50\n",
    "\n",
    "## Trainings Loop\n",
    "for i in range(50):\n",
    "    updaten.zero_grad()\n",
    "    output = netzwerk(train_images) # Forward Propagation\n",
    "    \n",
    "    loss   = loss_funktion(output, train_labels)\n",
    "    loss.backward()\n",
    "    acc=((output.max(dim=1)[1]==train_labels).sum()/float(output.shape[0])).item()\n",
    "    print(i, \n",
    "        \"Training Loss: %.2f Training Accuracy: %.2f\"\n",
    "        % (loss.item(), acc)\n",
    "    )\n",
    "    \n",
    "    updaten.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sie sehen, dass Sie ein neuronales Netz mit viel weniger Aufwand trainieren können. Sie können auch ohne großen Aufwand eine zweite oder dritte Hidden Layer zu Ihrem Modell hinzufügen.\n",
    "Fügen Sie einfach eine `nn.ReLU`- und eine `nn.Linear`-Schicht in `Sequential` ein, und *autograd* kann die Gradienten auch für diese Schichten berechnen. Alles andere bleibt gleich. Denken Sie nur daran, dass die Dimensionen von einer Layer zur nächsten passen müssen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Definieren von Netzwerk, Loss Funktion und Update Algorithmus\n",
    "netzwerk = nn.Sequential(nn.Linear(784,300), \n",
    "                         nn.ReLU(), \n",
    "                         nn.Linear(300,300),# <----- EXTRA LAYER\n",
    "                         nn.ReLU(), \n",
    "                         nn.Linear(300,10))\n",
    "print(netzwerk)\n",
    "loss_funktion = nn.CrossEntropyLoss()\n",
    "updaten = optim.SGD(netzwerk.parameters(), lr=0.3)\n",
    "EPOCHS = 50\n",
    "\n",
    "## Trainings Loop\n",
    "for i in range(50):\n",
    "    updaten.zero_grad()\n",
    "    output = netzwerk(train_images) # Forward Propagation\n",
    "    \n",
    "    loss   = loss_funktion(output, train_labels)\n",
    "    loss.backward()\n",
    "    \n",
    "    acc=((output.max(dim=1)[1]==train_labels).sum()/float(output.shape[0])).item()\n",
    "    print(i,\n",
    "        \"Training Loss: %.2f Training Accuracy: %.2f\"\n",
    "        % (loss.item(), acc)\n",
    "    )\n",
    "    \n",
    "    updaten.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sie haben vielleicht bemerkt, dass wir den Stochastic Gradient Descent als Optimierer verwenden (Weights aktualisieren). Bislang haben wir nur über den Gradient Descent gesprochen.\n",
    "Tatsächlich wird der Gradient Descent, wie gestern erklärt, eigentlich nicht mehr verwendet, sondern alternativ der Stochastic Gradient Descent.\n",
    "\n",
    "Der Unterschied:<br>\n",
    "Im Stochastic Gradient Descent wird der Datensatz nicht auf einmal durch das Netz geschickt, sondern der Datensatz wird in kleineren Teilen (**minibatch**) durch das Netzwerk geführt. <br>\n",
    "In diesem Datensatz gibt es insgesamt 60000 Bilder. Beim Gradient Descent, wird der Forwardpass gleichzeitig mit 60000 Bilder durchgeführt, und für die 60000 Bilder wird gleichzeitig der Loss berechnet. Danach werden die Weights **einmal** upgedatet.\n",
    "Dann wiederholt sich der Schritt. \n",
    "\n",
    "Es wäre effizienter, wenn nicht nach allen 60000 Bilder ein Update stattfinden würde. Sondern schon nach 200 oder sogar nur 100, so dass das Netz viel schneller lernen kann.\n",
    "Genau das macht der Stochastic  Gradient Descent.  Nicht alle Bilder, sondern nur z.B. 32 Bilder werden auf einmal durch das Netz geschickt. Für diese 32 Bilder wird dann der Loss berechnet und die Updates werden durchgeführt.\n",
    "Dann wird dieser Schritt wiederholt, aber diesmal mit 32 neuen Bildern. Auf diese Weise können die Weights innerhalb einer Epoche sehr viel häufiger aktualisiert werden.\n",
    "\n",
    "Die Batchsize gibt an, wie groß ein Minibatch (der kleine Teil der Daten, der durch das Netz geführt wird) sein soll, und kann ebenfalls die Performance des Modells beeinflussen.\n",
    "<center>\n",
    "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcT0TVrYkk0A0FfPvnzYTe747F0qPLG2rU2Bmg&usqp=CAU\" style=\"width: 600px;\">\n",
    "</center>\n",
    "<h8><center>Source: Insu Han and Jongheon Jeong, http://alinlab.kaist.ac.kr/resource/Lec2_SGD.pdf</center></h8>\n",
    "\n",
    "\n",
    "Vorteile:<br>\n",
    "* schneller\n",
    "* braucht weniger Speicherplatz (auf der Graphikkarte)\n",
    "\n",
    "Nachteile: <br>\n",
    "* kann nicht das Optimum finden (kann aber auch gut sein, um overfitting zu verhindern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um die Vorteile des Stochastic Gradient Descent nutzen zu können, müssen wir die Daten zunächst in Minibatches aufteilen. Auch hierfür kann man PyTorch verwenden, die entsprechenden Funktionen sind im Submodul `torch.utils.data` verfügbar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `torch.utils.data` gibt es zwei Funktionen, die Sie benötigen:\n",
    "\n",
    "* `data.TensorDataset(input,labels)` erzeugt einen PyTorch-Dataset aus den Daten.\n",
    "* `data.DataLoader(Dataset, batch_size)` erzeugt Minibatches der angegebenen Größe aus einem PyTorch Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data.TensorDataset(train_images, train_labels) # input sind unsere Tensors die einmal die Bilder und einmal die Labels beinhalten\n",
    "loader = data.DataLoader(train_data, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Variable `loader` enthält nun 1875 Minibatches mit jeweils 32 Bildern und deren 32 Beschriftungen. In der nächsten Zelle können Sie den Inhalt des ersten Batches sehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(loader)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um alles zusammenzubringen, braucht man einen zweiten `for-loop`, der die Minibatches einen nach dem anderen innerhalb des ersten `for-loops` auswählt und sie durch das Netzwerk führt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Definieren von Netzwerk, LossFunktion und Update Algorithmus\n",
    "netzwerk = nn.Sequential(nn.Linear(784,300), \n",
    "                         nn.ReLU(), \n",
    "                         nn.Linear(300,300),\n",
    "                         nn.ReLU(), \n",
    "                         nn.Linear(300,10))\n",
    "loss_funktion = nn.CrossEntropyLoss()\n",
    "updaten = optim.SGD(netzwerk.parameters(), lr=0.3)\n",
    "EPOCHS = 2\n",
    "\n",
    "## Trainings Loop\n",
    "for i in range(EPOCHS):\n",
    "    loss_list = [] # diese Liste speichtert den Loss jedes Minibatches\n",
    "                   # damit können wir am Ende den Durschnittslosd innerhalb des Epochs berechnen\n",
    "    for minibatch in loader: # for-loop geht durch alle Minibatches\n",
    "        images, labels = minibatch # minibatch wird in Bilder und Labels geteilt\n",
    "        \n",
    "        updaten.zero_grad()\n",
    "        output = netzwerk(images) # Forward Propagation\n",
    "    \n",
    "        loss   = loss_funktion(output, labels)\n",
    "        loss.backward()\n",
    "        loss_list.append(loss.item())\n",
    "        updaten.step()\n",
    "        \n",
    "    output = netzwerk(train_images)\n",
    "    acc=((output.max(dim=1)[1]==train_labels).sum()/float(output.shape[0])).item()\n",
    "    print(\n",
    "        \"Training Loss: %.2f Training Accuracy: %.2f\"\n",
    "        % (np.mean(loss_list), acc)\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nach nur zwei Epochen ist die Accuracy viel höher als je zuvor. Ein einzelner Epoch dauert viel länger im Vergleich zu \"normalen\" Gradient Descent , aber die insgesamte Trainingszeit wird reduziert.\n",
    "\n",
    "Zur Berechnung der Accuracy wird der gesamte Datensatz, nachdem alle Minibatches das Netzwerk durchlaufen haben, erneut durch das Netzwerk geschickt (ohne die Gewichte zu ändern). Die Accuracy wird dann auf der Grundlage dieser Vorhersagen berechnet, Der Epochloss ist der durchschnittliche Loss der Minibatches.\n",
    "\n",
    "Tipp: Wenn Sie den Wert eines Tensors und nicht den Tensor selbst kopieren/ausgeben wollen, können Sie `x.item()` verwenden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fortgeschrittene Layers\n",
    "\n",
    "\n",
    "Im Folgenden werden wir uns mit den neuen Layers befassen, die man neben linearen Layers verwendet.\n",
    "\n",
    "## Dropout\n",
    "\n",
    "Dropout wird während des Trainings verwendet, um einzelne Neuronen nach dem Zufallsprinzip aus dem Netz *zu entfernen*. Mit anderen Worten, sie werden für eine kurze Zeit ausgeschaltet. Mathematisch gesehen bedeutet dies, dass ihre Output einfach auf Null gesetzt wird.\n",
    "Jedes Mal, wenn ein Batch das Netz durchläuft, wird der Output von zufällig ausgewählten Neuronen auf Null gesetzt. \n",
    "\n",
    "Diese zufällige temporäre *Löschung*  von Neuronen zwingt das Netz, sich nicht auf einzelne Neuronen zu verlassen. Ähnlich wie bei Random Forest, wo zufällig Variablen entfernt werden, verhindert Dropout ein *overfitting* .\n",
    "Während der Validierung des Netzwerkes (Valideriungs- oder Testset), werden jedoch keine Neuronen mehr entfernt. Das heißt, die Dropout-Layer wird übersprungen.\n",
    "\n",
    "Wie viele Neuronen aus dem Netzwerk entfernt werden, ist ein weiterer Hyperparameter, der ebenso wie die Lernrate von Ihnen gewählt werden kann. Der Dropout wird in Prozent angegeben. Ein Dropout von `0,8` bedeutet also, dass in dieser Layer 80% der Neuronen entfernt werden bzw. ihr Output auf Null gesetzt wird. Ein Standardwert für den Dropout ist `0.2`. Oft wird eine Dropout-Layer auch direkt nach dem Input verwendet.\n",
    "\n",
    "In PyTorch wird eine Dropout-Layer wie folgt definiert: `nn.Dropout(0.2)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1235)\n",
    "\n",
    "beispiel_x = torch.tensor([[1.,2.,3.,4.,5.]] )\n",
    "do = nn.Dropout(0.5)\n",
    "do(beispiel_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drei der Werte wurden auf `0` gesetzt, aber die anderen Werte haben sich verdoppelt. **Warum ist das passiert?**\n",
    "\n",
    "Das liegt daran, dass wir mit Dropout trainieren, aber ohne Dropout evaluieren. Das bedeutet, dass bei einem Dropout von \"`0.5` nur die Hälfte der Neuronen verwendet wird. Bei dem Forward Pass werden die Werte als gewichtete Summe weitergegeben. Wenn die Hälfte der Neuronen den Wert `0` hat, ist die Summe natürlich viel kleiner als in einem Netz, in dem kein Dropout verwendet wird.I m Laufe des Trainings wird die Skala der Weights an die erwartete Skala der Inputs angepasst.\n",
    "\n",
    "Ohne Dropout: $$$z = \\beta_0 + \\beta_11 + \\beta_22 +\\beta_33 +\\beta_44 +\\beta_55$$\n",
    "\n",
    "Mit Dropout: $$\\begin{align}z&= \\beta_0 + \\beta_10 + \\beta_22 +\\beta_30 +\\beta_44 +\\beta_50 \\\n",
    "&=\\beta_0 + \\beta_22+\\beta_44\\end{align}$$\n",
    "\n",
    "\n",
    "Die Summe $z$ mit Dropout ist immer kleiner als die Summe ohne Dropout. Indem wir die Größe der weitergeleiteten Inputs erhöhen, stellen wir sicher, dass sich die Weights nicht an die falsche Skala der Inputs anpassen.\n",
    "\n",
    "Denn bei der Evaluierung, bei der kein Dropout verwendet wird, haben wir plötzlich doppelt so viele Inputs. Diese Diskreptanz zwischen Training und Evaluierung wird damit vorgebeugt.\n",
    "\n",
    "Sie können eine Schicht oder ein komplettes Netz vom Trainings- in den Evaluierungsmodus wechseln, indem Sie `network.eval()` verwenden. Um es in den Trainingsmodus zu versetzen, wird `.train()` verwendet. WICHTIG ist, dass sich standardmäßig jede Schicht und jedes Netz im Modus `train()` befindet.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "do.eval()\n",
    "do(beispiel_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im `.eval()` Modus wird der Dropout nicht angewendet.\n",
    "\n",
    "## Batchnorm\n",
    "\n",
    "\n",
    "Batchnorm-Layers sind eine weitere häufig verwendete Layer in neuronalen Netzen. Wie der Name suggeriert, normalisieren sie die Batches oder die besser gesagt die Activations der Minibatches.\n",
    "Erinnern Sie sich daran, dass wir unsere Eingaben skalieren. Batchnorm tut dasselbe, aber im Netz selbst, so dass die Werte tiefer im Netz auch ungefähr die selbe Größenordnung haben. Grundsätzlich werden die Variablen wie folgt normalisiert:\n",
    "\n",
    "\n",
    "$$x_s = \\frac{x-\\bar{x}}{sd_x}$$\n",
    "\n",
    "Dabei steht $\\bar{x}$ für den Mittelwert und $sd_x$ ist die Standardabweichung von $x$.<br><br>\n",
    "In einem klassischen neuronalen Netz werden die Activations einer Layer in zwei Schritten berechnet. Zunächst wird die lineare Transformation durchgeführt:\n",
    "\n",
    "$$Z = XW^T+b$$\n",
    "*Hier ist $X$ ein Minibatch, also zum Beispiel nur 32 Bilder*\n",
    "\n",
    "Darauf folgt eine nicht-lineare Aktivierungsfunktion:\n",
    "\n",
    "$$A = \\sigma(Z)$$\n",
    "\n",
    "Mit Batchnorm werden die Werte vor der Aktivierungsfunktion noch einmal normalisiert.\n",
    "\n",
    "$$Z_s = (\\frac{Z-\\bar{Z}}{sd_Z}) \\cdot \\gamma + \\beta $$\n",
    "\n",
    "Dies geschieht jedes Neuron unabhängig von einander. Der Mittelwert $\\bar{Z}$ und die Standardabweichung $sd_Z$ werden auch hier nur pro Minibatch berechnet. Neu ist das $\\gamma$ und $\\beta$. Dies sind nur zwei einzelne Parameter, die die Normalverteilung $N(0,1)$ verschieben und skalieren können. Auch diese beiden Parameter sind lernbar, d.h. sie werden auch beim Training verändert.\n",
    "\n",
    "\n",
    "Wichtig ist auch, dass während des Trainings die Mittelwerte der Minibatches zusammengefasst werden zu einem Mittelwert über alle Minibatches. Dieser Durchschnitt wird dann in der Evaluierung des Testsets zur Normalisierung der Minibatches verwendet.\n",
    "\n",
    "*Ob man zuerst die Aktivierungsfunktion und dann die Batchnorm anwendet oder umgekehrt, ist eine Frage, die Ihnen niemand beantworten kann. Es gibt Pro und Contra für beide Methoden.\n",
    "\n",
    "Vervollständige die `layer_one`. Die Größe der Hidden Layers und des Dropouts ist Ihnen überlassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_x, batch_y = next(iter(loader)) # hier wird der erste Minibatch ausgewählt\n",
    "\n",
    "layer_one = nn.Sequential(nn.Linear(____,___),\n",
    "                         nn.BatchNorm1d(_____),\n",
    "                         nn.ReLU(),\n",
    "                         nn.Dropout(____))\n",
    "layer_one(batch_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>Lösung:</b></summary>\n",
    "\n",
    "```python\n",
    "layer_one = nn.Sequential(nn.Linear(784,300),\n",
    "                         nn.BatchNorm1d(300),\n",
    "                         nn.ReLU(),\n",
    "                         nn.Dropout(0.2))\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erweitern Sie nun das komplette Netz von vorhin.\n",
    "Wichtig: Nach der letzten linearen Layer wird weder Batchnorm noch Dropout verwendet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netzwerk = nn.Sequential(nn.Linear(784,300), \n",
    "                         ______________,\n",
    "                         ______________,\n",
    "                         ______________,\n",
    "                         nn.Linear(300,300),\n",
    "                         ______________,\n",
    "                         ______________,\n",
    "                         ______________,\n",
    "                         nn.Linear(300,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>Lösung:</b></summary>\n",
    "\n",
    "```python\n",
    "netzwerk = nn.Sequential(nn.Linear(784,300), \n",
    "                         nn.BatchNorm1d(300),\n",
    "                         nn.ReLU(),\n",
    "                         nn.Dropout(0.2),\n",
    "                         nn.Linear(300,300),\n",
    "                         nn.BatchNorm1d(300),\n",
    "                         nn.ReLU(),\n",
    "                         nn.Dropout(0.2),\n",
    "                         nn.Linear(300,10))\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers\n",
    "\n",
    "\n",
    "Optimieres bestimmen, wie genau die Weights des Netzes geupdatet werden. Bisher haben Sie immer den `SGD` verwendet.  In der PyTorch-Implementierung vom `SGD` gibt es mehr Parameter für `SGD`, die das Training effektiver machen können. Einer davon ist \"Momentum\". Wenn Sie Momentum für das Training verwenden, werden die Weights nicht nur gemäß den Gradienten des aktuellen Minibatch aktualisiert. Auch die Gradienten der vorherigen Minibatches haben einen Einfluss. Der Parameter `momentum` gibt an, wie stark oder schwach der Einfluss der vorherigen Minibatches ist.\n",
    "\n",
    "Das Momentum soll den Loss in einer geraden Linie optimieren. Als Beispiel kann man sich einen Ball vorstellen, der einen Hügel hinunterrollt. Je länger diese in dieselbe Richtung rollt, desto schneller wird sie. Und je schneller sie wird, desto weniger Einfluss haben kleine Richtungsänderungen, die durch Veränderungen des Geländes (Gradients) ausgelöst werden. \n",
    "\n",
    "Wenn also die Gradients eine Zeit lang in dieselbe Richtung zeigen, dann sollte ein einzelner Minibatch diese Richtung nicht auf einmal ändern. \n",
    "\n",
    "Einer der am häufigsten verwendeten Algorithmen für das Training von Netzen ist der \"ADAM\"-Algorithmus. Er kombiniert viele Verbesserungen von SGD, einschließlich einer Version von Momentum.  \n",
    "\n",
    "**ADAM muss nicht immer besser sein als SGD**.\n",
    "\n",
    "Eine Übersicht über alle verfügbaren Optimierungsalgorithmen finden Sie [hier](https://pytorch.org/docs/stable/optim.html#algorithms). \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss_funktion = nn.CrossEntropyLoss()\n",
    "updaten = optim.Adam(netzwerk.parameters(), lr=0.1)\n",
    "EPOCHS = 10\n",
    "\n",
    "## Trainings Loop\n",
    "for i in range(EPOCHS):\n",
    "    loss_list = [] # diese Liste speichtert den Loss jedes Minibatches\n",
    "                   # damit können wir am Ende den Durschnittsloss innerhalb des Epochs berechnen\n",
    "    netzwerk.train()\n",
    "    for minibatch in loader: # for-loop geht durch alle Minibatches\n",
    "        images, labels = minibatch # minibatch wird in Bilder und Labels geteilt\n",
    "        \n",
    "        updaten.zero_grad()\n",
    "        output = netzwerk(images) # Forward Propagation\n",
    "    \n",
    "        loss   = loss_funktion(output, labels)\n",
    "        loss.backward()\n",
    "        loss_list.append(loss.item())\n",
    "        updaten.step()\n",
    "    netzwerk.eval()    \n",
    "    output = netzwerk(train_images)\n",
    "    acc=((output.max(dim=1)[1]==train_labels).sum()/float(output.shape[0])).item()\n",
    "    \n",
    "    print(i,\n",
    "        \"Training Loss: %.2f Training Accuracy: %.2f\"\n",
    "        % (np.mean(loss_list), acc)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Übungsaufgabe\n",
    "\n",
    "Für die Übungsaufgabe werden Sie ein Netzwerk trainieren, aber dieses Mal mit den Toxizitätsdaten aus Woche 5.\n",
    "Laden Sie zunächst erneut alle erforderlichen Libraries und Daten. Verwenden Sie dieses Mal auch Batchnorm, Dropout und den ADAM-Algorithmus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "%run ../utils/utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_tox = pd.read_csv(\"../data/toxicity/sr-mmp.tab\", sep = \"\\t\")\n",
    "data_tox = data_tox.iloc[:,1:] #alle Spalten bis auf die erste (index 0) werden ausgewählt\n",
    "data_tox.columns = [\"smiles\", \"activity\"]\n",
    "data_tox.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als Nächstes berechnen Sie die Fingerprints. Wie in Woche 05 steht Ihnen dafür die Funktion `get_fingerprints` zur Verfügung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fps = get_fingerprints(data_tox)\n",
    "fps[\"activity\"] = data_tox.activity\n",
    "fps.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bevor Sie sie in Pytorch verwenden können, müssen Sie sowohl die Fingerprints als auch die `acitivty` in `tensors` umwandeln. Beachten Sie, dass beide im DataFrame `fps` sind. \n",
    "\n",
    "`.values` konvertiert einen DataFrame in ein `np.array`.\n",
    "\n",
    "Dann werden die Daten in einen Trainings- und einen Testsatz aufgeteilt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fps = torch.tensor(___.values, dtype=torch.float32) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test=train_test_split(fps,test_size= 0.2 , train_size= 0.8, random_state=1234)\n",
    "\n",
    "\n",
    "train_x = train[:,:-1]\n",
    "train_y = train[:,-1]\n",
    "test_x = test[:,:-1]\n",
    "test_y = test[:,-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt wollen wir wieder Minibatches verwenden. Dazu müssen wir unsere Trainingsdaten noch in einen `DataLoader` umwandeln. Warum nur die Trainingsdaten? Die Verwendung von Minibatches ist nur für das Training relevant. Solange Ihr Computer in der Lage ist, den Testdatensatz auf einmal durch das Netz zu führen, brauchen wir den Testdatensatz nicht in Minibatches aufzuteilen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=data.TensorDataset(______, _____) # input sind unsere Tensors die einmal die Fingerprints die activities\n",
    "loader=data.DataLoader(train_data, batch_size = 32)\n",
    "len(loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passen Sie das Netz so an, dass der Input und der Output die richtige Größe haben. Also die Länge der Fingerprinzs und die Anzahl der Klassen, die wir vorhersagen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netzwerk = nn.Sequential(nn.Linear(), \n",
    "                         nn.BatchNorm1d(),\n",
    "                         nn.ReLU(), \n",
    "                         nn.Dropout(),\n",
    "                         nn.Linear(),\n",
    "                         nn.BatchNorm1d(),\n",
    "                         nn.ReLU(), \n",
    "                         nn.Dropout(),\n",
    "                         nn.Linear())\n",
    "\n",
    "loss_funktion = nn.BCEWithLogitsLoss()\n",
    "updaten = ___________________, lr=0.1)    \n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als letztes füllen Sie den `for loop`. \n",
    "\n",
    "`.squeeze` konvertiert den `(n,1)` `output` tensor zu einem 1-dimensionalen `tensor` der Länge `n`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(EPOCHS):\n",
    "    loss_list = [] # diese Liste speichtert den Loss jedes Minibatches\n",
    "    for minibatch in loader: # for-loop geht durch alle minibatches\n",
    "        updaten.__________\n",
    "        molecules, activity = minibatch # Minibatchh wird in Bilder und Labels geteilt\n",
    "        output = netzwerk(____________) # Forward Propagation\n",
    "        loss   = loss_funktion(output.squeeze(), ____________)\n",
    "        loss._______\n",
    "        loss_list.append(loss.item())\n",
    "        updaten.________\n",
    "    # Hier wird die Accuracy für den Testsatz berechnet\n",
    "    output = netzwerk(test_x)\n",
    "    acc = torch.sum((output>0).squeeze().int() == test_y)/float(test_y.shape[0])\n",
    "   \n",
    "    print(\n",
    "        \"Training Loss: %.2f Test Accuracy: %.2f\"\n",
    "        % (np.mean(loss_list), acc.item())\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b2063f454e5583264956edca724ed174a35400d49c5baf96fcf9ea99fcd5830b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ma1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
