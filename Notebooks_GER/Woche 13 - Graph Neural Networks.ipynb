{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f5efb70",
   "metadata": {},
   "source": [
    "# Graph Neural Networks\n",
    "\n",
    "---\n",
    "**Lernziele**\n",
    "* Sie verstehen, wie Moleküle für den Computer als Graph dargestellt werden können. \n",
    "* Sie verstehen, wie Graph Neural Networks funktionieren.\n",
    "* Sie können ein Graph Neural Network als Pytorch Klasse schreiben.\n",
    "---\n",
    "\n",
    "Graph Neural Networks sind noch eine relativ neue Methode. Intuitiv können wir sehen, dass sich Moleküle sehr gut als (mathematischen) Graph darstellen lassen. Dabei entsprechen die Bindungen im Molekül den Kanten (edges) des Graphs und die Atome  den Knoten (nodes).<br> Für den Computer sind jedoch Graphen nicht so leicht zu lesen wie zum Beispiel ein SMILES string. Wir stellen die Moleküle als zwei Matrizen dar. Zum einen die Adjacency-Matrix, die die Atomverknüpfung (also die Bindungen) darstellt. Dazu kommen dann noch die Feature-Matrix. Für das Machine Learning One-Hot koidieren wir hier die Atome.<br><br>\n",
    "<center>\n",
    "<img src=\"https://www.researchgate.net/profile/Jorge_Galvez2/publication/236018587/figure/fig1/AS:299800013623305@1448489301609/The-chemical-graph-and-adjacency-matrix-of-the-isopentane.png\" style=\"width: 600px;\">\n",
    "</center>\n",
    "<h8><center>Galvez et. al. 2010</center></h8><br><br>\n",
    "Wir nehmen nochmal die Daten aus der Tox21 Challenge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc9870f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.rdmolops import GetAdjacencyMatrix\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils import data\n",
    "import math\n",
    "from sklearn.metrics import roc_auc_score\n",
    "%run ../utils/onehotencoder.py\n",
    "np.set_printoptions(linewidth=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2024983d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laden der Daten\n",
    "data_tox = pd.read_csv(\"../data/toxicity/sr-mmp.tab\", sep = \"\\t\")\n",
    "data_tox = data_tox.iloc[:,1:] #alle Spalten bis auf die erste (index 0) werden ausgewählt\n",
    "data_tox.columns = [\"smiles\", \"activity\"]\n",
    "data_tox.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1321a720",
   "metadata": {},
   "source": [
    "## Adjacency Matrix und One-Hot Encoded Feature Matrix\n",
    "Mit den Smiles können wir diesmal leider  nicht so viel mit anfangen. Aber zum Glück gibt es in RDKit direkt eine Funktion, um die Adjacency Matrix zu erstellen. Diese haben wir oben schon importiert.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120a224f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mols = [Chem.MolFromSmiles(x) for x in data_tox['smiles']]\n",
    "A = [GetAdjacencyMatrix(x) for x in mols]\n",
    "print(A[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c0e03f",
   "metadata": {},
   "source": [
    "Um die Atome zu One-Hot Encoden benutzen wir die vorgeschriebene `onehotencode()` Funktion.\n",
    "Ähnlich wie bei RNNs Smilestoken kodiert werden, wird hier nur der Atomtype eines jedes Atoms erfasst. Diese Atome werden dann in one-hot kodiert.\n",
    "Pro Moleküle werden diese Vektoren untereinander zu einer Matrix kombiniert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9fb9b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feat = onehotencode(mols)\n",
    "feat[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3926e98c",
   "metadata": {},
   "source": [
    "Oben ist zusehen, wie eine Feature-Matrix für ein Molekül aussieht.\n",
    "Wenn wir uns die `.shape` ansehen, können wir erkennen, dass unser Molekül aus `29` Atomen besteht. Insgesamt immer Datensatz gibt es `25` Atomtypen. Die Anzahl Spalten (Anzahl der Features) muss für alle Moleküle gleich sein, sonst können wir die Moleküle nicht durch das Netzwerk führen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4961d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2fd903",
   "metadata": {},
   "source": [
    "Vielleicht ist ihnen aufgefallen, dass auf der Diagonalen der Adjacency Matrix noch Nullen stehen. In einer Graph Convolution sollen aber nicht nur die Features der benachbarten, sondern auch des zentralen Atoms mit in die Rechnung genommen werden. Dafür werden Einsen auf der Diagonalen der Adjacency-Matrix benötigt. \n",
    "Die Funktion `np.fill_diagonal(matrix, value)` erlaubt es Ihnen, die Werte der Diagonalen einer Matrix zu ändern. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c81e5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for matrix in A:\n",
    "    np.fill_diagonal(matrix, 1)\n",
    "print(A[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362ff2ee",
   "metadata": {},
   "source": [
    "## Graph Convolution\n",
    "Jetzt wollen wir die Informationen der Knoten an die Nachbarknoten entlang der Kanten weiterzugeben, was als (Graph) Convolution bezeichnet wird. Hierfür werden während des forward pass diese mathematischen Operationen des ausgeführt: $$\\hat{X} = \\hat{D}^{-1}\\hat{A}XW$$\n",
    "Hier ist $\\hat{A}$ unsere Adjacency Matrix, mit Einsen auf der Diagonalen. $X$ ist die Feature-Matrix und $W$ sind die Weights, die pytorch initialisiert. Es fehlt noch $D$, die Degree Matrix. Diese Matrix enthält die Anzahl der Verbindungen, die jedes Atom im Molekül hat. Diese Werte werden in der Degree-Matrix auf der Diagonalen platziert. Die Anzahl der Verbindung jedes Moleküls lässt sich leicht aus der Adjacency-Matrix berechnen. Dafür müssen Sie die Summe der Reihe oder Spalten der$\\hat{A}$ Matrix berechnen. Diese Summe gibt die Degrees eines Atom an. Um genau zu sein, ist es der Degree + 1 des Atoms, da Sie die Diagonale der Adjacency-Matrix bereits mit Einsen gefüllt haben.\n",
    "Um die Degree-Matrix zu erstellen, müssen Sie die Summen der Reihen in jeder Adjacency-Matrix berechnen und dieses auf die Diagonale einer neuen Matrix platzieren.\n",
    "\n",
    "\n",
    "Dieser Prozess ist relativ simpel mit `numpy`. `np.sum(A[i], axis=0)` berechnet die Summe pro Spalte und `np.diag()` erstellt aus einem 1D-Array eine Matrix mit den Werten des 1D Arrays auf der Diagonalen. Sie können die beiden Funktion in Kombination benutzen, um die Degree-Matrizen zu erstellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61bef88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "D =[]\n",
    "for matrix in A:\n",
    "    D.append(np.diag(np.sum(matrix, axis=1)))\n",
    "print(D[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb2560b",
   "metadata": {},
   "source": [
    "Bevor wir anfangen ein Netzwerk zu bauen und es mit unseren Daten zu füttern, können wir noch einen Schritt durchführen, um einiges an Rechenaufwand zu sparen, denn $\\hat{D}^{-1}\\hat{A}$ kann schon vor dem Trainieren das Netzwerk berechnet werden. Dadurch kann Zeit gespart werden, da dieser Schritt nicht ständig im Netzwerk wiederholt werden muss.\n",
    "\n",
    "`np.linalg.matrix_power` wird benötigt, um das Invers der Matrix `D` zu berechnen. Ansonsten ist dieser Schritt eine einfache Matrixmultiplikation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0159329",
   "metadata": {},
   "outputs": [],
   "source": [
    "DA = []\n",
    "for i in range(len(D)):\n",
    "    DA.append(np.matmul(np.linalg.matrix_power(D[i], -1),A[i]))\n",
    "DA[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1fe5a8",
   "metadata": {},
   "source": [
    "Wir haben also die Liste der Adjacency Matrizen `DA`. Diese enthält die Informationen über die Struktur des Moleküls. Die Liste `feat` enthält die Features, also die Informationen zu den einzelnen Knoten. Und als Letztes erstellen wir noch eine `labels` Liste. Diese enthält die Information, die wir vorhersagen wollen (toxisch vs. nicht toxisch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040c982e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DA = [torch.tensor(x,dtype=torch.float32) for x in DA] # Konvertieren der Arrays zu Tensoren\n",
    "feat = [torch.tensor(x,dtype=torch.float32) for x in feat] # Konvertieren der Arrays zu Tensoren\n",
    "labels = [torch.tensor([x], dtype=torch.float32) for x in data_tox['activity']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bd3802",
   "metadata": {},
   "source": [
    "## Graph Convolution Layer\n",
    "Wir wollen uns für die Graph Convolution die Struktur von PyTorch zunutze machen. Wie schon letzte Woche verwenden wir dafür Klassen.\n",
    "Letzte Woche haben wir eine Klasse benutzt um aus verschiedene Layern (z.B. `nn.Linear()`)ein Netzwerk zu erstellen. \n",
    "Diese Layers sind in pyTorch schon vorgeschrieben, aber auch wir können unsere eigenen Layer erstellen. Auch hierfür brauchen wir wieder die PyTorch Klasse.\n",
    "Unser Ziel ist es, so etwas wie `nn.Linear()` zu programmieren, sodass wir unser Repertoire an Hidden Layers (Linear, RNN, GRU, Dropout, ...) um eine Graph Convolution Layer erweitern.<br> Auch dafür können wir die`nn.Module` Klasse als Basis für unsere Graph Convolution verwenden. Wir fangen mit dem minimalen Grundgerüst an:\n",
    "```python\n",
    "class GraphConvolution(nn.Module):\n",
    "    pass\n",
    "```\n",
    "Damit wir richtig auf die Funktionen der übergeordneten `nn.Module` Klasse zugreifen können, müssen wir diese mit `super().__init__()` initialisieren. Unsere Convolutional Layer muss natürlich wissen, wie groß der Input ist, und wie groß der Output werden soll. Außerdem müssen wir hier selbst die Weights und Biases initialisieren.\n",
    "```python\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.in_featuers = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        self.bias = nn.Parameter(torch.FloatTensor(out_features))\n",
    "```\n",
    "Dann können wir auch schon die `forward()` Funktion programmieren. Hier passiert die eigentliche Convolution, bei der die Knoten mit den Features `x` durch Matrixmultiplikation mit der Adjacency Matrix `adj` über ihre Nachbarknoten lernen. Indem wir noch die lernbaren `weights` dazwischenschalten, können wir das Ganze später optimieren.\n",
    "```python\n",
    "    def forward(self, x, adj):\n",
    "        support = torch.mm(x, self.weight)\n",
    "        output = torch.mm(adj, support)\n",
    "        return output + self.bias\n",
    "```\n",
    "Damit würde unsere `GraphConvolution` Klasse auch schon funktionieren. Wir initialisieren die Weights aber noch etwas optimiert mit der Funktion `reset_parameters()`. Und mit der Funktion `__repr__()` können wir uns die Layer noch richtig anzeigen lassen. Unsere finale Klasse sieht dann so aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e85d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        self.bias = nn.Parameter(torch.FloatTensor(out_features))\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(0, stdv)\n",
    "        self.bias.data.uniform_(-stdv, stdv)\n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        support = torch.mm(x, self.weight)\n",
    "        output = torch.mm(adj, support)\n",
    "        return output + self.bias\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + 'in_features=' + str(self.in_features) + ', ' \\\n",
    "               + 'out_features=' + str(self.out_features) + ')'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8d289f",
   "metadata": {},
   "source": [
    "Wir können auch überprüfen, ob unsere Graph Convolution Layer auch schon funktioniert.\n",
    "Zunächst speichern wir eine Feature-Matrix und eine Adjacency Matrix als Beispiel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2d1fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_beispiel = feat[1]\n",
    "adj_beispiel = DA[1]\n",
    "print('Features:', feat_beispiel.shape)\n",
    "print('DA:', adj_beispiel.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab0b55b",
   "metadata": {},
   "source": [
    "Wir erstellen nun eine GraphConvolution. Beachten Sie, dass die Inputgröße, der ersten Layer, der Größe des Featurevektors (Anzahl der Spalten) (`25`) entspricht. Also genauso wie bei einer `nn.Linear` Layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73a6370",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = GraphConvolution(25, 100)\n",
    "conv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab305ee3",
   "metadata": {},
   "source": [
    "Wir können nun das Beispiel durch die Convolution führen. Sie werden sehen, dass sich die Dimension der Features auf 100 vergrößert haben wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1c2cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = conv(feat_beispiel, adj_beispiel)\n",
    "print('\\nOutput:', output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9417a666",
   "metadata": {},
   "source": [
    "## Graph Neural Network\n",
    "Um jetzt ein Netz zu bauen, können wir wieder die `nn.Module` Basisklasse verwenden. \n",
    "\n",
    "Letzte Woche haben wir, unseren eigenen Autoencoder zusammen gestellt. Diese Woche benutzen wir diese Art von Klasse, um mehrere Graph Convolutions miteinander zu verbinden. Wichtig ist aber auch zu beachten, dass wir wie bei regulären Convolution Neural Networks nicht nur Convolutional Layers benutzen können. Im CNN selber müssen wir am Ende den Tensor flatten. Dadurch erhalten wir einen Vektor, der dann durch eine linear Layer geführt werden wird. Das Gleiche gilt auch für Graph Convolutions. Um den Output dieser Graph Convolutions in eine linear Layer zu führen, können wir aber nicht `Flatten` benutzen, sondern wir berechnen einfach den Mittelwert über alle Spalten.\n",
    "\n",
    "Das macht die Funktion `aggregate()`.\n",
    "\n",
    "Sie können im Netzwerk erkennen, dass wir zwei Graph Convolution Layers benutzen, und dann eine linear Layer.\n",
    "Im `forward` Pass wird erst der Input `(x, adj)` durch die erste Graph Layer geführt, dann folgt eine ReLU Funktion. Dann wiederholt sich das selber Spiel für die zweite Convolution. \n",
    "Nun wenden wir die `aggregate` Funktion an. Diese berechnet jetzt den Mittelwert eines jeden Features. Der Output dieser Layer hat also in dem Beispiel die Größe `[1,100]`.\n",
    "Als Letztes benutzen wir die linear Layer, um die Vorhersage zu machen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ed67d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphNN(nn.Module):\n",
    "    def __init__(self):#in_features, out_features, size_labels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GraphConvolution(25, 100)\n",
    "        self.conv2 = GraphConvolution(100, 100)\n",
    "        self.lin = nn.Linear(100, 1)\n",
    "        \n",
    "    def aggregate(self, convoluted_graph): # we use mean aggregation, max or min could also be used as hyperparameter\n",
    "        return torch.mean(convoluted_graph, dim=0, keepdim=True)\n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        x = self.conv1(x, adj)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, adj)\n",
    "        x = F.relu(x)\n",
    "        x = self.aggregate(x)\n",
    "        x = self.lin(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8283147",
   "metadata": {},
   "source": [
    "Nun werden wir noch schnell den Datensatz in Training und Testset teilen. Hierfür nehmen wir einfach  die ersten 1800 Moleküle als Trainingsset und den Rest als Testset. Wichtig ist, dass wir in diesem Beispiel keine Minibtaches benutzen. Das liegt daran, dass Featurematrizen der einzelnen Moleküle unterschiedlich groß sind, bzw. unterschiedliche viele Reihen haben. Das heißt wir können sie nicht ohne weiteres, wie zum Beispiel beim CNN als 3D Tensor speichern. Es gibt zwar Möglichkeiten dieses Problem zu lösen, diese sind aber nicht relevant für das Notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1ef789",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat = feat[:1800]\n",
    "train_DA = DA[:1800]\n",
    "train_labels = labels[:1800]\n",
    "\n",
    "\n",
    "test_feat = feat[1800:]\n",
    "test_DA = DA[1800:]\n",
    "test_labels = labels[1800:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b44562",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn = GraphNN()\n",
    "loss_funktion= nn.BCEWithLogitsLoss()\n",
    "optimizer=optim.Adam(gnn.parameters(), lr =0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a75bda0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    loss_list_train = []\n",
    "    acc_list_train= []\n",
    "    gnn.train()\n",
    "    for k in range(len(train_feat)):\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        output=gnn(train_feat[k], train_DA[k]).flatten()\n",
    "\n",
    "        loss=loss_funktion(output,train_labels[k])\n",
    "        loss.backward()\n",
    "        loss_list_train.append(loss.item())\n",
    "        optimizer.step()\n",
    "\n",
    "        acc_list_train.append(np.sum((torch.round(torch.sigmoid(output)) == train_labels[k]).detach().numpy()))\n",
    "    loss_list_test = []\n",
    "    acc_list_test= []\n",
    "    gnn.eval()\n",
    "    for k in range(len(test_feat)):\n",
    "    \n",
    "        output=gnn(test_feat[k], test_DA[k]).flatten()\n",
    "\n",
    "        loss=loss_funktion(output,test_labels[k])\n",
    "        loss_list_test.append(loss.item())\n",
    " \n",
    "\n",
    "        acc_list_test.append(np.sum((torch.round(torch.sigmoid(output)) == test_labels[k]).detach().numpy()))\n",
    "            \n",
    "        \n",
    "    print(i,\"Train Loss: %.2f Train Accuracy: %.2f Test Loss: %.2f Test Accuracy: %.2f\"\n",
    "        % (np.mean(loss_list_train), np.mean(acc_list_train),np.mean(loss_list_test), np.mean(acc_list_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d732ba",
   "metadata": {},
   "source": [
    "Sie sehen, das Training dauert sehr lange und ist nicht besonders erfolgreich. Das hie vorgestellte Models ist ein sehr, sehr einfaches. Tatsächlich werden normalerweise komplexere Graph Convolution genommen. Auch wird nicht nur der Atomtyp, sondern eine Vielzahl als Input für die Featurematrix genommen. \n",
    "\n",
    "Seit einigen Jahren gibt es auch [PyTorch Geometric](https://pytorch-geometric.readthedocs.io/en/latest/) und [Deep Graph Library](https://www.dgl.ai/). Beide stellen eine Erweiterung zu PyTorch. Sie enthalten wichtige Funktionalitäten, die relevant für den Umgang mit Graphen sind. Auch enthalten die Libraries, die wichtigsten Graph Layers. Dadurch müssen Sie die Layers sind nicht mehr selber schreiben."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe796496",
   "metadata": {},
   "source": [
    "## Übungsaufgabe:\n",
    "\n",
    "Hier sehen Sie unser Graph Neural Network.\n",
    "Das Problem ist, dass dieses Netzwerk keine Flexibilität bietet.\n",
    "Die Gewichtsmatrizen des Netzwerks werden immer die selber Größen haben.\n",
    "Auch fehlt Dropout. Wir brauchen kein Batchnorm, da wir keine Minibatches benutzen.\n",
    "\n",
    "\n",
    "```python\n",
    "class GraphNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = GraphConvolution(25, 100)\n",
    "        self.conv2 = GraphConvolution(100, 100)\n",
    "        self.lin = nn.Linear(100, 1)\n",
    "        \n",
    "    def aggregate(self, convoluted_graph): \n",
    "        return torch.mean(convoluted_graph, dim=0, keepdim=True)\n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        x = self.conv1(x, adj)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, adj)\n",
    "        x = F.relu(x)\n",
    "        x = self.aggregate(x)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "```\n",
    "\n",
    "Können Sie das Netzwerk so umschreiben, dass wir flexibel den Input, als auch die Hiddengrößen?\n",
    "Sie können testen, ob ihr Netzwerk noch funktioniert mit dem `beispiel_DA` und `beispie_feat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021d2d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "beispiel_DA = test_DA[0]\n",
    "beispiel_feat = test_feat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23046cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = GraphConvolution(25, 100)\n",
    "        self.conv2 = GraphConvolution(100, 100)\n",
    "        self.lin = nn.Linear(100, 1)\n",
    "        \n",
    "    def aggregate(self, convoluted_graph): \n",
    "        return torch.mean(convoluted_graph, dim=0, keepdim=True)\n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        x = self.conv1(x, adj)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, adj)\n",
    "        x = F.relu(x)\n",
    "        x = self.aggregate(x)\n",
    "        x = self.lin(x)\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
